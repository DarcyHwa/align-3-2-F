# -*- coding:utf8 -*-
"""
DTWå¯¹é½ç®—æ³•æ¨¡å— - åŸºäºå·²åˆ’åˆ†åŒºé—´çš„åŠ¨æ€æ—¶é—´è§„æ•´å¯¹é½

æœ¬æ¨¡å—å®ç°äº†è·¨è¯­è¨€å¥å­å¯¹é½ç³»ç»Ÿçš„æ ¸å¿ƒDTWç®—æ³•éƒ¨åˆ†ï¼š
1. è¯»å–å·²åˆ’åˆ†å®Œæˆçš„å¯¹é½åŒºé—´æ•°æ®
2. å®ç°å®Œæ•´çš„DTWåŠ¨æ€æ—¶é—´è§„æ•´ç®—æ³•
3. æ”¯æŒå¤šç§å¥å­ç»„åˆæ¨¡å¼ï¼ˆ1-1ã€1-å¤šã€å¤š-1ã€2-2ç­‰ï¼‰
4. è®¡ç®—æœ€ä¼˜å¯¹é½è·¯å¾„å’Œç›¸ä¼¼åº¦å¾—åˆ†
5. è¾“å‡ºå¯¹é½ç»“æœåˆ°å¤šç§æ ¼å¼

æ ¸å¿ƒç®—æ³•æµç¨‹ï¼š
1. åŠ è½½åŒºé—´åˆ’åˆ†æ•°æ®å’Œç›¸ä¼¼åº¦çŸ©é˜µ
2. åˆå§‹åŒ–DTWç®—æ³•å‚æ•°å’Œæ•°æ®ç»“æ„
3. åœ¨æ¯ä¸ªåŒºé—´å†…æ‰§è¡ŒDTWæœç´¢
4. è®¡ç®—è·ç¦»å‡½æ•°å’Œè·¯å¾„ä¼˜åŒ–
5. ç”Ÿæˆæœ€ç»ˆå¯¹é½ç»“æœ

æŠ€æœ¯ç‰¹ç‚¹ï¼š
- å®Œå…¨åŸºäºæ‚¨å·²å®ç°çš„æ•°æ®ç»“æ„
- å¤ç°å‚è€ƒä»£ç çš„DTWæ ¸å¿ƒç®—æ³•
- æ”¯æŒçµæ´»çš„å¥å­ç»„åˆé…ç½®
- é«˜æ•ˆçš„ç¼“å­˜å’Œä¼˜åŒ–æœºåˆ¶

ä½œè€…ï¼šAugment-2
"""

import os
import sys
import json
import math
import time
import numpy as np
import requests
from typing import Dict, List, Tuple, Any
from collections import defaultdict


# å…¨å±€å¸¸é‡
INFINITE = float('inf')

# ä½å±‚ç®—æ³•å‚æ•°ï¼ˆå¯ç”¨æ‰€æœ‰åŠŸèƒ½çš„ä¼˜åŒ–é…ç½®ï¼‰
coeff_sent_len = 0.33  # å¹³è¡¡è·¯å¾„è·ç¦»ä¸­"å¥é•¿æƒ©ç½š"çš„æƒé‡ï¼ˆå€¼è¶Šå¤§è¶Šä¾èµ–å¥é•¿ï¼‰
coeff_neighbour_sim = 0.6  # é‚»è¿‘å¥ç›¸ä¼¼åº¦æƒ©ç½šç³»æ•°ï¼Œç”¨äºæŠ‘åˆ¶å¯¹è¯­å¢ƒç›¸ä¼¼ä½†éå¯¹åº”å¥å­çš„é”™è¯¯å¯¹é½
only_one_2_one_pairing = False  # å¯ç”¨å¤šç§å¯¹é½æ¨¡å¼ï¼Œä¸é™åˆ¶ä»…ä½¿ç”¨ 1-1 / 1-0 / 0-1 ä¸‰ç§é…å¯¹å½¢å¼


class SimpleOOBTree:
    """
    ç®€å•çš„æœ‰åºå­—å…¸å®ç°ï¼Œæ›¿ä»£BTrees.OOBTree
    """
    def __init__(self):
        self._data = {}

    def __setitem__(self, key, value):
        self._data[key] = value

    def __getitem__(self, key):
        return self._data[key]

    def __delitem__(self, key):
        del self._data[key]

    def __contains__(self, key):
        return key in self._data

    def __len__(self):
        return len(self._data)

    def maxKey(self):
        if not self._data:
            raise ValueError("Empty tree")
        return max(self._data.keys())

    def keys(self):
        return sorted(self._data.keys())

    def items(self):
        return [(k, self._data[k]) for k in sorted(self._data.keys())]

# DTWç®—æ³•å‚æ•°ï¼ˆå¯ç”¨æ‰€æœ‰åŠŸèƒ½çš„ä¼˜åŒ–é…ç½®ï¼‰
COEFF_SENT_LEN = 0.33  # å¹³è¡¡è·¯å¾„è·ç¦»ä¸­"å¥é•¿æƒ©ç½š"çš„æƒé‡
COEFF_NEIGHBOUR_SIM = 0.6  # é‚»è¿‘å¥ç›¸ä¼¼åº¦æƒ©ç½šç³»æ•°
ONLY_ONE_2_ONE_PAIRING = False  # å¯ç”¨å¤šç§å¯¹é½æ¨¡å¼ï¼Œä¸é™åˆ¶ä»…ä½¿ç”¨ 1-1 / 1-0 / 0-1 ä¸‰ç§é…å¯¹å½¢å¼


def load_interval_data(interval_file_path: str, sentence_id: int) -> Dict[str, Any]:
    """
    ä»JSONæ–‡ä»¶ä¸­åŠ è½½æŒ‡å®šsentence_idçš„åŒºé—´åˆ’åˆ†æ•°æ®
    
    å‚æ•°:
        interval_file_path (str): åŒºé—´æ•°æ®æ–‡ä»¶è·¯å¾„
        sentence_id (int): ç›®æ ‡å¥å­ID
        
    è¿”å›å€¼:
        Dict[str, Any]: åŒ…å«åŒºé—´ã€é”šç‚¹ã€å¥å­ç­‰å®Œæ•´æ•°æ®çš„å­—å…¸
    """
    print(f"\n=== åŠ è½½åŒºé—´åˆ’åˆ†æ•°æ® ===")
    print(f"æ–‡ä»¶è·¯å¾„: {interval_file_path}")
    print(f"ç›®æ ‡å¥å­ID: {sentence_id}")
    
    try:
        with open(interval_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if data.get('sentence_id') != sentence_id:
            raise ValueError(f"æ–‡ä»¶ä¸­çš„sentence_id ({data.get('sentence_id')}) ä¸ç›®æ ‡ID ({sentence_id}) ä¸åŒ¹é…")
        
        print(f"âœ“ æ•°æ®åŠ è½½æˆåŠŸ")
        print(f"  æœ€ç»ˆé”šç‚¹æ•°é‡: {data['final_anchors']['count']}")
        print(f"  å¯å¯¹é½åŒºé—´æ•°é‡: {data['alignable_intervals']['count']}")
        print(f"  è‹±æ–‡å¥å­æ•°: {data['source_data']['document_size']['english_sentences']}")
        print(f"  ä¸­æ–‡å¥å­æ•°: {data['source_data']['document_size']['chinese_sentences']}")
        
        return data
        
    except FileNotFoundError:
        raise FileNotFoundError(f"åŒºé—´æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {interval_file_path}")
    except json.JSONDecodeError as e:
        raise ValueError(f"JSONæ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")


def load_similarity_matrix(matrix_file_path: str, sentence_id: int) -> np.ndarray:
    """
    ä»JSONæ–‡ä»¶ä¸­åŠ è½½æŒ‡å®šsentence_idçš„ç›¸ä¼¼åº¦çŸ©é˜µ
    
    å‚æ•°:
        matrix_file_path (str): ç›¸ä¼¼åº¦çŸ©é˜µæ–‡ä»¶è·¯å¾„
        sentence_id (int): ç›®æ ‡å¥å­ID
        
    è¿”å›å€¼:
        np.ndarray: ç›¸ä¼¼åº¦çŸ©é˜µ
    """
    print(f"\n=== åŠ è½½ç›¸ä¼¼åº¦çŸ©é˜µ ===")
    print(f"æ–‡ä»¶è·¯å¾„: {matrix_file_path}")
    
    try:
        with open(matrix_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        for item in data:
            if item.get('sentence_id') == sentence_id:
                matrix = np.array(item['semantic_similarity_matrix'], dtype=np.float64)
                print(f"âœ“ ç›¸ä¼¼åº¦çŸ©é˜µåŠ è½½æˆåŠŸï¼Œç»´åº¦: {matrix.shape}")
                return matrix
        
        raise ValueError(f"æœªæ‰¾åˆ°sentence_idä¸º{sentence_id}çš„ç›¸ä¼¼åº¦çŸ©é˜µ")
        
    except FileNotFoundError:
        raise FileNotFoundError(f"ç›¸ä¼¼åº¦çŸ©é˜µæ–‡ä»¶æœªæ‰¾åˆ°: {matrix_file_path}")


def load_sentence_data(sentence_file_path: str, sentence_id: int) -> Tuple[List[str], List[str]]:
    """
    ä»JSONæ–‡ä»¶ä¸­åŠ è½½æŒ‡å®šsentence_idçš„å¥å­æ•°æ®
    
    å‚æ•°:
        sentence_file_path (str): å¥å­æ•°æ®æ–‡ä»¶è·¯å¾„
        sentence_id (int): ç›®æ ‡å¥å­ID
        
    è¿”å›å€¼:
        Tuple[List[str], List[str]]: (è‹±æ–‡å¥å­åˆ—è¡¨, ä¸­æ–‡å¥å­åˆ—è¡¨)
    """
    print(f"\n=== åŠ è½½å¥å­æ•°æ® ===")
    print(f"æ–‡ä»¶è·¯å¾„: {sentence_file_path}")
    
    try:
        with open(sentence_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        for item in data:
            if item.get('sentence_id') == sentence_id:
                english_sentences = item.get('english_sentence_text', [])
                chinese_sentences = item.get('chinese_sentence_text', [])
                print(f"âœ“ å¥å­æ•°æ®åŠ è½½æˆåŠŸ")
                print(f"  è‹±æ–‡å¥å­æ•°: {len(english_sentences)}")
                print(f"  ä¸­æ–‡å¥å­æ•°: {len(chinese_sentences)}")
                return english_sentences, chinese_sentences
        
        raise ValueError(f"æœªæ‰¾åˆ°sentence_idä¸º{sentence_id}çš„å¥å­æ•°æ®")
        
    except FileNotFoundError:
        raise FileNotFoundError(f"å¥å­æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {sentence_file_path}")


def call_qwen3_embedding(text_list: List[str]) -> List[np.ndarray]:
    """
    è°ƒç”¨è¿œç¨‹Qwen3-Embedding-8Bæ¨¡å‹è®¡ç®—æ–‡æœ¬åˆ—è¡¨çš„å‘é‡è¡¨ç¤º
    æ”¯æŒæ‰¹é‡å¤„ç†ï¼Œæé«˜æ•ˆç‡

    å‚æ•°:
        text_list (List[str]): éœ€è¦å‘é‡åŒ–çš„æ–‡æœ¬åˆ—è¡¨

    è¿”å›å€¼:
        List[np.ndarray]: å¯¹åº”çš„åµŒå…¥å‘é‡åˆ—è¡¨
    """
    if not text_list:
        print("âš ï¸ è¾“å…¥æ–‡æœ¬åˆ—è¡¨ä¸ºç©ºï¼Œè¿”å›ç©ºåˆ—è¡¨")
        return []

    try:
        # è°ƒç”¨è¿œç¨‹Qwen3-Embedding-8B API
        api_url = "https://api.siliconflow.cn/v1/embeddings"
        api_key = "sk-qmyqlcevlelaxuxuvwhkpdqsyhoadeaudrawwylzhntpuknv"

        payload = {
            "model": "Qwen/Qwen3-Embedding-8B",
            "input": text_list,
            "encoding_format": "float",
        }
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
        }

        print(f"æ­£åœ¨è°ƒç”¨Qwen3-Embedding-8Bæ¨¡å‹ï¼Œå¤„ç† {len(text_list)} ä¸ªæ–‡æœ¬...")
        response = requests.post(api_url, json=payload, headers=headers, timeout=60)
        response.raise_for_status()

        resp_json = response.json()
        embeddings_data = resp_json.get("data", [])

        if len(embeddings_data) != len(text_list):
            raise RuntimeError(f"è¿”å›çš„åµŒå…¥æ•°é‡({len(embeddings_data)})ä¸è¾“å…¥æ–‡æœ¬æ•°é‡({len(text_list)})ä¸ä¸€è‡´")

        # å¤„ç†è¿”å›çš„åµŒå…¥å‘é‡
        embeddings = []
        for item in embeddings_data:
            embedding = np.array(item.get("embedding"), dtype=np.float64)

            # Qwen3-Embedding-8Bå·²ç»L2å½’ä¸€åŒ–ï¼Œä½†ä¸ºäº†å®‰å…¨èµ·è§å†æ¬¡æ£€æŸ¥
            norm = np.linalg.norm(embedding)
            if norm > 0 and abs(norm - 1.0) > 1e-6:
                print(f"è­¦å‘Šï¼šè¿œç¨‹å‘é‡æœªå®Œå…¨å½’ä¸€åŒ–ï¼Œnorm={norm:.6f}ï¼Œé‡æ–°å½’ä¸€åŒ–")
                embedding = embedding / norm

            embeddings.append(embedding)

        print(f"âœ“ Qwen3-Embedding-8Bè°ƒç”¨æˆåŠŸï¼Œè¿”å› {len(embeddings)} ä¸ªå‘é‡")

        # è®°å½•APIè°ƒç”¨ç»Ÿè®¡
        if not hasattr(call_qwen3_embedding, 'stats'):
            call_qwen3_embedding.stats = {'total_calls': 0, 'total_texts': 0}
        call_qwen3_embedding.stats['total_calls'] += 1
        call_qwen3_embedding.stats['total_texts'] += len(text_list)

        return embeddings

    except requests.exceptions.RequestException as e:
        print(f"âŒ Qwen3-Embedding-8Bè°ƒç”¨å¤±è´¥: {e}")
        raise e
    except Exception as e:
        print(f"âŒ åµŒå…¥å‘é‡è®¡ç®—å¼‚å¸¸: {e}")
        raise e





def call_single_qwen3_embedding(text: str) -> np.ndarray:
    """
    è°ƒç”¨å•ä¸ªæ–‡æœ¬çš„Qwen3-Embedding-8BåµŒå…¥å‘é‡è®¡ç®—
    è¿™æ˜¯å¯¹æ‰¹é‡å‡½æ•°çš„ç®€å•åŒ…è£…

    å‚æ•°:
        text (str): éœ€è¦å‘é‡åŒ–çš„æ–‡æœ¬

    è¿”å›å€¼:
        np.ndarray: åµŒå…¥å‘é‡
    """
    results = call_qwen3_embedding([text])
    return results[0]


def print_qwen3_stats():
    """
    æ‰“å°Qwen3-Embedding-8B APIè°ƒç”¨ç»Ÿè®¡ä¿¡æ¯
    """
    if hasattr(call_qwen3_embedding, 'stats'):
        stats = call_qwen3_embedding.stats
        print(f"\nğŸ“ˆ Qwen3-Embedding-8B APIè°ƒç”¨ç»Ÿè®¡:")
        print(f"  - æ€»è°ƒç”¨æ¬¡æ•°: {stats['total_calls']}")
        print(f"  - æ€»å¤„ç†æ–‡æœ¬æ•°: {stats['total_texts']}")
        print(f"  - å¹³å‡æ¯æ¬¡è°ƒç”¨æ–‡æœ¬æ•°: {stats['total_texts'] / max(1, stats['total_calls']):.1f}")
    else:
        print("ğŸ“ˆ æœªè¿›è¡ŒQwen3-Embedding-8B APIè°ƒç”¨")


def safe_vector_norm(vector: np.ndarray) -> float:
    """
    å®‰å…¨è®¡ç®—å‘é‡çš„æ¨¡é•¿ï¼ŒåŒ…å«å¼‚å¸¸å¤„ç†

    å‚æ•°:
        vector (np.ndarray): è¾“å…¥å‘é‡

    è¿”å›å€¼:
        float: å‘é‡çš„æ¨¡é•¿
    """
    try:
        norm = np.linalg.norm(vector)
        return norm
    except Exception as e:
        print(f"linalg.norm è®¡ç®—å¤±è´¥: {e}ï¼Œä½¿ç”¨æ‰‹åŠ¨è®¡ç®—")
        # æ‰‹åŠ¨è®¡ç®—å‘é‡æ¨¡é•¿
        norm = 0.0
        for k in range(len(vector)):
            norm += vector[k] ** 2
        norm = math.sqrt(norm)
        print(f"æ‰‹åŠ¨è®¡ç®—å‘é‡æ¨¡é•¿: {norm}")
        return norm


def calculate_max_group_size(sents1: List[str], sents2: List[str]) -> int:
    """
    åŠ¨æ€è®¡ç®—æœ€å¤§ç»„åˆå¤§å°

    æ ¹æ®åŒè¯­å¥å­æ•°é‡åŠ¨æ€è®¡ç®—æœ€å¤§ç»„åˆå¤§å°ï¼š
    max_group_size = ceil(max(len(sents1), len(sents2)) / 2)

    å‚æ•°:
        sents1 (List[str]): æºè¯­è¨€å¥å­åˆ—è¡¨
        sents2 (List[str]): ç›®æ ‡è¯­è¨€å¥å­åˆ—è¡¨

    è¿”å›å€¼:
        int: è®¡ç®—å¾—åˆ°çš„æœ€å¤§ç»„åˆå¤§å°
    """
    max_sentences = max(len(sents1), len(sents2))
    max_group_size = math.ceil(max_sentences / 2)

    # è®¾ç½®åˆç†çš„æœ€å°å€¼å’Œæœ€å¤§å€¼
    max_group_size = max(2, max_group_size)  # æœ€å°ä¸º2
    max_group_size = min(max_sentences - 1, max_group_size)  # æœ€å¤§ä¸ºåŒè¯­æœ€å¤§å­å¥æ•°å‡1

    print(f"ğŸ“Š åŠ¨æ€è®¡ç®—æœ€å¤§ç»„åˆå¤§å°:")
    print(f"  - æºè¯­è¨€å¥å­æ•°: {len(sents1)}")
    print(f"  - ç›®æ ‡è¯­è¨€å¥å­æ•°: {len(sents2)}")
    print(f"  - æœ€å¤§å¥å­æ•°: {max_sentences}")
    print(f"  - è®¡ç®—å…¬å¼: ceil({max_sentences} / 2) = {math.ceil(max_sentences / 2)}")
    print(f"  - èŒƒå›´é™åˆ¶: æœ€å°ä¸º2ï¼Œæœ€å¤§ä¸º{max_sentences - 1}")
    print(f"  - æœ€ç»ˆæœ€å¤§ç»„åˆå¤§å°: {max_group_size}")

    return max_group_size


def safe_vector_normalize(vector: np.ndarray) -> np.ndarray:
    """
    å®‰å…¨å½’ä¸€åŒ–å‘é‡ï¼ŒåŒ…å«å¼‚å¸¸å¤„ç†

    å‚æ•°:
        vector (np.ndarray): è¾“å…¥å‘é‡

    è¿”å›å€¼:
        np.ndarray: å½’ä¸€åŒ–åçš„å‘é‡
    """
    norm = safe_vector_norm(vector)
    if norm > 0:
        return vector / norm
    else:
        print("è­¦å‘Šï¼šé›¶å‘é‡æ— æ³•å½’ä¸€åŒ–")
        return vector


def generate_allowed_groups(params: Dict[str, Any], sents1: List[str], sents2: List[str]) -> List[Tuple[int, int]]:
    """
    ç”Ÿæˆå…è®¸çš„å¥å­ç»„åˆæ¨¡å¼

    å‚æ•°:
        params (Dict[str, Any]): å‚æ•°å­—å…¸
        sents1 (List[str]): æºè¯­è¨€å¥å­åˆ—è¡¨
        sents2 (List[str]): ç›®æ ‡è¯­è¨€å¥å­åˆ—è¡¨

    è¿”å›å€¼:
        List[Tuple[int, int]]: å…è®¸çš„ç»„åˆåˆ—è¡¨ï¼Œå¦‚[(0,1), (1,0), (1,1), (1,2), (2,1), (2,2)]
    """
    print(f"\n=== ç”Ÿæˆå…è®¸çš„å¥å­ç»„åˆ ===")

    # åŠ¨æ€è®¡ç®—æœ€å¤§ç»„åˆå¤§å°
    max_group_size = calculate_max_group_size(sents1, sents2)

    allowed_groups = [(0, 1), (1, 0), (1, 1)]  # åŸºç¡€ç»„åˆï¼šç©ºå¯¹1ï¼Œ1å¯¹ç©ºï¼Œ1å¯¹1

    if not only_one_2_one_pairing:
        for i in range(2, max_group_size + 1):
            allowed_groups.append((1, i))  # 1å¯¹å¤š
            allowed_groups.append((i, 1))  # å¤šå¯¹1

        if params.get('noEmptyPair', False):
            if (1, 0) in allowed_groups:
                allowed_groups.remove((1, 0))  # ç§»é™¤ç©ºå¯¹é½
            if (0, 1) in allowed_groups:
                allowed_groups.remove((0, 1))

        if not params.get('no2_2Group', False):
            allowed_groups.append((2, 2))  # 2å¯¹2ç»„åˆ

    # å°†ç»“æœå­˜å‚¨åˆ°paramsä¸­ï¼ˆä¸å‚è€ƒä»£ç ä¸€è‡´ï¼‰
    params['allowedGroups'] = allowed_groups

    print(f"*** å…è®¸çš„å¥å­ç»„åˆ: {allowed_groups}")
    print(f"*** ä½¿ç”¨åŠ¨æ€è®¡ç®—çš„æœ€å¤§ç»„åˆå¤§å°: {max_group_size}")
    return allowed_groups


def lenPenalty(len1: float, len2: float) -> float:
    """
    æ ¹æ®å¥é•¿æ¯”ä¾‹è®¡ç®—é•¿åº¦æƒ©ç½šï¼Œå€¼åŸŸ [0,1]ï¼›é•¿åº¦è¶Šæ¥è¿‘æƒ©ç½šè¶Šå°ã€‚
    å‚è€ƒ Bertalign ç®—æ³•ã€‚

    å‚æ•°:
        len1 (float): ç¬¬ä¸€ä¸ªå¥å­çš„é•¿åº¦
        len2 (float): ç¬¬äºŒä¸ªå¥å­çš„é•¿åº¦

    è¿”å›å€¼:
        float: é•¿åº¦æƒ©ç½šå€¼ï¼ŒèŒƒå›´[0,1]
    """
    min_len = min(len1, len2)  # è¾ƒçŸ­é•¿åº¦
    max_len = max(len1, len2)  # è¾ƒé•¿é•¿åº¦
    if max_len == 0:
        return 0
    return 1 - np.log2(1 + min_len / max_len)  # å¯¹æ•°æƒ©ç½šå‡½æ•°


def len_penalty(len1: float, len2: float) -> float:
    """
    æ ¹æ®å¥é•¿æ¯”ä¾‹è®¡ç®—é•¿åº¦æƒ©ç½šï¼Œå€¼åŸŸ [0,1]ï¼›é•¿åº¦è¶Šæ¥è¿‘æƒ©ç½šè¶Šå°
    å‚è€ƒ Bertalign ç®—æ³•
    
    å‚æ•°:
        len1 (float): ç¬¬ä¸€ä¸ªå¥å­çš„é•¿åº¦
        len2 (float): ç¬¬äºŒä¸ªå¥å­çš„é•¿åº¦
    
    è¿”å›å€¼:
        float: é•¿åº¦æƒ©ç½šå€¼ï¼ŒèŒƒå›´[0,1]
    """
    min_len = min(len1, len2)
    max_len = max(len1, len2)
    if max_len == 0:
        return 0
    return 1 - np.log2(1 + min_len / max_len)


def distance_dtw(sents1: List[str], sents2: List[str], sim_mat: np.ndarray,
                embeds1: List[np.ndarray], embeds2: List[np.ndarray],
                inf_i: int, i: int, inf_j: int, j: int, char_ratio: float,
                dist_hash: Dict[str, float], params: Dict[str, Any], use_coeff: bool = True) -> float:
    """
    è®¡ç®—æºåŒºé—´ (inf_i,i] ä¸ç›®æ ‡åŒºé—´ (inf_j,j] çš„åŠ æƒè·ç¦»
    å®Œæ•´å¤ç°å‚è€ƒä»£ç çš„distance_dtwå‡½æ•°é€»è¾‘

    å‚æ•°:
        sents1 (List[str]): æºè¯­è¨€å¥å­åˆ—è¡¨
        sents2 (List[str]): ç›®æ ‡è¯­è¨€å¥å­åˆ—è¡¨
        sim_mat (np.ndarray): ç›¸ä¼¼åº¦çŸ©é˜µ
        embeds1 (List[np.ndarray]): æºè¯­è¨€åµŒå…¥å‘é‡åˆ—è¡¨
        embeds2 (List[np.ndarray]): ç›®æ ‡è¯­è¨€åµŒå…¥å‘é‡åˆ—è¡¨
        inf_i (int): æºåŒºé—´èµ·å§‹åæ ‡
        i (int): æºåŒºé—´ç»“æŸåæ ‡
        inf_j (int): ç›®æ ‡åŒºé—´èµ·å§‹åæ ‡
        j (int): ç›®æ ‡åŒºé—´ç»“æŸåæ ‡
        char_ratio (float): å­—ç¬¦æ¯”ä¾‹
        dist_hash (Dict[str, float]): è·ç¦»ç¼“å­˜
        params (Dict[str, Any]): å‚æ•°å­—å…¸
        use_coeff (bool): æ˜¯å¦ä½¿ç”¨ç³»æ•°

    è¿”å›å€¼:
        float: è®¡ç®—å¾—åˆ°çš„åŠ æƒè·ç¦»
    """
    # å¦‚æœè·ç¦»å·²å­˜å‚¨åœ¨dist_hashä¸­ï¼Œç›´æ¥è¿”å›
    key = f"{inf_i}-{i};{inf_j}-{j}"
    if key in dist_hash:
        return dist_hash[key]

    # coeffè¡¨ç¤ºå¯¹é½ä¸­æ¶‰åŠçš„æ€»æ®µæ•°ï¼ˆä¸¤ç§è¯­è¨€ï¼‰
    coeff = 1
    penalty = params.get('penalty_n_n', 0.06)  # å¤šå¯¹å¤šå¯¹é½æƒ©ç½š

    # 1-0å’Œ0-1å…³ç³»çš„æƒ…å†µï¼ˆç©ºå¯¹é½ï¼‰
    if inf_i == i or inf_j == j:
        dist_null = params.get('distNull', 1.0) * coeff
        dist_hash[key] = dist_null
        return dist_null

    if i < 0 or j < 0 or inf_i < -2 or inf_j < -2:
        dist_hash[key] = INFINITE
        return INFINITE

    coeff = 2

    # è®¡ç®—ç›¸ä¼¼åº¦
    if params.get('useEncoder', True) and embeds1 is not None and embeds2 is not None:
        # ä½¿ç”¨åµŒå…¥å‘é‡è®¡ç®—ç›¸ä¼¼åº¦
        # 1-1å…³ç³»çš„æƒ…å†µ
        if inf_i == i - 1 and inf_j == j - 1:
            sim = sim_mat[i, j]
            if use_coeff:
                penalty = 0
        # n-nå…³ç³»çš„æƒ…å†µ
        else:
            # è®¡ç®—embed_iï¼ˆæºè¯­è¨€åµŒå…¥å‘é‡ï¼‰
            if inf_i == i - 1:
                # å•ä¸ªå¥å­ï¼Œç›´æ¥ä½¿ç”¨é¢„è®¡ç®—çš„åµŒå…¥å‘é‡
                embed_i = embeds1[i].copy()
                len_i = len(sents1[inf_i + 1])
            else:
                # å¤šä¸ªå¥å­ï¼Œéœ€è¦æ‹¼æ¥æ–‡æœ¬å¹¶åŠ¨æ€è°ƒç”¨Qwen3-Embedding-8Bè®¡ç®—åµŒå…¥å‘é‡
                source_sentences = []
                sent_i = sents1[inf_i + 1]
                source_sentences.append(f"[{inf_i + 1}] {sent_i}")
                for coord_i in range(inf_i + 2, i + 1):
                    sentence = sents1[coord_i]
                    source_sentences.append(f"[{coord_i}] {sentence}")
                    sent_i += " " + sentence  # ä½¿ç”¨ç©ºæ ¼ä½œä¸ºæ‹¼æ¥åˆ†éš”ç¬¦
                    if use_coeff:
                        coeff += 1
                len_i = len(sent_i)

                # åŠ¨æ€è°ƒç”¨Qwen3-Embedding-8Bæ¨¡å‹è®¡ç®—ç»„åˆå¥å­çš„åµŒå…¥å‘é‡
                embed_i = call_single_qwen3_embedding(sent_i)
                if params.get('veryVerbose', False):
                    print(f"    ğŸ”— ç»„åˆæºæ–‡æœ¬ (å¥å­ {inf_i + 1}-{i}):")
                    for sent_info in source_sentences:
                        print(f"      {sent_info}")
                    print(f"    ğŸ“ ç»„åˆæºæ–‡æœ¬å®Œæ•´å†…å®¹: '{sent_i}'")
                    print(f"    ğŸ“Š ç»„åˆæºæ–‡æœ¬å‘é‡ç»´åº¦: {embed_i.shape}")

            # è®¡ç®—embed_jï¼ˆç›®æ ‡è¯­è¨€åµŒå…¥å‘é‡ï¼‰
            if inf_j == j - 1:
                # å•ä¸ªå¥å­ï¼Œç›´æ¥ä½¿ç”¨é¢„è®¡ç®—çš„åµŒå…¥å‘é‡
                embed_j = embeds2[j].copy()
                len_j = len(sents2[inf_j + 1])
            else:
                # å¤šä¸ªå¥å­ï¼Œéœ€è¦æ‹¼æ¥æ–‡æœ¬å¹¶åŠ¨æ€è°ƒç”¨Qwen3-Embedding-8Bè®¡ç®—åµŒå…¥å‘é‡
                target_sentences = []
                sent_j = sents2[inf_j + 1]
                target_sentences.append(f"[{inf_j + 1}] {sent_j}")
                for coord_j in range(inf_j + 2, j + 1):
                    sentence = sents2[coord_j]
                    target_sentences.append(f"[{coord_j}] {sentence}")
                    sent_j += " " + sentence  # ä½¿ç”¨ç©ºæ ¼ä½œä¸ºæ‹¼æ¥åˆ†éš”ç¬¦
                    if use_coeff:
                        coeff += 1
                len_j = len(sent_j)

                # åŠ¨æ€è°ƒç”¨Qwen3-Embedding-8Bæ¨¡å‹è®¡ç®—ç»„åˆå¥å­çš„åµŒå…¥å‘é‡
                embed_j = call_single_qwen3_embedding(sent_j)
                if params.get('veryVerbose', False):
                    print(f"    ğŸ”— ç»„åˆç›®æ ‡æ–‡æœ¬ (å¥å­ {inf_j + 1}-{j}):")
                    for sent_info in target_sentences:
                        print(f"      {sent_info}")
                    print(f"    ğŸ“ ç»„åˆç›®æ ‡æ–‡æœ¬å®Œæ•´å†…å®¹: '{sent_j}'")
                    print(f"    ğŸ“Š ç»„åˆç›®æ ‡æ–‡æœ¬å‘é‡ç»´åº¦: {embed_j.shape}")

            sim = float(np.matmul(embed_i, np.transpose(embed_j)))
    else:
        # ä½¿ç”¨é¢„è®¡ç®—çš„ç›¸ä¼¼åº¦çŸ©é˜µ
        # 1-1å…³ç³»çš„æƒ…å†µï¼šæ— æƒ©ç½š
        if inf_i == i - 1 and inf_j == j - 1:
            sim = sim_mat[i, j]
            penalty = 0
        else:
            # n-nå…³ç³»çš„æƒ…å†µ - è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦
            sim_sum = 0
            count = 0
            for x in range(inf_i + 1, i + 1):
                for y in range(inf_j + 1, j + 1):
                    if 0 <= x < len(sents1) and 0 <= y < len(sents2):
                        sim_sum += sim_mat[x, y]
                        count += 1
                        if use_coeff:
                            coeff += 1
            sim = sim_sum / count if count > 0 else 0

        # è®¡ç®—å¥é•¿
        len_i = sum(len(sents1[x]) for x in range(max(0, inf_i + 1), min(i + 1, len(sents1))))
        len_j = sum(len(sents2[y]) for y in range(max(0, inf_j + 1), min(j + 1, len(sents2))))

    # è®¡ç®—å¥é•¿ï¼ˆå¦‚æœè¿˜æ²¡æœ‰è®¡ç®—ï¼‰
    if 'len_i' not in locals():
        len_i = sum(len(sents1[x]) for x in range(max(0, inf_i + 1), min(i + 1, len(sents1))))
    if 'len_j' not in locals():
        len_j = sum(len(sents2[y]) for y in range(max(0, inf_j + 1), min(j + 1, len(sents2))))

    # è®¡ç®—ä¸é‚»è¿‘å¥å­çš„ç›¸ä¼¼åº¦å¹¶ä»å…¨å±€ç›¸ä¼¼åº¦ä¸­å‡å»ï¼ˆé‚»è¿‘å¥æƒ©ç½šï¼‰
    if (not params.get('noMarginPenalty', False) and embeds1 is not None and embeds2 is not None and
        'embed_i' in locals() and 'embed_j' in locals()):
        nb = 0  # é‚»å±…è®¡æ•°å™¨
        nn = 0  # æœ‰æ•ˆé‚»å±…ç»„æ•°

        # è®¡ç®—ç›®æ ‡è¯­è¨€é‚»è¿‘å¥ç›¸ä¼¼åº¦
        if inf_j >= 0 and inf_j < len(embeds2):
            left_embed_j = embeds2[inf_j].copy()  # å·¦ä¾§é‚»è¿‘å¥åµŒå…¥å‘é‡
            left_sim_j = np.matmul(embed_i, np.transpose(left_embed_j))
            nb += 1
        else:
            left_sim_j = 0

        if j + 1 < len(embeds2):
            right_embed_j = embeds2[j + 1].copy()  # å³ä¾§é‚»è¿‘å¥åµŒå…¥å‘é‡
            right_sim_j = np.matmul(embed_i, np.transpose(right_embed_j))
            nb += 1
        else:
            right_sim_j = 0

        neighbour_sim_j = 0  # ç›®æ ‡è¯­è¨€é‚»è¿‘å¥ç›¸ä¼¼åº¦
        if nb > 0:
            neighbour_sim_j = (left_sim_j + right_sim_j) / nb
            nn += 1

        # è®¡ç®—æºè¯­è¨€é‚»è¿‘å¥ç›¸ä¼¼åº¦
        nb = 0
        if inf_i >= 0 and inf_i < len(embeds1):
            left_embed_i = embeds1[inf_i].copy()  # å·¦ä¾§é‚»è¿‘å¥åµŒå…¥å‘é‡
            left_sim_i = np.matmul(left_embed_i, np.transpose(embed_j))
            nb += 1
        else:
            left_sim_i = 0

        if i + 1 < len(embeds1):
            right_embed_i = embeds1[i + 1].copy()  # å³ä¾§é‚»è¿‘å¥åµŒå…¥å‘é‡
            right_sim_i = np.matmul(right_embed_i, np.transpose(embed_j))
            nb += 1
        else:
            right_sim_i = 0

        neighbour_sim_i = 0  # æºè¯­è¨€é‚»è¿‘å¥ç›¸ä¼¼åº¦
        if nb > 0:
            neighbour_sim_i = (left_sim_i + right_sim_i) / nb
            nn += 1

        # è®¡ç®—å¹³å‡é‚»è¿‘å¥ç›¸ä¼¼åº¦å¹¶åº”ç”¨æƒ©ç½š
        average_neighbour_sim = 0  # å¹³å‡é‚»è¿‘å¥ç›¸ä¼¼åº¦
        if nn > 0:
            average_neighbour_sim = (neighbour_sim_i + neighbour_sim_j) / nn
        sim -= coeff_neighbour_sim * average_neighbour_sim

    # å¤„ç†ç©ºå¥å­çš„æƒ…å†µ
    if len_i * len_j == 0:
        dist_null = params.get('distNull', 1.0) * coeff
        dist_hash[key] = dist_null
        return dist_null

    # è®¡ç®—æœ€ç»ˆè·ç¦»
    dist = 1 - sim  # è·ç¦» = 1 - ç›¸ä¼¼åº¦
    if use_coeff:
        dist += penalty * coeff  # æ·»åŠ æƒ©ç½šé¡¹

    # ç»“åˆå¥é•¿æƒ©ç½šå’Œç›¸ä¼¼åº¦è·ç¦»
    dist = (1 - coeff_sent_len) * dist + coeff_sent_len * lenPenalty(len_i * char_ratio, len_j)

    dist *= coeff  # ä¹˜ä»¥ç³»æ•°
    dist_hash[key] = dist  # ç¼“å­˜ç»“æœ
    return dist


def dtw(sents1: List[str], sents2: List[str], sim_mat: np.ndarray,
        embeds1: List[np.ndarray], embeds2: List[np.ndarray],
        path_hash: Dict[Tuple[int, int], List], dist_hash: Dict[str, float],
        x_2_y: Dict[int, int], y_2_x: Dict[int, int],
        x_begin: int, y_begin: int, x_end: int, y_end: int,
        char_ratio: float, allowed_groups: List[Tuple[int, int]],
        params: Dict[str, Any]) -> Tuple[List[List[int]], float]:
    """
    åœ¨æŒ‡å®šåŒºé—´å†…é€’æ¨è®¡ç®—å±€éƒ¨æœ€ä¼˜è·¯å¾„
    å®Œæ•´å¤ç°å‚è€ƒä»£ç çš„dtwå‡½æ•°é€»è¾‘

    å‚æ•°:
        sents1 (List[str]): æºè¯­è¨€å¥å­åˆ—è¡¨
        sents2 (List[str]): ç›®æ ‡è¯­è¨€å¥å­åˆ—è¡¨
        sim_mat (np.ndarray): ç›¸ä¼¼åº¦çŸ©é˜µ
        embeds1 (List[np.ndarray]): æºè¯­è¨€åµŒå…¥å‘é‡åˆ—è¡¨
        embeds2 (List[np.ndarray]): ç›®æ ‡è¯­è¨€åµŒå…¥å‘é‡åˆ—è¡¨
        path_hash (Dict): è·¯å¾„å“ˆå¸Œè¡¨
        dist_hash (Dict): è·ç¦»å“ˆå¸Œè¡¨
        x_2_y (Dict): xåˆ°yçš„é”šç‚¹æ˜ å°„
        y_2_x (Dict): yåˆ°xçš„é”šç‚¹æ˜ å°„
        x_begin (int): xèµ·å§‹åæ ‡
        y_begin (int): yèµ·å§‹åæ ‡
        x_end (int): xç»“æŸåæ ‡
        y_end (int): yç»“æŸåæ ‡
        char_ratio (float): å­—ç¬¦æ¯”ä¾‹
        allowed_groups (List): å…è®¸çš„ç»„åˆåˆ—è¡¨
        params (Dict): å‚æ•°å­—å…¸

    è¿”å›å€¼:
        Tuple[List[List[int]], float]: (æœ€ä¼˜è·¯å¾„, ç´¯ç§¯è·ç¦»)
    """
    if params.get('veryVerbose', False):
        print(f"    DTWè®¡ç®—åŒºé—´: ({x_begin},{y_begin}) -> ({x_end},{y_end})")

    for i in range(x_begin, x_end + 1):
        for j in range(y_begin, y_end + 1):
            # è·¯å¾„å“ˆå¸Œè¡¨è®°å½•å·²è®¡ç®—è·¯å¾„çš„ç»“æœï¼Œä»¥å‡å°‘é€’å½’
            dtw_key = (i, j)

            # å¦‚æœå·²è®¡ç®—è¿‡åˆ™è·³è¿‡
            if dtw_key in path_hash:
                continue

            path_by_group = {}  # å„ç»„çš„è·¯å¾„è®°å½•
            dist_by_group = {}  # å„ç»„çš„è·ç¦»è®°å½•

            # æ£€æŸ¥æ¯ä¸ªå…è®¸çš„ç»„åˆ
            for group in params['allowedGroups']:
                previous_i = i - group[0]  # å‰ä¸€ä¸ªiåæ ‡
                previous_j = j - group[1]  # å‰ä¸€ä¸ªjåæ ‡
                previous_key = (previous_i, previous_j)  # å‰ä¸€ä¸ªç‚¹çš„é”®

                # åŸåˆ™ä¸Šprevious_keyåº”è¯¥èƒ½æ‰¾åˆ°
                if previous_key in path_hash:
                    (path_by_group[group], dist_by_group[group]) = path_hash[previous_key]
                else:
                    (path_by_group[group], dist_by_group[group]) = ([], INFINITE)

                # ä¸ºå½“å‰ç»„å¢åŠ è·ç¦»
                dist_by_group[group] += distance_dtw(sents1, sents2, sim_mat, embeds1, embeds2,
                                                   previous_i, i, previous_j, j,
                                                   char_ratio, dist_hash, params)

            best_group = None  # æœ€ä½³ç»„åˆ
            min_dist = INFINITE  # æœ€å°è·ç¦»
            for group in params['allowedGroups']:
                if dist_by_group[group] < min_dist:
                    min_dist = dist_by_group[group]
                    best_group = group

            if best_group is not None:
                path = path_by_group[best_group][:]  # æ³¨æ„ï¼šè¿™é‡Œåˆ›å»ºå‰¯æœ¬ï¼
                path.append([i, j])
                path_hash[dtw_key] = [path, min_dist]
            else:
                path_hash[dtw_key] = [[], INFINITE]

    return path_hash[(x_end, y_end)]


def run_dtw(interval_data: Dict[str, Any], sim_mat: np.ndarray,
           sents1: List[str], sents2: List[str], embeds1: List[np.ndarray], embeds2: List[np.ndarray],
           params: Dict[str, Any]) -> Tuple[List[List[int]], List[List[int]], float]:
    """
    åœ¨é”šç‚¹/åŒºé—´çº¦æŸä¸‹æ‰§è¡ŒåŠ¨æ€æ—¶é—´è§„æ•´ï¼ˆDTWï¼‰ç®—æ³•
    å®Œæ•´å¤ç°å‚è€ƒä»£ç çš„run_dtwå‡½æ•°é€»è¾‘ï¼ŒåŒ…æ‹¬æŸæœç´¢ã€é”šç‚¹çº¦æŸç­‰

    å‚æ•°:
        interval_data (Dict): åŒºé—´åˆ’åˆ†æ•°æ®
        sim_mat (np.ndarray): ç›¸ä¼¼åº¦çŸ©é˜µ
        sents1 (List[str]): æºè¯­è¨€å¥å­åˆ—è¡¨
        sents2 (List[str]): ç›®æ ‡è¯­è¨€å¥å­åˆ—è¡¨
        embeds1 (List[np.ndarray]): æºè¯­è¨€åµŒå…¥å‘é‡åˆ—è¡¨
        embeds2 (List[np.ndarray]): ç›®æ ‡è¯­è¨€åµŒå…¥å‘é‡åˆ—è¡¨
        params (Dict): å‚æ•°å­—å…¸

    è¿”å›å€¼:
        Tuple[List[List[int]], List[List[int]], float]: (xè·¯å¾„, yè·¯å¾„, æ€»å¾—åˆ†)
    """
    print(f"\n=== å¼€å§‹æ‰§è¡ŒDTWç®—æ³• ===")

    # è·å–åŒºé—´å’Œé”šç‚¹ä¿¡æ¯
    intervals = interval_data['alignable_intervals']['intervals']
    final_anchors = interval_data['final_anchors']['anchors']

    print(f"å¯å¯¹é½åŒºé—´æ•°é‡: {len(intervals)}")
    print(f"æœ€ç»ˆé”šç‚¹æ•°é‡: {len(final_anchors)}")

    # è·å–å·²ç”Ÿæˆçš„å…è®¸ç»„åˆï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰
    allowed_groups = params.get('allowedGroups', [])

    # æ„å»ºå…¨å±€é”šç‚¹æ˜ å°„ï¼ˆä»åŒºé—´æ•°æ®ä¸­æå–ï¼‰
    filtered_x = []
    filtered_y = []
    # æ³¨æ„ï¼šå·²å»é™¤é¢„å®šä¹‰é”šç‚¹åŠŸèƒ½ï¼Œåªä½¿ç”¨åŠ¨æ€æå–çš„é”šç‚¹

    for anchor in final_anchors:
        x, y = anchor['coordinates']
        filtered_x.append(x)
        filtered_y.append(y)

    # è®¡ç®—å­—ç¬¦æ¯”ä¾‹
    nb_chars1 = sum(len(sent) for sent in sents1)
    nb_chars2 = sum(len(sent) for sent in sents2)
    char_ratio = nb_chars2 / nb_chars1 if nb_chars1 > 0 else 1.0

    print(f"å­—ç¬¦æ¯”ä¾‹: {char_ratio:.3f}")

    # åˆå§‹åŒ–DTWæ•°æ®ç»“æ„
    path_hash = {}  # è·¯å¾„å“ˆå¸Œè¡¨ï¼Œè®°å½•åˆ°è¾¾æ¯ä¸ªç‚¹çš„æœ€ä½³è·¯å¾„
    dist_hash = {"-2--1;-2--1": 0.0}  # å¯¹äºç‚¹(-1,-1)çš„ä¸‹ç•Œï¼Œè·ç¦»ä¸º0

    # åˆå§‹åŒ–ç©ºè·¯å¾„ - è¿™æ˜¯å…³é”®æ­¥éª¤ï¼
    x_first = intervals[0]['coordinates']['start'][0]  # ç¬¬ä¸€ä¸ªåŒºé—´çš„èµ·å§‹xåæ ‡
    y_first = intervals[0]['coordinates']['start'][1]  # ç¬¬ä¸€ä¸ªåŒºé—´çš„èµ·å§‹yåæ ‡

    path_hash[(x_first, y_first)] = [[[-1, -1]], 0]  # åˆå§‹è·¯å¾„å’Œå¾—åˆ†
    print(f"é¦–ä¸ªå¯¹é½ç‚¹: {x_first}-{y_first}")
    print(f"åˆå§‹åŒ– DTW: èµ·ç‚¹ {intervals[0]['coordinates']['start']} ç»ˆç‚¹ {intervals[-1]['coordinates']['end']}")

    # è½¬æ¢åŒºé—´æ ¼å¼ä»¥åŒ¹é…å‚è€ƒä»£ç 
    converted_intervals = []
    for interval in intervals:
        start_coords = interval['coordinates']['start']
        end_coords = interval['coordinates']['end']
        converted_intervals.append([start_coords, end_coords])

    lastBestPath = [[x_first - 1, y_first - 1]]  # ä¸Šä¸€ä¸ªæœ€ä½³è·¯å¾„
    lastBestScore = 0  # ä¸Šä¸€ä¸ªæœ€ä½³å¾—åˆ†

    t8 = time.time()

    # å¤„ç†æ¯ä¸ªå¯å¯¹é½åŒºé—´
    for interval in converted_intervals:
        (x_begin, y_begin) = interval[0]  # åŒºé—´èµ·å§‹åæ ‡
        (x_end, y_end) = interval[1]      # åŒºé—´ç»“æŸåæ ‡
        print(f"æ­£åœ¨å¤„ç†åŒºé—´ {interval}")
        key_xy = (x_begin, y_begin)
        coeff_y_per_x = (y_end - y_begin) / (x_end - x_begin) if (x_end - x_begin) != 0 else 1  # Yè½´ç›¸å¯¹Xè½´çš„æ–œç‡

        # è¿™äº›å­—å…¸ç”¨äºå¼•å¯¼è·¯å¾„é è¿‘ä½äºåŒºé—´å†…çš„é”šç‚¹
        x_2_y = {}  # Xåæ ‡åˆ°Yåæ ‡çš„æ˜ å°„
        y_2_x = {}  # Yåæ ‡åˆ°Xåæ ‡çš„æ˜ å°„
        anchor_count = 0
        for i in range(len(filtered_x)):
            x = filtered_x[i]
            y = filtered_y[i]
            if x < x_begin:
                continue
            if x > x_end:
                break
            if y < y_begin or y > y_end:
                continue
            x_2_y[x] = y
            y_2_x[y] = x
            anchor_count += 1

        print(f"*** åŒºé—´å†…é”šç‚¹æ•°é‡: {anchor_count}")

        # è¿æ¥å¯å¯¹é½åŒºé—´ä¹‹é—´çš„ç©ºéš™
        # å¦‚æœè·¯å¾„ä¸­æœ€åä¸€ä¸ªç‚¹ä¸å½“å‰åŒºé—´ç¬¬ä¸€ä¸ªç‚¹ä¹‹é—´æœ‰ç©ºéš™ï¼Œåœ¨è·¯å¾„ä¸­æ·»åŠ ç©ºç‚¹()
        if key_xy not in path_hash:
            (lastI, lastJ) = lastBestPath[-1]
            if params.get('verbose', False):
                print(f"åœ¨è·¯å¾„ä¸­æ’å…¥ç©ºéš™: ({lastI},{lastJ}) â†’ ({x_begin},{y_begin})")
            lastBestPath.append(())  # ç©ºç‚¹è¡¨ç¤ºè·¯å¾„ä¸­çš„æ–­ç‚¹
            lastBestPath.append((x_begin - 1, y_begin - 1))
            path_hash[key_xy] = [lastBestPath, lastBestScore]

        # ç°åœ¨åœ¨åŒºé—´å†…æ¯ä¸ªé”šç‚¹ä¹‹é—´è¿è¡ŒDTWæœç´¢
        # è·¯å¾„æ˜¯é€’å½’è®¡ç®—çš„ï¼Œä½†ä¸ºäº†æœ€å°åŒ–é€’å½’æ·±åº¦ï¼Œ
        # DTWå“ˆå¸Œé€šè¿‡é€ç‚¹è°ƒç”¨å‡½æ•°é€æ­¥å¡«å……
        previous_x = x_begin  # å‰ä¸€ä¸ªå¤„ç†çš„xåæ ‡
        previous_y = y_begin  # å‰ä¸€ä¸ªå¤„ç†çš„yåæ ‡
        processed_anchors = 0

        for x in range(x_begin, x_end + 1):
            localBeam = params.get('dtwBeam', 7)  # å±€éƒ¨æœç´¢æŸå®½ï¼ˆä¼˜åŒ–åé»˜è®¤å€¼ï¼‰

            # å¦‚æœ(x,y)æ˜¯é”šç‚¹ï¼Œä»xå¼€å§‹è¿è¡ŒDTWï¼
            if x in x_2_y:
                y = x_2_y[x]
                if params.get('verbose', False):
                    print(f"å¤„ç†é”šç‚¹ ({x},{y})")

                # æ³¨æ„ï¼šå·²å»é™¤é¢„å®šä¹‰é”šç‚¹åŠŸèƒ½ï¼Œæ‰€æœ‰é”šç‚¹éƒ½ä½¿ç”¨ç›¸åŒçš„å±€éƒ¨æŸå®½
                # è®¡ç®—åå·®å’ŒæŸå®½
                # å¦‚æœ(x,y)è·ç¦»åŒºé—´å¯¹è§’çº¿å¤ªè¿œï¼Œå°†è¢«ä¸¢å¼ƒ
                deviation = 0
                if y >= y_begin and (x_end - x_begin) * (y_end - y_begin) != 0:
                    deviation = abs((y - y_begin) / (y_end - y_begin) - (x - x_begin) / (x_end - x_begin))
                else:
                    continue

                # ç¬¬ä¸€ä¸ªæ¡ä»¶ï¼šåå·® > localDiagBeam
                if (deviation > params.get('localDiagBeam', 0.35) and
                    deviation * (y_end - y_begin) > params.get('dtwBeam', 7)):
                    del x_2_y[x]
                    if y in y_2_x:
                        del y_2_x[y]
                    if params.get('verbose', False):
                        print(f"*** åå·®è¿‡å¤§ï¼š{deviation * (y_end - y_begin):.3f} - é”šç‚¹ ({x},{y}) è·ç¦»åŒºé—´å¯¹è§’çº¿å¤ªè¿œï¼Œå·²ä¸¢å¼ƒ")
                        continue

                    # ç¬¬äºŒä¸ªæ¡ä»¶ï¼šdeltaXå’ŒdeltaYä¹‹é—´çš„æ¯”ä¾‹è¶…è¿‡4ï¼ˆæœ€å¤§å…è®¸1-4æˆ–4-1åˆ†ç»„ï¼‰
                    if (params.get('noEmptyPair', False) and (
                            min(y - previous_y, x - previous_x) == 0 or
                            max(y - previous_y, x - previous_x) / min(y - previous_y, x - previous_x) > 4)):
                        del x_2_y[x]
                        if y in y_2_x:
                            del y_2_x[y]
                        if params.get('verbose', False):
                            print(f"*** åå·®é”šç‚¹ ({x},{y}) è·ç¦»å‰ä¸€ä¸ªç‚¹å¤ªè¿‘ï¼Œå·²ä¸¢å¼ƒ")
                        continue

                    # å¤„ç†ç©ºéš™ï¼ˆè€ƒè™‘éå•è°ƒæ€§ï¼‰ï¼š
                    # å¦‚æœ y < previous_yï¼Œæ‰©å¤§åŒºåŸŸï¼šå°†yè®¾ä¸ºprevious_yï¼Œå‡å°‘previous_xï¼Œ
                    # å¯¹åº”åˆ°x_2_y[prev_x] < yçš„æœ€åä¸€ä¸ªç‚¹

                    if y < previous_y:
                        print(f"*** å•è°ƒæ€§åå·®ï¼šy={y} < previous_y={previous_y}ï¼Œé‡æ–°è®¡ç®— previous_x")
                        prev_x = previous_x
                        # æ ¹æ®yæŸ¥æ‰¾å‰ä¸€ä¸ªç‚¹
                        found = False
                        while prev_x > x_begin:
                            prev_x -= 1
                            if prev_x in x_2_y:
                                prev_y = x_2_y[prev_x]
                                if prev_y < y:
                                    y = previous_y
                                    previous_x = prev_x
                                    previous_y = prev_y
                                    found = True
                                    break
                        if not found:
                            y = previous_y
                            previous_x = x_begin
                            previous_y = y_begin

                # è®¡ç®—ä¸‹ç•Œå€¼ä»¥ç»™å®šåŒºé—´æ¥åˆ‡æ–­é€’å½’ï¼šx_inf,y_infä¹‹å‰çš„ç‚¹ä¸äºˆè€ƒè™‘
                x_inf = previous_x - localBeam
                y_inf = previous_y - localBeam

                if params.get('veryVerbose', False):
                    print(f"å¯åŠ¨ DTW å­æœç´¢: ({max(x_begin, x_inf)},{max(y_begin, y_inf)}) â†’ ({x},{y})")

                print(f"å¯åŠ¨ DTW å­æœç´¢: ({max(x_begin, x_inf)},{max(y_begin, y_inf)}) â†’ ({x},{y})")
                (path, dist) = dtw(sents1, sents2, sim_mat, embeds1, embeds2, path_hash, dist_hash, x_2_y, y_2_x,
                                   max(x_begin, x_inf), max(y_begin, y_inf), x, y, char_ratio, allowed_groups, params)

                if dist == INFINITE and params.get('verbose', False):
                    print(f"ä»ç‚¹ ({x},{y}) èµ·è·¯å¾„ä¸å¯è¾¾ï¼Œè·ç¦»ä¸º âˆ")
                    # ä»x,yå¼€å§‹åˆå§‹åŒ–æ–°åŒºé—´
                    x_begin = x
                    y_begin = y
                    key_xy = (x_begin, y_begin)
                    # åœ¨æ­¤åˆ›å»ºlastBestPathçš„å‰¯æœ¬ï¼Œå¹¶æ·»åŠ æ–­ç‚¹
                    lastBestPath = lastBestPath[:]
                    lastBestPath.append(())  # ç©ºç‚¹è¡¨ç¤ºè·¯å¾„ä¸­çš„æ–­ç‚¹
                    lastBestPath.append((x_begin - 1, y_begin - 1))
                    path_hash[key_xy] = [lastBestPath, lastBestScore]
                else:
                    lastBestPath = path
                    lastBestScore = dist

                if params.get('veryVerbose', False):
                    print(f"å½“å‰è·¯å¾„è·ç¦» = {dist}")
                previous_x = x
                previous_y = y

        (lastBestPath, lastBestScore) = path_hash[(previous_x, previous_y)]

    # ä¸æ–‡æœ¬æœ«å°¾è¿›è¡Œè¿æ¥
    last_x = len(sents1) - 1
    last_y = len(sents2) - 1
    if (last_x - previous_x) + (last_y - previous_y) < 200:
        if params.get('verbose', False):
            print(f"æœ«å°¾å¯¹é½ç‚¹ ({last_x},{last_y})")
        dtw(sents1, sents2, sim_mat, embeds1, embeds2, path_hash, dist_hash, x_2_y, y_2_x,
            previous_x, previous_y, last_x, last_y, char_ratio, allowed_groups, params)

    # å¦‚æœæœ€åä¸€ä¸ªç‚¹æœªè¢«ä¸¢å¼ƒ
    score = INFINITE
    if (last_x, last_y) in path_hash:
        (best_path, score) = path_hash[(last_x, last_y)]
    # å¦åˆ™ä½¿ç”¨æœ€åä¸€ä¸ªåŒºé—´
    if score == INFINITE:
        (best_path, score) = path_hash[(previous_x, previous_y)]

    t9 = time.time()
    if params.get('verbose', False):
        print(f"\n9. Elapsed time for complete DTW-->", t9 - t8, "s.\n")

    print(f"\nâœ“ DTWç®—æ³•å®Œæˆï¼Œæ€»å¾—åˆ†: {score:.4f}")
    print(f"å®Œæ•´è·¯å¾„: {best_path}")

    # è½¬æ¢è·¯å¾„æ ¼å¼ä¸ºåˆ†ç»„å¯¹é½
    # å‚è€ƒä»£ç çš„è·¯å¾„æ ¼å¼ï¼šæ¯ä¸ªç‚¹è¡¨ç¤ºä¸€ä¸ªåŒºé—´çš„ä¸Šç•Œ
    # ä¾‹å¦‚ [[-1,-1],[0,1],[3,2]] è¡¨ç¤ºå¯¹é½ï¼š(0:0,1), (1,2,3:2)
    x_dtw = []
    y_dtw = []

    # å¤„ç†è·¯å¾„ï¼Œå°†è¿ç»­çš„åæ ‡è½¬æ¢ä¸ºåˆ†ç»„
    prev_x = -1
    prev_y = -1

    for i, point in enumerate(best_path):
        if i == 0 and point == [-1, -1]:
            # è·³è¿‡èµ·å§‹è™šæ‹Ÿç‚¹
            prev_x = -1
            prev_y = -1
            continue
        elif point == ():
            # é‡åˆ°æ–­ç‚¹ï¼Œè·³è¿‡
            continue
        elif len(point) == 2 and point[0] >= 0 and point[1] >= 0:
            # æœ‰æ•ˆå¯¹é½ç‚¹ï¼Œåˆ›å»ºä»å‰ä¸€ä¸ªç‚¹åˆ°å½“å‰ç‚¹çš„åˆ†ç»„
            curr_x = point[0]
            curr_y = point[1]

            # åˆ›å»ºxåˆ†ç»„ï¼šä»prev_x+1åˆ°curr_x
            x_group = []
            for x in range(prev_x + 1, curr_x + 1):
                if 0 <= x < len(sents1):
                    x_group.append(x)

            # åˆ›å»ºyåˆ†ç»„ï¼šä»prev_y+1åˆ°curr_y
            y_group = []
            for y in range(prev_y + 1, curr_y + 1):
                if 0 <= y < len(sents2):
                    y_group.append(y)

            # åªæœ‰å½“ä¸¤ä¸ªç»„éƒ½éç©ºæ—¶æ‰æ·»åŠ 
            if x_group and y_group:
                x_dtw.append(x_group)
                y_dtw.append(y_group)

            prev_x = curr_x
            prev_y = curr_y

    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆå¯¹é½ï¼Œåˆ›å»ºé»˜è®¤1-1å¯¹é½
    if not x_dtw:
        print("æœªæ‰¾åˆ°æœ‰æ•ˆå¯¹é½ç‚¹ï¼Œåˆ›å»ºé»˜è®¤1-1å¯¹é½")
        min_len = min(len(sents1), len(sents2))
        for i in range(min_len):
            x_dtw.append([i])
            y_dtw.append([i])
        score = 1.0

    print(f"æœ€ç»ˆå¯¹é½ç»„æ•°: {len(x_dtw)}")
    return x_dtw, y_dtw, score


def calc_int(group_x: List[int], group_y: List[int]) -> Tuple[int, int, int, int]:
    """
    è®¡ç®—ç»„çš„è¾¹ç•Œåæ ‡

    å‚æ•°:
        group_x (List[int]): Xåæ ‡ç»„
        group_y (List[int]): Yåæ ‡ç»„

    è¿”å›å€¼:
        Tuple[int, int, int, int]: (x_inf, x_sup, y_inf, y_sup)
    """
    if len(group_x) == 0:
        x_inf = x_sup = -1
    else:
        x_inf = group_x[0] - 1  # ç»„çš„ä¸‹ç•Œæ˜¯ç¬¬ä¸€ä¸ªå…ƒç´ å‡1
        x_sup = group_x[-1]     # ç»„çš„ä¸Šç•Œæ˜¯æœ€åä¸€ä¸ªå…ƒç´ 

    if len(group_y) == 0:
        y_inf = y_sup = -1
    else:
        y_inf = group_y[0] - 1  # ç»„çš„ä¸‹ç•Œæ˜¯ç¬¬ä¸€ä¸ªå…ƒç´ å‡1
        y_sup = group_y[-1]     # ç»„çš„ä¸Šç•Œæ˜¯æœ€åä¸€ä¸ªå…ƒç´ 

    return (x_inf, x_sup, y_inf, y_sup)


def prev(groups: List[Dict], i: int) -> int:
    """
    æ‰¾åˆ°å‰ä¸€ä¸ªæœ‰æ•ˆç»„çš„ç´¢å¼•

    å‚æ•°:
        groups (List[Dict]): ç»„åˆ—è¡¨
        i (int): å½“å‰ç»„ç´¢å¼•

    è¿”å›å€¼:
        int: å‰ä¸€ä¸ªæœ‰æ•ˆç»„çš„ç´¢å¼•ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›-1
    """
    for j in range(i - 1, -1, -1):
        if not groups[j].get('deleted', False):
            return j
    return -1


def next(groups: List[Dict], i: int) -> int:
    """
    æ‰¾åˆ°ä¸‹ä¸€ä¸ªæœ‰æ•ˆç»„çš„ç´¢å¼•

    å‚æ•°:
        groups (List[Dict]): ç»„åˆ—è¡¨
        i (int): å½“å‰ç»„ç´¢å¼•

    è¿”å›å€¼:
        int: ä¸‹ä¸€ä¸ªæœ‰æ•ˆç»„çš„ç´¢å¼•ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›-1
    """
    for j in range(i + 1, len(groups)):
        if not groups[j].get('deleted', False):
            return j
    return -1


def compute_gain(gains: SimpleOOBTree, groups: List[Dict], i: int, sents1: List[str], sents2: List[str],
                sim_mat: np.ndarray, embeds1: List[np.ndarray], embeds2: List[np.ndarray],
                char_ratio: float, params: Dict[str, Any]) -> None:
    """
    è®¡ç®—å°†ç¬¬ i ä¸ªç»„ä¸å·¦å³ç›¸é‚»ç»„åˆå¹¶æ‰€å¸¦æ¥çš„ç›¸ä¼¼åº¦å¢ç›Š

    å‚æ•°:
        gains (SimpleOOBTree): å¢ç›Šæ’åºBæ ‘
        groups (List[Dict]): ç»„ä¿¡æ¯åˆ—è¡¨
        i (int): å½“å‰ç»„ç´¢å¼•
        sents1 (List[str]): æºè¯­è¨€å¥å­åˆ—è¡¨
        sents2 (List[str]): ç›®æ ‡è¯­è¨€å¥å­åˆ—è¡¨
        sim_mat (np.ndarray): ç›¸ä¼¼åº¦çŸ©é˜µ
        embeds1 (List[np.ndarray]): æºè¯­è¨€åµŒå…¥å‘é‡åˆ—è¡¨
        embeds2 (List[np.ndarray]): ç›®æ ‡è¯­è¨€åµŒå…¥å‘é‡åˆ—è¡¨
        char_ratio (float): å­—ç¬¦æ¯”ä¾‹
        params (Dict[str, Any]): å‚æ•°å­—å…¸
    """
    if groups[i].get('deleted', False):
        return

    group_x = groups[i]['x']  # å½“å‰ç»„çš„Xåæ ‡
    group_y = groups[i]['y']  # å½“å‰ç»„çš„Yåæ ‡
    dist = groups[i]['dist']  # å½“å‰ç»„çš„è·ç¦»

    # æ¸…é™¤ä¹‹å‰çš„å¢ç›Šè®°å½•
    if 'gain' in groups[i]:
        old_gain = groups[i]['gain']
        if old_gain in gains:
            if isinstance(gains[old_gain], list) and i in gains[old_gain]:
                gains[old_gain].remove(i)
                if len(gains[old_gain]) == 0:
                    del gains[old_gain]

    # è®¡ç®—ä¸å‰ä¸€ç»„åˆå¹¶çš„å¢ç›Š
    prev_i = prev(groups, i)
    prev_gain = 0
    if prev_i != -1:
        prev_group_x = groups[prev_i]['x']
        prev_group_y = groups[prev_i]['y']
        no_empty = len(prev_group_x) > 0 and len(prev_group_y) > 0  # æ£€æŸ¥æ˜¯å¦ä¸ºéç©ºç»„
        new_group_x1 = prev_group_x + group_x  # åˆå¹¶åçš„Xç»„
        new_group_y1 = prev_group_y + group_y  # åˆå¹¶åçš„Yç»„
        (inf_x, sup_x, inf_y, sup_y) = calc_int(new_group_x1, new_group_y1)
        prev_dist = distance_dtw(sents1, sents2, sim_mat, embeds1, embeds2, inf_x, sup_x,
                                inf_y, sup_y, char_ratio, {}, params, False)
        prev_gain = dist - prev_dist  # å¢ç›Š = åŸè·ç¦» - æ–°è·ç¦»
        if no_empty:
            prev_gain -= params.get('penalty_n_n', 0.06)  # éç©ºç»„åˆå¹¶æƒ©ç½š
        else:
            prev_gain += params.get('penalty_0_n', 0.1)  # ç©ºç»„åˆå¹¶å¥–åŠ±

    # è®¡ç®—ä¸ä¸‹ä¸€ç»„åˆå¹¶çš„å¢ç›Š
    next_i = next(groups, i)
    next_gain = 0
    if next_i != -1:
        next_group_x = groups[next_i]['x']
        next_group_y = groups[next_i]['y']
        no_empty = len(next_group_x) > 0 and len(next_group_y) > 0  # æ£€æŸ¥æ˜¯å¦ä¸ºéç©ºç»„
        new_group_x2 = group_x + next_group_x  # åˆå¹¶åçš„Xç»„
        new_group_y2 = group_y + next_group_y  # åˆå¹¶åçš„Yç»„
        (inf_x, sup_x, inf_y, sup_y) = calc_int(new_group_x2, new_group_y2)
        next_dist = distance_dtw(sents1, sents2, sim_mat, embeds1, embeds2, inf_x, sup_x,
                                inf_y, sup_y, char_ratio, {}, params, False)
        next_gain = dist - next_dist  # å¢ç›Š = åŸè·ç¦» - æ–°è·ç¦»
        if no_empty:
            next_gain -= params.get('penalty_n_n', 0.06)  # éç©ºç»„åˆå¹¶æƒ©ç½š
        else:
            next_gain += params.get('penalty_0_n', 0.1)  # ç©ºç»„åˆå¹¶å¥–åŠ±

    # é€‰æ‹©æœ€ä½³å¢ç›Šæ–¹å‘
    if next_gain > prev_gain and next_gain > 0:
        groups[i]['gain'] = next_gain
        groups[i]['direction'] = 1  # å‘å³åˆå¹¶
        groups[i]['newX'] = group_x + next_group_x
        groups[i]['newY'] = group_y + next_group_y
        groups[i]['newDist'] = next_dist
        groups[i]['mergeWith'] = next_i

        # æ·»åŠ åˆ°å¢ç›ŠBæ ‘
        if next_gain not in gains:
            gains[next_gain] = []
        gains[next_gain].append(i)

    elif prev_gain > 0:
        groups[i]['gain'] = prev_gain
        groups[i]['direction'] = -1  # å‘å·¦åˆå¹¶
        groups[i]['newX'] = prev_group_x + group_x
        groups[i]['newY'] = prev_group_y + group_y
        groups[i]['newDist'] = prev_dist
        groups[i]['mergeWith'] = prev_i

        # æ·»åŠ åˆ°å¢ç›ŠBæ ‘
        if prev_gain not in gains:
            gains[prev_gain] = []
        gains[prev_gain].append(i)
    else:
        # æ²¡æœ‰æ­£å¢ç›Š
        groups[i]['gain'] = 0
        groups[i]['direction'] = 0


def late_grouping(x_dtw: List[List[int]], y_dtw: List[List[int]], sents1: List[str], sents2: List[str],
                 sim_mat: np.ndarray, embeds1: List[np.ndarray], embeds2: List[np.ndarray],
                 char_ratio: float, params: Dict[str, Any]) -> Tuple[List[List[int]], List[List[int]]]:
    """
    åœ¨è·å¾—åˆæ­¥ DTW å¥å­çº§å¯¹é½ç»“æœåï¼Œé€šè¿‡è´ªå¿ƒæ–¹å¼å°è¯•å°†ç›¸é‚»ç»„åˆå¹¶ï¼Œ
    ä»¥æå‡æ•´ä½“ç›¸ä¼¼åº¦å¾—åˆ†å¹¶å‡å°‘ç©ºå¯¹é½ã€‚

    ä½¿ç”¨Qwen3-Embedding-8Bæ¨¡å‹è¿›è¡Œé«˜è´¨é‡çš„ç»„åˆå¥å­å‘é‡åŒ–ã€‚

    å‚æ•°:
        x_dtw (List[List[int]]): Xä¾§å¯¹é½åˆ†ç»„åˆ—è¡¨
        y_dtw (List[List[int]]): Yä¾§å¯¹é½åˆ†ç»„åˆ—è¡¨
        sents1 (List[str]): æºè¯­è¨€å¥å­åˆ—è¡¨
        sents2 (List[str]): ç›®æ ‡è¯­è¨€å¥å­åˆ—è¡¨
        sim_mat (np.ndarray): ç›¸ä¼¼åº¦çŸ©é˜µ
        embeds1 (List[np.ndarray]): æºè¯­è¨€å¥å‘é‡åˆ—è¡¨
        embeds2 (List[np.ndarray]): ç›®æ ‡è¯­è¨€å¥å‘é‡åˆ—è¡¨
        char_ratio (float): å­—ç¬¦é•¿åº¦æ¯”ä¾‹
        params (Dict[str, Any]): å‚æ•°å­—å…¸

    è¿”å›å€¼:
        Tuple[List[List[int]], List[List[int]]]: æ›´æ–°åçš„ x_dtw, y_dtw è·¯å¾„
    """
    print("*** å¼€å§‹æ‰§è¡ŒåæœŸåˆ†ç»„ä¼˜åŒ–ï¼ˆä½¿ç”¨Qwen3-Embedding-8Bï¼‰...")

    # è¿™ä¸ªBæ ‘è®°å½•æŒ‰å¢ç›Šæ’åºçš„æ¯ä¸ªç»„çš„ç´¢å¼•
    gains = SimpleOOBTree()  # å¢ç›Šæ’åºBæ ‘
    groups = []  # ç»„ä¿¡æ¯åˆ—è¡¨

    # é¢„æ”¶é›†æ‰€æœ‰éœ€è¦è®¡ç®—çš„ç»„åˆæ–‡æœ¬ï¼Œè¿›è¡Œæ‰¹é‡å‘é‡åŒ–
    print("*** é¢„æ”¶é›†ç»„åˆæ–‡æœ¬è¿›è¡Œæ‰¹é‡å‘é‡åŒ–...")
    all_combined_texts = []
    text_to_group_mapping = {}

    # åˆå§‹åŒ–ç»„æ•°æ®ç»“æ„ï¼šå¯¹æ¯ä¸ªç»„ï¼Œè®°å½•x,yå’Œå¯¹åº”çš„è·ç¦»
    print("*** åˆå§‹åŒ–åæœŸåˆ†ç»„æ•°æ®ç»“æ„...")
    for (group_x, group_y) in zip(x_dtw, y_dtw):
        (inf_x, sup_x, inf_y, sup_y) = calc_int(group_x, group_y)

        # æ”¶é›†å¯èƒ½éœ€è¦çš„ç»„åˆæ–‡æœ¬
        if inf_x != sup_x - 1:  # å¤šä¸ªæºå¥å­éœ€è¦ç»„åˆ
            source_sentences = [sents1[idx] for idx in range(inf_x + 1, sup_x + 1) if 0 <= idx < len(sents1)]
            combined_text_x = " ".join(source_sentences)
            if combined_text_x not in all_combined_texts:
                all_combined_texts.append(combined_text_x)
                text_to_group_mapping[combined_text_x] = 'source'
                print(f"    ğŸ“ æ”¶é›†ç»„åˆæºæ–‡æœ¬ (å¥å­ {inf_x+1}-{sup_x}):")
                for i, sent in enumerate(source_sentences, inf_x+1):
                    print(f"      [{i}] {sent}")
                print(f"    ğŸ”— ç»„åˆåå®Œæ•´æºæ–‡æœ¬: '{combined_text_x}'")

        if inf_y != sup_y - 1:  # å¤šä¸ªç›®æ ‡å¥å­éœ€è¦ç»„åˆ
            target_sentences = [sents2[idx] for idx in range(inf_y + 1, sup_y + 1) if 0 <= idx < len(sents2)]
            combined_text_y = " ".join(target_sentences)
            if combined_text_y not in all_combined_texts:
                all_combined_texts.append(combined_text_y)
                text_to_group_mapping[combined_text_y] = 'target'
                print(f"    ğŸ“ æ”¶é›†ç»„åˆç›®æ ‡æ–‡æœ¬ (å¥å­ {inf_y+1}-{sup_y}):")
                for i, sent in enumerate(target_sentences, inf_y+1):
                    print(f"      [{i}] {sent}")
                print(f"    ğŸ”— ç»„åˆåå®Œæ•´ç›®æ ‡æ–‡æœ¬: '{combined_text_y}'")

        # è®¡ç®—åˆå§‹è·ç¦»
        dist = distance_dtw(sents1, sents2, sim_mat, embeds1, embeds2, inf_x, sup_x, inf_y,
                           sup_y, char_ratio, {}, params, False)
        groups.append({
            'x': group_x,
            'y': group_y,
            'original_x': group_x.copy(),  # ä¿å­˜åŸå§‹åˆ†ç»„
            'original_y': group_y.copy(),  # ä¿å­˜åŸå§‹åˆ†ç»„
            "dist": dist,
            'deleted': False
        })

    # æ‰¹é‡è®¡ç®—ç»„åˆæ–‡æœ¬çš„åµŒå…¥å‘é‡
    if all_combined_texts:
        print(f"*** æ‰¹é‡è®¡ç®— {len(all_combined_texts)} ä¸ªç»„åˆæ–‡æœ¬çš„åµŒå…¥å‘é‡...")
        batch_embeddings = call_qwen3_embedding(all_combined_texts)
        print(f"*** æ‰¹é‡å‘é‡åŒ–å®Œæˆ")

        # è¯¦ç»†æ˜¾ç¤ºæ‰€æœ‰ç»„åˆæ–‡æœ¬
        print(f"\nğŸ“‹ **å®Œæ•´ç»„åˆæ–‡æœ¬åˆ—è¡¨** ({len(all_combined_texts)} ä¸ª):")
        source_count = 0
        target_count = 0
        for i, text in enumerate(all_combined_texts, 1):
            text_type = text_to_group_mapping.get(text, 'unknown')
            if text_type == 'source':
                source_count += 1
                print(f"  ğŸ‡¨ğŸ‡³ [{i}] ç»„åˆæºæ–‡æœ¬: '{text}'")
            elif text_type == 'target':
                target_count += 1
                print(f"  ğŸ‡ºğŸ‡¸ [{i}] ç»„åˆç›®æ ‡æ–‡æœ¬: '{text}'")
            else:
                print(f"  â“ [{i}] æœªçŸ¥ç±»å‹æ–‡æœ¬: '{text}'")

        print(f"\nğŸ“Š ç»„åˆæ–‡æœ¬ç»Ÿè®¡:")
        print(f"  - ç»„åˆæºæ–‡æœ¬æ•°é‡: {source_count}")
        print(f"  - ç»„åˆç›®æ ‡æ–‡æœ¬æ•°é‡: {target_count}")
        print(f"  - æ€»è®¡: {len(all_combined_texts)}")
    else:
        print("*** æ— éœ€è¦ç»„åˆçš„æ–‡æœ¬")

    print(f"*** å…±æœ‰ {len(groups)} ä¸ªåˆå§‹åˆ†ç»„")

    # ç¬¬ä¸€è½®è¿­ä»£ï¼šå¯¹æ¯ä¸ªç»„ï¼Œè®¡ç®—å‘å·¦æˆ–å‘å³åˆ†ç»„çš„ç›¸ä¼¼åº¦å¢ç›Š
    print("*** è®¡ç®—å„ç»„åˆå¹¶å¢ç›Š...")
    for i in range(len(groups)):
        compute_gain(gains, groups, i, sents1, sents2, sim_mat, embeds1, embeds2, char_ratio, params)

    if len(gains) > 0:
        best_gain = gains.maxKey()  # è·å–æœ€å¤§å¢ç›Š
        print(f"*** æœ€ä½³åˆå§‹å¢ç›Š: {best_gain:.6f}")
    else:
        best_gain = 0
        print("*** æ²¡æœ‰æ­£å¢ç›Šï¼Œè·³è¿‡åˆ†ç»„ä¼˜åŒ–")

    # å½“æœ€ä½³åˆ†ç»„èƒ½äº§ç”Ÿæ­£å¢ç›Šæ—¶ï¼Œç»§ç»­åˆå¹¶
    merge_count = 0
    while best_gain > 0:
        # è·å–å…·æœ‰æœ€ä½³å¢ç›Šçš„ç»„åˆ—è¡¨
        best_groups = gains[best_gain]
        i = best_groups[0]  # é€‰æ‹©ç¬¬ä¸€ä¸ªç»„è¿›è¡Œåˆå¹¶

        # ä»å¢ç›ŠBæ ‘ä¸­ç§»é™¤
        best_groups.remove(i)
        if len(best_groups) == 0:
            del gains[best_gain]

        # æ‰§è¡Œåˆå¹¶
        merge_with = groups[i]['mergeWith']
        direction = groups[i]['direction']

        print(f"*** åˆå¹¶ç»„ {i} ä¸ç»„ {merge_with}ï¼Œå¢ç›Š: {best_gain:.6f}")

        # æ ‡è®°è¢«åˆå¹¶çš„ç»„ä¸ºå·²åˆ é™¤
        groups[merge_with]['deleted'] = True

        # æ›´æ–°å½“å‰ç»„
        groups[i]['x'] = groups[i]['newX']
        groups[i]['y'] = groups[i]['newY']
        groups[i]['dist'] = groups[i]['newDist']

        # æ›´æ–°å¢ç›Šï¼Œåœ¨å·¦å³ä¸¤ä¾§
        compute_gain(gains, groups, i, sents1, sents2, sim_mat, embeds1, embeds2, char_ratio, params)

        # æ›´æ–°å·¦å³ä¸¤ä¾§çš„å¢ç›Š
        prev_i = prev(groups, i)
        if prev_i != -1:
            compute_gain(gains, groups, prev_i, sents1, sents2, sim_mat, embeds1, embeds2, char_ratio, params)

        next_i = next(groups, i)
        if next_i != -1:
            compute_gain(gains, groups, next_i, sents1, sents2, sim_mat, embeds1, embeds2, char_ratio, params)

        # ä¸ºä¸‹ä¸€æ¬¡è¿­ä»£è®¡ç®—æœ€ä½³å¢ç›Š
        if len(gains) > 0:
            best_gain = gains.maxKey()
        else:
            best_gain = 0

        merge_count += 1

    print(f"*** åæœŸåˆ†ç»„å®Œæˆï¼Œå…±æ‰§è¡Œ {merge_count} æ¬¡åˆå¹¶")

    # é‡å»ºx_dtwå’Œy_dtw
    x_dtw = []
    y_dtw = []
    for group in groups:
        if not group.get('deleted', False):
            x_dtw.append(group['x'])
            y_dtw.append(group['y'])

    print(f"*** ä¼˜åŒ–ååˆ†ç»„æ•°: {len(x_dtw)}")

    # å¦‚æœæ‰€æœ‰ç»„éƒ½è¢«åˆ é™¤äº†ï¼Œæ¢å¤åŸå§‹åˆ†ç»„
    if len(x_dtw) == 0:
        print("âš ï¸ æ‰€æœ‰ç»„éƒ½è¢«åˆå¹¶åˆ é™¤ï¼Œæ¢å¤åŸå§‹åˆ†ç»„")
        x_dtw = []
        y_dtw = []
        for group in groups:
            if 'original_x' in group and 'original_y' in group:
                x_dtw.append(group['original_x'])
                y_dtw.append(group['original_y'])
            else:
                x_dtw.append(group['x'])
                y_dtw.append(group['y'])
        print(f"*** æ¢å¤ååˆ†ç»„æ•°: {len(x_dtw)}")

    return (x_dtw, y_dtw)


def save_alignment_results(output_file_path: str, sentence_id: int, x_dtw: List[List[int]], y_dtw: List[List[int]],
                          sents1: List[str], sents2: List[str], score: float, params: Dict[str, Any]) -> None:
    """
    ä¿å­˜DTWå¯¹é½ç»“æœåˆ°JSONæ–‡ä»¶

    å‚æ•°:
        output_file_path (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„
        sentence_id (int): å¥å­ID
        x_dtw (List[List[int]]): xè½´å¯¹é½è·¯å¾„
        y_dtw (List[List[int]]): yè½´å¯¹é½è·¯å¾„
        sents1 (List[str]): æºè¯­è¨€å¥å­åˆ—è¡¨
        sents2 (List[str]): ç›®æ ‡è¯­è¨€å¥å­åˆ—è¡¨
        score (float): å¯¹é½å¾—åˆ†
        params (Dict): å‚æ•°å­—å…¸
    """
    print(f"\n=== ä¿å­˜å¯¹é½ç»“æœ ===")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_file_path}")
    print(f"è¾“å…¥æ•°æ®æ£€æŸ¥:")
    print(f"  x_dtw: {x_dtw}")
    print(f"  y_dtw: {y_dtw}")
    print(f"  score: {score}")

    # æ„å»ºå¯¹é½ç»“æœæ•°æ®
    alignment_pairs = []
    for i, (x_group, y_group) in enumerate(zip(x_dtw, y_dtw)):
        # è·å–å¯¹åº”çš„å¥å­æ–‡æœ¬
        english_texts = [sents1[x] for x in x_group if 0 <= x < len(sents1)]
        chinese_texts = [sents2[y] for y in y_group if 0 <= y < len(sents2)]

        alignment_pairs.append({
            'pair_id': i + 1,
            'english_indices': x_group,
            'chinese_indices': y_group,
            'english_texts': english_texts,
            'chinese_texts': chinese_texts,
            'alignment_type': f"{len(x_group)}-{len(y_group)}"
        })

        print(f"  å¯¹é½å¯¹ {i+1}: {x_group} -> {y_group} ({len(x_group)}-{len(y_group)})")

    # æ„å»ºå®Œæ•´çš„ç»“æœæ•°æ®
    result_data = {
        'sentence_id': sentence_id,
        'dtw_alignment_result': {
            'total_pairs': len(alignment_pairs),
            'alignment_score': float(score),
            'alignment_pairs': alignment_pairs
        },
        'statistics': {
            'english_sentence_count': len(sents1),
            'chinese_sentence_count': len(sents2),
            'alignment_types': {}
        },
        'parameters': params,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
    }

    # ç»Ÿè®¡å¯¹é½ç±»å‹
    for pair in alignment_pairs:
        alignment_type = pair['alignment_type']
        if alignment_type not in result_data['statistics']['alignment_types']:
            result_data['statistics']['alignment_types'][alignment_type] = 0
        result_data['statistics']['alignment_types'][alignment_type] += 1

    try:
        with open(output_file_path, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2)

        print(f"âœ“ å¯¹é½ç»“æœå·²ä¿å­˜")
        print(f"  å¯¹é½å¯¹æ•°: {len(alignment_pairs)}")
        print(f"  å¯¹é½å¾—åˆ†: {score:.4f}")
        print(f"  å¯¹é½ç±»å‹åˆ†å¸ƒ: {result_data['statistics']['alignment_types']}")

    except Exception as e:
        print(f"âœ— ä¿å­˜å¤±è´¥: {e}")
        raise


def print_alignment_summary(x_dtw: List[List[int]], y_dtw: List[List[int]],
                           sents1: List[str], sents2: List[str], score: float) -> None:
    """
    æ‰“å°å¯¹é½ç»“æœæ‘˜è¦

    å‚æ•°:
        x_dtw (List[List[int]]): xè½´å¯¹é½è·¯å¾„
        y_dtw (List[List[int]]): yè½´å¯¹é½è·¯å¾„
        sents1 (List[str]): æºè¯­è¨€å¥å­åˆ—è¡¨
        sents2 (List[str]): ç›®æ ‡è¯­è¨€å¥å­åˆ—è¡¨
        score (float): å¯¹é½å¾—åˆ†
    """
    print(f"\n=== å¯¹é½ç»“æœæ‘˜è¦ ===")
    print(f"æ€»å¯¹é½å¯¹æ•°: {len(x_dtw)}")
    print(f"å¯¹é½å¾—åˆ†: {score:.4f}")
    print(f"å¹³å‡æ¯å¯¹å¾—åˆ†: {score/len(x_dtw):.4f}" if len(x_dtw) > 0 else "N/A")

    # ç»Ÿè®¡å¯¹é½ç±»å‹
    alignment_types = {}
    for x_group, y_group in zip(x_dtw, y_dtw):
        alignment_type = f"{len(x_group)}-{len(y_group)}"
        alignment_types[alignment_type] = alignment_types.get(alignment_type, 0) + 1

    print(f"å¯¹é½ç±»å‹åˆ†å¸ƒ:")
    for alignment_type, count in sorted(alignment_types.items()):
        print(f"  {alignment_type}: {count} å¯¹")

    print(f"\nè¯¦ç»†å¯¹é½ç»“æœ:")
    for i, (x_group, y_group) in enumerate(zip(x_dtw, y_dtw)):
        english_texts = [sents1[x] for x in x_group if 0 <= x < len(sents1)]
        chinese_texts = [sents2[y] for y in y_group if 0 <= y < len(sents2)]

        print(f"  å¯¹é½å¯¹ {i+1} ({len(x_group)}-{len(y_group)}):")
        print(f"    è‹±æ–‡[{','.join(map(str, x_group))}]: {' | '.join(english_texts)}")
        print(f"    ä¸­æ–‡[{','.join(map(str, y_group))}]: {' | '.join(chinese_texts)}")
        print()


def load_precomputed_embeddings(sents1: List[str], sents2: List[str],
                              sim_mat: np.ndarray) -> Tuple[List[np.ndarray], List[np.ndarray]]:
    """
    ä½¿ç”¨Qwen3-Embedding-8Bæ¨¡å‹è®¡ç®—å•å¥åµŒå…¥å‘é‡
    è¿™æ ·å¯ä»¥ç¡®ä¿å•å¥å’Œç»„åˆå¥å­ä½¿ç”¨ç›¸åŒçš„å‘é‡ç©ºé—´

    å‚æ•°:
        sents1 (List[str]): æºè¯­è¨€å¥å­åˆ—è¡¨
        sents2 (List[str]): ç›®æ ‡è¯­è¨€å¥å­åˆ—è¡¨
        sim_mat (np.ndarray): é¢„è®¡ç®—çš„ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆç”¨äºéªŒè¯ï¼‰

    è¿”å›å€¼:
        Tuple[List[np.ndarray], List[np.ndarray]]: (æºè¯­è¨€åµŒå…¥å‘é‡åˆ—è¡¨, ç›®æ ‡è¯­è¨€åµŒå…¥å‘é‡åˆ—è¡¨)
    """
    print("*** ä½¿ç”¨Qwen3-Embedding-8Bè®¡ç®—å•å¥åµŒå…¥å‘é‡...")

    try:
        # ä½¿ç”¨Qwen3-Embedding-8Bè®¡ç®—æ‰€æœ‰å•å¥çš„åµŒå…¥å‘é‡
        print(f"  æ­£åœ¨è®¡ç®— {len(sents1)} ä¸ªæºè¯­è¨€å¥å­çš„åµŒå…¥å‘é‡...")
        embeds1 = call_qwen3_embedding(sents1)

        print(f"  æ­£åœ¨è®¡ç®— {len(sents2)} ä¸ªç›®æ ‡è¯­è¨€å¥å­çš„åµŒå…¥å‘é‡...")
        embeds2 = call_qwen3_embedding(sents2)

        # éªŒè¯å‘é‡ç»´åº¦
        if embeds1 and embeds2:
            dim1 = embeds1[0].shape[0] if len(embeds1[0].shape) > 0 else len(embeds1[0])
            dim2 = embeds2[0].shape[0] if len(embeds2[0].shape) > 0 else len(embeds2[0])
            print(f"âœ“ åµŒå…¥å‘é‡è®¡ç®—å®Œæˆ")
            print(f"  æºè¯­è¨€: {len(embeds1)} ä¸ªå¥å­ï¼Œç»´åº¦: {dim1}")
            print(f"  ç›®æ ‡è¯­è¨€: {len(embeds2)} ä¸ªå¥å­ï¼Œç»´åº¦: {dim2}")

            # éªŒè¯ç›¸ä¼¼åº¦çŸ©é˜µçš„ä¸€è‡´æ€§
            if len(embeds1) > 0 and len(embeds2) > 0:
                computed_sim = np.matmul(embeds1[0], np.transpose(embeds2[0]))
                original_sim = sim_mat[0, 0]
                print(f"  ç›¸ä¼¼åº¦éªŒè¯: è®¡ç®—å€¼={computed_sim:.4f}, åŸå§‹å€¼={original_sim:.4f}")

        return embeds1, embeds2

    except Exception as e:
        print(f"âŒ Qwen3-Embedding-8Bè®¡ç®—å¤±è´¥: {e}")
        raise e





def main():
    """
    ä¸»å‡½æ•° - æ‰§è¡Œå®Œæ•´çš„DTWå¯¹é½æµç¨‹
    é›†æˆQwen3-Embedding-8Bæ¨¡å‹è¿›è¡Œé«˜è´¨é‡å¥å­å¯¹é½
    """
    print("ğŸš€ å¼€å§‹æ‰§è¡ŒDTWå¯¹é½ç®—æ³•ï¼ˆé›†æˆQwen3-Embedding-8Bï¼‰...")

    # ç›´æ¥ä½¿ç”¨è®¾å®šçš„å¯†é’¥ï¼Œä¸æ£€æŸ¥ç¯å¢ƒå˜é‡
    print("âœ“ ä½¿ç”¨é¢„è®¾çš„APIå¯†é’¥è¿›è¡Œè¿œç¨‹åµŒå…¥å‘é‡è®¡ç®—")

    # é’ˆå¯¹åŒè¯­å­å¥æ•°1-6ä¼˜åŒ–çš„é…ç½®å‚æ•°
    params = {
        'verbose': True,
        'veryVerbose': True,   # å¯ç”¨è¯¦ç»†æ—¥å¿—æŸ¥çœ‹DTWå†³ç­–è¿‡ç¨‹
        'distNull': 1.0,
        'penalty_n_n': 0.01,  # å¤šå¯¹å¤šå¯¹é½æƒ©ç½šï¼ˆ1-6å¥å­ä¸­å¤šå¯¹å¤šå¸¸è§ï¼Œä¿æŒä½æƒ©ç½šï¼‰
        'penalty_0_n': 0.05,  # ç©ºç»„åˆå¹¶å¥–åŠ±
        'noEmptyPair': False,  # å…è®¸ç©ºå¯¹é½
        'no2_2Group': False,   # å…è®¸2-2ç»„åˆ
        'dtwBeam': 7,          # æŸæœç´¢å®½åº¦ï¼ˆé’ˆå¯¹1-6å¥å­ä¼˜åŒ–ï¼š20â†’7ï¼Œå‡å°‘65%è®¡ç®—æˆæœ¬ï¼‰
        'localDiagBeam': 0.35, # é”šç‚¹åå·®é˜ˆå€¼ï¼ˆé’ˆå¯¹1-6å¥å­ä¼˜åŒ–ï¼šå…è®¸çº¦2.1æ ¼å­åå·®ï¼Œæ›´ç²¾ç¡®ï¼‰
        'useEncoder': True,    # å¯ç”¨Qwen3-Embedding-8BåµŒå…¥å‘é‡è®¡ç®—
        'noMarginPenalty': False,  # å¯ç”¨é‚»è¿‘å¥æƒ©ç½šï¼ˆæé«˜å¯¹é½è´¨é‡ï¼‰
        'lateGrouping': True   # å¯ç”¨åæœŸåˆ†ç»„ä¼˜åŒ–ï¼Œæå‡å¯¹é½è´¨é‡
    }

    print("ğŸ“‹ ç®—æ³•é…ç½®ï¼ˆé’ˆå¯¹åŒè¯­å­å¥æ•°1-6ä¼˜åŒ–ï¼‰:")
    print(f"  - åµŒå…¥å‘é‡æ¨¡å‹: Qwen3-Embedding-8B")
    print(f"  - æŸæœç´¢å®½åº¦: {params['dtwBeam']} (ä¼˜åŒ–ï¼š20â†’7ï¼Œå‡å°‘65%è®¡ç®—æˆæœ¬)")
    print(f"  - é”šç‚¹åå·®é˜ˆå€¼: {params['localDiagBeam']} (ä¼˜åŒ–ï¼š0.5â†’0.35ï¼Œå…è®¸2.1æ ¼å­åå·®ï¼Œæ›´ç²¾ç¡®)")
    print(f"  - é‚»è¿‘å¥æƒ©ç½š: {'å¯ç”¨' if not params['noMarginPenalty'] else 'ç¦ç”¨'}")
    print(f"  - åæœŸåˆ†ç»„ä¼˜åŒ–: {'å¯ç”¨' if params['lateGrouping'] else 'ç¦ç”¨'}")
    print(f"  - æ•°æ®è§„æ¨¡: å®Œç¾é€‚é…1-6å¥å­åŒè¯­æ•°æ®ï¼Œæœ€å¤§æœç´¢ç©ºé—´36æ ¼å­")

    # è‡ªåŠ¨æ£€æµ‹è·¯å¾„é…ç½®
    def get_data_paths():
        """æ ¹æ®å½“å‰å·¥ä½œç›®å½•è‡ªåŠ¨è°ƒæ•´æ•°æ®æ–‡ä»¶è·¯å¾„"""
        current_dir = os.path.basename(os.getcwd())
        if current_dir == "new-code":
            # ä» new-code ç›®å½•å†…è¿è¡Œ
            data_prefix = "../new-data/"
        else:
            # ä»é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
            data_prefix = "new-data/"

        return {
            "interval_file_path": f"{data_prefix}complete_interval_extraction-output.json",
            "matrix_file_path": f"{data_prefix}Qwen3-Embedding-8B-output.json",
            "sentence_file_path": f"{data_prefix}sentence.json",
            "output_file_path": f"{data_prefix}dtw_alignment-B-output.json"
        }

    # è·å–è·¯å¾„é…ç½®
    paths = get_data_paths()
    sentence_id = 0
    interval_file_path = paths["interval_file_path"]
    matrix_file_path = paths["matrix_file_path"]
    sentence_file_path = paths["sentence_file_path"]
    output_file_path = paths["output_file_path"]

    try:
        # æ­¥éª¤1: åŠ è½½æ‰€æœ‰å¿…è¦æ•°æ®
        print(f"\nğŸ“ æ­¥éª¤1: æ•°æ®åŠ è½½é˜¶æ®µ")
        interval_data = load_interval_data(interval_file_path, sentence_id)
        sim_mat = load_similarity_matrix(matrix_file_path, sentence_id)
        sents1, sents2 = load_sentence_data(sentence_file_path, sentence_id)

        # æ„å»ºå•å¥åµŒå…¥å‘é‡ï¼ˆç”¨äºé‚»è¿‘å¥æƒ©ç½šè®¡ç®—å’Œå•å¥æƒ…å†µï¼‰
        print("ğŸ”§ æ„å»ºå•å¥åµŒå…¥å‘é‡...")
        embeds1, embeds2 = load_precomputed_embeddings(sents1, sents2, sim_mat)

        # è®¡ç®—å­—ç¬¦æ¯”ä¾‹
        nb_chars1 = sum(len(sent) for sent in sents1)
        nb_chars2 = sum(len(sent) for sent in sents2)
        char_ratio = nb_chars2 / nb_chars1 if nb_chars1 > 0 else 1.0

        print(f"ğŸ“Š æ–‡æœ¬ç»Ÿè®¡:")
        print(f"  - æºè¯­è¨€å­—ç¬¦æ•°: {nb_chars1}")
        print(f"  - ç›®æ ‡è¯­è¨€å­—ç¬¦æ•°: {nb_chars2}")
        print(f"  - å­—ç¬¦æ¯”ä¾‹: {char_ratio:.3f}")

        # æ­¥éª¤2: æ‰§è¡ŒDTWç®—æ³•
        print(f"\nğŸ”„ æ­¥éª¤2: DTWç®—æ³•æ‰§è¡Œ")

        # ç”Ÿæˆå…è®¸çš„å¥å­ç»„åˆå¹¶æ·»åŠ åˆ°å‚æ•°ä¸­
        allowed_groups = generate_allowed_groups(params, sents1, sents2)
        params['allowedGroups'] = allowed_groups

        x_dtw, y_dtw, score = run_dtw(interval_data, sim_mat, sents1, sents2, embeds1, embeds2, params)

        if len(x_dtw) == 0:
            print(f"âŒ DTWç®—æ³•æœªäº§ç”Ÿæœ‰æ•ˆå¯¹é½ç»“æœ")
            return

        # æ­¥éª¤3: åæœŸåˆ†ç»„ä¼˜åŒ–ï¼ˆä½¿ç”¨Qwen3-Embedding-8Bï¼‰
        if params.get('lateGrouping', False):
            print(f"\nğŸ”§ æ­¥éª¤3: åæœŸåˆ†ç»„ä¼˜åŒ–ï¼ˆä½¿ç”¨Qwen3-Embedding-8Bï¼‰")
            x_dtw, y_dtw = late_grouping(x_dtw, y_dtw, sents1, sents2, sim_mat, embeds1, embeds2, char_ratio, params)
            print(f"âœ“ åæœŸåˆ†ç»„ä¼˜åŒ–å®Œæˆ")

        # æ­¥éª¤4: è¾“å‡ºç»“æœ
        print(f"\nğŸ“Š æ­¥éª¤4: ç»“æœè¾“å‡º")
        print_alignment_summary(x_dtw, y_dtw, sents1, sents2, score)
        save_alignment_results(output_file_path, sentence_id, x_dtw, y_dtw, sents1, sents2, score, params)

        print(f"\nğŸ‰ DTWå¯¹é½ç®—æ³•æ‰§è¡Œå®Œæˆï¼")
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {output_file_path}")
        print(f"ğŸ” ä½¿ç”¨Qwen3-Embedding-8Bæ¨¡å‹è¿›è¡Œäº†é«˜è´¨é‡çš„å¥å­å‘é‡åŒ–")

        # æ˜¾ç¤ºAPIè°ƒç”¨ç»Ÿè®¡
        print_qwen3_stats()


    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯:")
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
        raise


if __name__ == "__main__":
    main()
