# -*- coding:utf8 -*-
"""
é”šç‚¹å¯†åº¦è¿‡æ»¤æ¨¡å— - åŸºäºç›¸ä¼¼åº¦çŸ©é˜µçš„æ™ºèƒ½é”šç‚¹æå–ï¼ˆæ— é¢„å®šä¹‰é”šç‚¹ï¼‰

æœ¬æ¨¡å—å®ç°äº†è·¨è¯­è¨€å¥å­å¯¹é½ç³»ç»Ÿçš„ç¬¬ä¸€é˜¶æ®µï¼šé”šç‚¹å¯†åº¦è¿‡æ»¤
ä¸»è¦åŠŸèƒ½ï¼š
1. ä»ç›¸ä¼¼åº¦çŸ©é˜µä¸­æ™ºèƒ½æå–å€™é€‰é”šç‚¹ï¼ˆå®Œå…¨è‡ªåŠ¨åŒ–ï¼Œæ— éœ€é¢„å®šä¹‰é”šç‚¹ï¼‰
2. è®¡ç®—æ¯ä¸ªé”šç‚¹çš„å±€éƒ¨å¯†åº¦å’Œè´¨é‡è¯„åˆ†
3. è¿‡æ»¤ä½è´¨é‡é”šç‚¹ï¼Œä¿ç•™é«˜è´¨é‡é”šç‚¹
4. è§£å†³é”šç‚¹å†²çªï¼Œç¡®ä¿é”šç‚¹åˆ†å¸ƒåˆç†
5. è¾“å‡ºè¿‡æ»¤åçš„é”šç‚¹æ•°æ®ä¾›åç»­åŒºé—´åˆ’åˆ†ä½¿ç”¨

æ ¸å¿ƒç®—æ³•æµç¨‹ï¼š
1. åŠ è½½å¥å­æ•°æ®å’Œç›¸ä¼¼åº¦çŸ©é˜µ
2. åŸºäºé˜ˆå€¼è‡ªåŠ¨æå–å€™é€‰é”šç‚¹ï¼ˆçº¯æ•°æ®é©±åŠ¨ï¼‰
3. è®¡ç®—å±€éƒ¨å¯†åº¦å’Œè´¨é‡è¯„åˆ†
4. æ‰§è¡Œè´¨é‡è¿‡æ»¤å’Œå†²çªè§£å†³
5. ä¿å­˜è¿‡æ»¤åçš„é”šç‚¹æ•°æ®

æŠ€æœ¯ç‰¹ç‚¹ï¼š
- å®Œå…¨å»é™¤é¢„å®šä¹‰é”šç‚¹ä¾èµ–
- çº¯åŸºäºç›¸ä¼¼åº¦çŸ©é˜µçš„è‡ªåŠ¨é”šç‚¹æå–
- æ•°æ®é©±åŠ¨çš„è´¨é‡è¯„ä¼°æœºåˆ¶

ä½œè€…ï¼šAI Assistant
æ—¥æœŸï¼š2025-01-29
"""

import os
import sys
import json
import math
import time
import numpy as np




########################################################################### è™šæ‹Ÿè¾¹ç•Œå¤„ç†å®‰å…¨å‡½æ•°

def safe_character_count(sentences, start_idx, end_idx):
    """
    å®‰å…¨çš„å­—ç¬¦è®¡æ•°å‡½æ•°ï¼Œé¿å…è¶Šç•Œè®¿é—®å¥å­åˆ—è¡¨
    
    å‚æ•°ï¼š
    sentences (list[str]): å¥å­åˆ—è¡¨
    start_idx (int): èµ·å§‹ç´¢å¼•
    end_idx (int): ç»“æŸç´¢å¼•ï¼ˆåŒ…å«ï¼‰
    
    è¿”å›å€¼ï¼š
    int: å­—ç¬¦æ€»æ•°
    """
    char_count = 0
    safe_start = max(0, start_idx)
    safe_end = min(end_idx, len(sentences) - 1)
    
    # ç¡®ä¿èŒƒå›´æœ‰æ•ˆ
    if safe_start <= safe_end and safe_start < len(sentences):
        for n in range(safe_start, safe_end + 1):
            if 0 <= n < len(sentences):  # åŒé‡æ£€æŸ¥
                char_count += len(sentences[n])
    
    return char_count


def is_valid_anchor(x, y, len_sents1, len_sents2):
    """
    æ£€æŸ¥é”šç‚¹æ˜¯å¦ä¸ºæœ‰æ•ˆçš„çœŸå®é”šç‚¹ï¼ˆéè™šæ‹Ÿè¾¹ç•Œï¼‰
    
    å‚æ•°ï¼š
    x (int): è‹±æ–‡å¥å­ç´¢å¼•
    y (int): ä¸­æ–‡å¥å­ç´¢å¼•  
    len_sents1 (int): è‹±æ–‡å¥å­æ€»æ•°
    len_sents2 (int): ä¸­æ–‡å¥å­æ€»æ•°
    
    è¿”å›å€¼ï¼š
    bool: Trueå¦‚æœæ˜¯æœ‰æ•ˆé”šç‚¹ï¼ŒFalseå¦‚æœæ˜¯è™šæ‹Ÿè¾¹ç•Œ
    """
    # æ£€æŸ¥ä¸‹ç•Œè™šæ‹Ÿè¾¹ç•Œ
    if x < 0 or y < 0:
        return False
    # æ£€æŸ¥ä¸Šç•Œè™šæ‹Ÿè¾¹ç•Œ  
    if x >= len_sents1 or y >= len_sents2:
        return False
    # æ£€æŸ¥ç‰¹æ®Šè™šæ‹Ÿè¾¹ç•Œ
    if (x == -1 and y == -1):
        return False
    return True


def safe_diagonal_range(j, i, X, coeff, delta_y, max_j):
    """
    å®‰å…¨çš„å¯¹è§’çº¿YèŒƒå›´è®¡ç®—ï¼Œé¿å…æç«¯è´Ÿæ•°ç´¢å¼•
    
    å‚æ•°ï¼š
    j (int): å½“å‰ç‚¹çš„yåæ ‡
    i (int): å½“å‰ç‚¹çš„xåæ ‡
    X (int): æ‰«æçš„xåæ ‡
    coeff (float): å¥å­é•¿åº¦æ¯”ä¾‹ç³»æ•°
    delta_y (int): yæ–¹å‘çª—å£åŠå¾„
    max_j (int): yåæ ‡çš„æœ€å¤§ç´¢å¼•
    
    è¿”å›å€¼ï¼š
    range: å®‰å…¨çš„yåæ ‡èŒƒå›´
    """
    diagonal_offset = (i - X) * coeff
    y_center = j - diagonal_offset
    
    # é˜²æ­¢æç«¯è´Ÿæ•°æˆ–è¿‡å¤§çš„æ•°å€¼
    if y_center < -1000 or y_center > max_j + 1000:
        return range(0, 0)  # è¿”å›ç©ºèŒƒå›´
    
    y_range_start = int(max(0, y_center - delta_y))
    y_range_end = int(min(y_center + delta_y + 1, max_j + 1))
    
    # ç¡®ä¿èŒƒå›´æœ‰æ•ˆ
    if y_range_start >= y_range_end:
        return range(0, 0)
        
    return range(y_range_start, y_range_end)


########################################################################### ç‚¹è¿‡æ»¤å‡½æ•°

def compute_local_density(params, i, j, points, max_i, max_j, sim_mat):
    """
    è®¡ç®—æ²¿å¯¹è§’çº¿çš„å±€éƒ¨å¯†åº¦ - è¿™æ˜¯é”šç‚¹è´¨é‡è¯„ä¼°çš„æ ¸å¿ƒç®—æ³•
    
    ç®—æ³•åŸç†ï¼š
    - åœ¨å½“å‰ç‚¹(i,j)å‘¨å›´å®šä¹‰ä¸€ä¸ªå±€éƒ¨çª—å£
    - è®¡ç®—ä¸‰ç§ä¸åŒä½ç½®çš„å±€éƒ¨ç©ºé—´å¯†åº¦ï¼šå‰å‘ã€åå‘ã€å±…ä¸­
    - è¿”å›ä¸‰ç§å¯†åº¦ä¸­çš„æœ€å¤§å€¼ä½œä¸ºè¯¥ç‚¹çš„å±€éƒ¨å¯†åº¦
    - é«˜å¯†åº¦åŒºåŸŸçš„ç‚¹æ›´å¯èƒ½æ˜¯çœŸæ­£çš„å¯¹é½é”šç‚¹
    
    å‚æ•°ï¼š
    params (dict): å…¨å±€å‚æ•°å­—å…¸ï¼ŒåŒ…å«deltaXã€deltaYç­‰çª—å£å¤§å°å‚æ•°
    i (int): å½“å‰ç‚¹çš„xåæ ‡ï¼ˆè‹±æ–‡å¥å­ç´¢å¼•ï¼‰
    j (int): å½“å‰ç‚¹çš„yåæ ‡ï¼ˆä¸­æ–‡å¥å­ç´¢å¼•ï¼‰
    points (dict): æ‰€æœ‰å€™é€‰é”šç‚¹çš„å­—å…¸ {(x,y): 1}
    max_i (int): è‹±æ–‡å¥å­çš„æœ€å¤§ç´¢å¼•
    max_j (int): ä¸­æ–‡å¥å­çš„æœ€å¤§ç´¢å¼•
    sim_mat (numpy.ndarray): ç›¸ä¼¼åº¦çŸ©é˜µ
    
    è¿”å›å€¼ï¼š
    float: å±€éƒ¨é”šç‚¹å¯†åº¦å€¼ï¼ˆ0-1ä¹‹é—´ï¼Œè¶Šé«˜è¡¨ç¤ºè¯¥ç‚¹å‘¨å›´é”šç‚¹è¶Šå¯†é›†ï¼‰
    """
    
    # === è‡ªé€‚åº”çª—å£å¤§å°è®¡ç®— ===
    len_sents1 = max_i + 1
    len_sents2 = max_j + 1
    
    adaptive_delta_x = max(1, len_sents1 // 2)
    adaptive_delta_y = max(1, len_sents2 // 2)
    
    if 'deltaX' in params and params['deltaX'] is not None and params['deltaX'] > 0:
        delta_x = params['deltaX']
        use_adaptive_x = False
    else:
        delta_x = adaptive_delta_x
        use_adaptive_x = True
        
    if 'deltaY' in params and params['deltaY'] is not None and params['deltaY'] > 0:
        delta_y = params['deltaY'] 
        use_adaptive_y = False
    else:
        delta_y = adaptive_delta_y
        use_adaptive_y = True
    
    if params.get('veryVerbose', False):
        print(f"  === çª—å£å¤§å°è®¡ç®— ===")
        print(f"  å¥å­æ•°é‡: è‹±æ–‡{len_sents1}å¥, ä¸­æ–‡{len_sents2}å¥")
        print(f"  è‡ªé€‚åº”è®¡ç®—: deltaX={adaptive_delta_x} (è‹±æ–‡{len_sents1}Ã·2), deltaY={adaptive_delta_y} (ä¸­æ–‡{len_sents2}Ã·2)")
        print(f"  å®é™…ä½¿ç”¨: deltaX={delta_x}{'(è‡ªé€‚åº”)' if use_adaptive_x else '(ç”¨æˆ·è®¾ç½®)'}, deltaY={delta_y}{'(è‡ªé€‚åº”)' if use_adaptive_y else '(ç”¨æˆ·è®¾ç½®)'}")
        print(f"  è®¡ç®—ç‚¹({i},{j})çš„å±€éƒ¨å¯†åº¦...")
        
    coeff = len_sents2 / len_sents1 if params['sentRatio'] == 0 else params['sentRatio']
    
    if params.get('veryVerbose', False):
        print(f"  å¥å­é•¿åº¦æ¯”ä¾‹ç³»æ•°: {coeff:.3f} (ä¸­æ–‡{len_sents2}å¥ / è‹±æ–‡{len_sents1}å¥)")
    
    local_space_size_before = 0
    nb_points_in_local_space_size_before = 0
    local_space_size_centered = 0
    nb_points_in_local_space_size_centered = 0
    local_space_size_after = 0
    nb_points_in_local_space_size_after = 0

    x_range_start = max(0, i - 2 * delta_x)
    x_range_end = min(i + 2 * delta_x + 1, max_i + 1)
    
    if params.get('veryVerbose', False):
        print(f"  æ‰«æXèŒƒå›´: [{x_range_start}, {x_range_end}) (çª—å£åŠå¾„2Ã—{delta_x}={2*delta_x})")
    
    scanned_points = 0
    
    for X in range(x_range_start, x_range_end):
        y_range = safe_diagonal_range(j, i, X, coeff, delta_y, max_j)
        
        for Y in y_range:
            scanned_points += 1
            
            if X <= i:
                local_space_size_before += 1
                if (X, Y) in points.keys():
                    # ä½¿ç”¨é”šç‚¹çš„ç›¸ä¼¼åº¦æƒé‡ï¼Œå¦‚æœæ˜¯æ–°æ ¼å¼åˆ™ä»anchor_infoä¸­è·å–
                    anchor_value = points[(X, Y)]
                    if isinstance(anchor_value, dict):
                        weight = anchor_value.get('similarity', sim_mat[X, Y])
                    else:
                        weight = sim_mat[X, Y]  # å…¼å®¹æ—§æ ¼å¼
                    nb_points_in_local_space_size_before += weight
                    
            if X >= i:
                local_space_size_after += 1
                if (X, Y) in points.keys():
                    # ä½¿ç”¨é”šç‚¹çš„ç›¸ä¼¼åº¦æƒé‡
                    anchor_value = points[(X, Y)]
                    if isinstance(anchor_value, dict):
                        weight = anchor_value.get('similarity', sim_mat[X, Y])
                    else:
                        weight = sim_mat[X, Y]  # å…¼å®¹æ—§æ ¼å¼
                    nb_points_in_local_space_size_after += weight
                    
            if max(0, i - delta_x) <= X < min(i + delta_x + 1, max_i + 1):
                local_space_size_centered += 1
                if (X, Y) in points.keys():
                    # ä½¿ç”¨é”šç‚¹çš„ç›¸ä¼¼åº¦æƒé‡
                    anchor_value = points[(X, Y)]
                    if isinstance(anchor_value, dict):
                        weight = anchor_value.get('similarity', sim_mat[X, Y])
                    else:
                        weight = sim_mat[X, Y]  # å…¼å®¹æ—§æ ¼å¼
                    nb_points_in_local_space_size_centered += weight

    if params.get('veryVerbose', False):
        print(f"  æ€»å…±æ‰«æäº† {scanned_points} ä¸ªç½‘æ ¼ç‚¹")
        print(f"  ç©ºé—´å¤§å°ç»Ÿè®¡: å‰å‘={local_space_size_before}, å±…ä¸­={local_space_size_centered}, åå‘={local_space_size_after}")
        print(f"  ç›¸ä¼¼åº¦æƒé‡æ€»å’Œ: å‰å‘={nb_points_in_local_space_size_before:.3f}, å±…ä¸­={nb_points_in_local_space_size_centered:.3f}, åå‘={nb_points_in_local_space_size_after:.3f}")

    densityBefore = 0
    densityAfter = 0   
    densityCentered = 0
    
    if local_space_size_before > 0:
        densityBefore = nb_points_in_local_space_size_before / local_space_size_before
        
    if local_space_size_after > 0:
        densityAfter = nb_points_in_local_space_size_after / local_space_size_after
        
    if local_space_size_centered > 0:
        densityCentered = nb_points_in_local_space_size_centered / local_space_size_centered

    final_density = max(densityBefore, densityAfter, densityCentered)
    
    if params.get('veryVerbose', False):
        print(f"  å¯†åº¦è®¡ç®—ç»“æœ: å‰å‘={densityBefore:.4f}, å±…ä¸­={densityCentered:.4f}, åå‘={densityAfter:.4f}")
        print(f"  æœ€ç»ˆå¯†åº¦: {final_density:.4f}")
        print()
    
    return final_density


def filter_points(params, points, max_i, max_j, average_density, sim_mat):
    """
    é”šç‚¹è´¨é‡è¿‡æ»¤å™¨ - åŸºäºå±€éƒ¨å¯†åº¦åˆ†æç§»é™¤ä½è´¨é‡å€™é€‰é”šç‚¹
    """
    
    print(f"\n=== å¼€å§‹é”šç‚¹è´¨é‡è¿‡æ»¤ ===")
    print(f"åˆå§‹å€™é€‰é”šç‚¹æ•°é‡: {len(points)}")
    print(f"å…¨å±€å¹³å‡å¯†åº¦åŸºå‡†: {average_density:.4f}")
    print(f"å¯†åº¦æ¯”ç‡é˜ˆå€¼: {params.get('minDensityRatio', 'N/A')}")
    
    filtered_x = []
    filtered_y = []
    nbDeleted = 0
    nbPreserved = 0

    if params['veryVerbose']:
        print(f"å¼€å§‹é€ä¸€è¯„ä¼° {len(points)} ä¸ªå€™é€‰é”šç‚¹çš„è´¨é‡...")

    points_key = sorted(list(points.keys()), key=lambda point: point[0])
    print(f"å€™é€‰é”šç‚¹åˆ—è¡¨ï¼ˆæŒ‰xåæ ‡æ’åºï¼‰: {len(points_key)} ä¸ªç‚¹")

    for idx, point in enumerate(points_key):
        (i, j) = point

        localDensity = compute_local_density(params, i, j, points, max_i, max_j, sim_mat)
        
        density_ratio = localDensity / average_density if average_density > 0 else 0
        
        # è·å–å½“å‰é”šç‚¹ä¿¡æ¯
        anchor_info = points[(i, j)]
        anchor_id = anchor_info.get('id', 'N/A') if isinstance(anchor_info, dict) else 'legacy'
        similarity = anchor_info.get('similarity', sim_mat[i, j]) if isinstance(anchor_info, dict) else sim_mat[i, j]
        
        # è®¡ç®—ç»¼åˆè´¨é‡è¯„åˆ†ï¼ˆç›¸ä¼¼åº¦æƒé‡0.4 + å¯†åº¦æ¯”ç‡æƒé‡0.6ï¼‰
        quality_score = 0.4 * similarity + 0.6 * min(density_ratio, 1.0)
        
        if params['veryVerbose']:
            print(f"  é”šç‚¹#{anchor_id} {idx+1}/{len(points_key)}: ({i},{j})")
            print(f"    ç›¸ä¼¼åº¦: {similarity:.4f}")
            print(f"    å±€éƒ¨å¯†åº¦: {localDensity:.4f}")
            print(f"    å¯†åº¦æ¯”ç‡: {density_ratio:.4f} (é˜ˆå€¼: {params['minDensityRatio']})")
            print(f"    è´¨é‡è¯„åˆ†: {quality_score:.4f}")

        should_keep = False
        reason = ""
        
        if average_density <= 0:
            should_keep = True
            reason = "å…¨å±€å¯†åº¦å¼‚å¸¸ï¼Œä¿å®ˆä¿ç•™"
        elif density_ratio >= params['minDensityRatio']:
            should_keep = True
            reason = f"å¯†åº¦åˆæ ¼ ({density_ratio:.4f} >= {params['minDensityRatio']})"
        else:
            should_keep = False
            reason = f"å¯†åº¦è¿‡ä½ ({density_ratio:.4f} < {params['minDensityRatio']})"

        if should_keep:
            # æ›´æ–°é”šç‚¹ä¿¡æ¯ï¼Œæ·»åŠ è´¨é‡è¯„ä¼°ç»“æœ
            if isinstance(anchor_info, dict):
                anchor_info.update({
                    'local_density': float(localDensity),
                    'density_ratio': float(density_ratio),
                    'quality_score': float(quality_score),
                    'filter_status': 'passed',
                    'filter_reason': reason
                })
                points[(i, j)] = anchor_info
            
            filtered_x.append(i)
            filtered_y.append(j)
            nbPreserved += 1
            if params['veryVerbose']:
                print(f"    â†’ ä¿ç•™: {reason}")
        else:
            del points[(i, j)]
            nbDeleted += 1
            if params['veryVerbose']:
                print(f"    â†’ ç§»é™¤: {reason}")
        
        if params['veryVerbose']:
            print()

    print(f"=== é”šç‚¹è¿‡æ»¤å®Œæˆ ===")
    print(f"åˆå§‹é”šç‚¹: {len(points_key)} ä¸ª")
    print(f"ä¿ç•™é”šç‚¹: {nbPreserved} ä¸ª")
    print(f"ç§»é™¤é”šç‚¹: {nbDeleted} ä¸ª")
    print(f"ä¿ç•™ç‡: {(nbPreserved/(nbPreserved+nbDeleted)*100):.1f}%")
    print()

    return (points, filtered_x, filtered_y)


def resolving_conflicts(params, points, max_i, max_j, sim_mat):
    """
    ç§»é™¤åœ¨åŒä¸€åæ ‡è½´ä¸Šå†²çªçš„é”šç‚¹ï¼šä¼˜å…ˆä¿ç•™è´¨é‡è¯„åˆ†æ›´é«˜çš„é”šç‚¹
    """    
    print(f"\n=== å¼€å§‹è§£å†³é”šç‚¹å†²çª ===")
    
    x2y = {}
    y2x = {}
    filtered_x = []
    filtered_y = []
    nbDeleted = 0
    conflict_details = []
    
    points_key = list(points.keys())
    
    print(f"æ£€æŸ¥ {len(points_key)} ä¸ªé”šç‚¹çš„åæ ‡å†²çª...")
    
    for point in points_key:
        (i, j) = point
        current_anchor = points[point]
        
        # Xåæ ‡å†²çªæ£€æŸ¥
        if i in x2y.keys():
            if x2y[i] != j:
                existing_j = x2y[i]
                existing_point = (i, existing_j)
                
                if existing_point in points:
                    existing_anchor = points[existing_point]
                    
                    # æ¯”è¾ƒé”šç‚¹è´¨é‡
                    current_quality = current_anchor.get('quality_score', 0) if isinstance(current_anchor, dict) else 0
                    existing_quality = existing_anchor.get('quality_score', 0) if isinstance(existing_anchor, dict) else 0
                    
                    current_id = current_anchor.get('id', 'N/A') if isinstance(current_anchor, dict) else 'legacy'
                    existing_id = existing_anchor.get('id', 'N/A') if isinstance(existing_anchor, dict) else 'legacy'
                    
                    nbDeleted += 1
                    
                    if current_quality > existing_quality:
                        del points[existing_point]
                        x2y[i] = j
                        conflict_details.append(f"Xå†²çª: ä¿ç•™é”šç‚¹#{current_id}({i},{j})[è´¨é‡:{current_quality:.3f}], ç§»é™¤é”šç‚¹#{existing_id}({i},{existing_j})[è´¨é‡:{existing_quality:.3f}]")
                    else:
                        del points[point]
                        conflict_details.append(f"Xå†²çª: ä¿ç•™é”šç‚¹#{existing_id}({i},{existing_j})[è´¨é‡:{existing_quality:.3f}], ç§»é™¤é”šç‚¹#{current_id}({i},{j})[è´¨é‡:{current_quality:.3f}]")
                        continue
        else:
            x2y[i] = j

        # Yåæ ‡å†²çªæ£€æŸ¥
        if j in y2x.keys():
            if y2x[j] != i:
                existing_i = y2x[j]
                existing_point = (existing_i, j)
                
                if existing_point in points:
                    existing_anchor = points[existing_point]
                    
                    # æ¯”è¾ƒé”šç‚¹è´¨é‡
                    current_quality = current_anchor.get('quality_score', 0) if isinstance(current_anchor, dict) else 0
                    existing_quality = existing_anchor.get('quality_score', 0) if isinstance(existing_anchor, dict) else 0
                    
                    current_id = current_anchor.get('id', 'N/A') if isinstance(current_anchor, dict) else 'legacy'
                    existing_id = existing_anchor.get('id', 'N/A') if isinstance(existing_anchor, dict) else 'legacy'
                    
                    nbDeleted += 1
                    
                    if current_quality > existing_quality:
                        del points[existing_point]
                        y2x[j] = i
                        conflict_details.append(f"Yå†²çª: ä¿ç•™é”šç‚¹#{current_id}({i},{j})[è´¨é‡:{current_quality:.3f}], ç§»é™¤é”šç‚¹#{existing_id}({existing_i},{j})[è´¨é‡:{existing_quality:.3f}]")
                    else:
                        del points[point]
                        conflict_details.append(f"Yå†²çª: ä¿ç•™é”šç‚¹#{existing_id}({existing_i},{j})[è´¨é‡:{existing_quality:.3f}], ç§»é™¤é”šç‚¹#{current_id}({i},{j})[è´¨é‡:{current_quality:.3f}]")
        else:
            y2x[j] = i

    if params.get('verbose', False) or params.get('veryVerbose', False):
        print(f"âœ… å†²çªè§£å†³å®Œæˆ:")
        print(f"  ç§»é™¤å†²çªé”šç‚¹: {nbDeleted} ä¸ª")
        if conflict_details and params.get('veryVerbose', False):
            print(f"  å†²çªè§£å†³è¯¦æƒ…:")
            for detail in conflict_details:
                print(f"    {detail}")

    # ç”Ÿæˆæœ€ç»ˆçš„é”šç‚¹åˆ—è¡¨
    points_key = list(points.keys())
    for point in points_key:
        (i, j) = point
        filtered_x.append(i)
        filtered_y.append(j)
        
    print(f"  æœ€ç»ˆä¿ç•™é”šç‚¹: {len(filtered_x)} ä¸ª")
    print()
    
    return (points, filtered_x, filtered_y)


########################################################################### ç›¸ä¼¼åº¦çŸ©é˜µè¯»å–å‡½æ•°

def load_sentences_data(sentences_file_path, sentence_id):
    """
    ä»JSONæ–‡ä»¶ä¸­è¯»å–æŒ‡å®šsentence_idçš„è‹±æ–‡å’Œä¸­æ–‡å¥å­æ•°æ®
    """
    
    print(f"\n=== å¼€å§‹åŠ è½½å¥å­æ•°æ® ===")
    print(f"æ–‡ä»¶è·¯å¾„: {sentences_file_path}")
    print(f"ç›®æ ‡å¥å­ID: {sentence_id}")
    
    try:
        print("æ­£åœ¨è¯»å–JSONæ–‡ä»¶...")
        with open(sentences_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"æ–‡ä»¶è¯»å–æˆåŠŸï¼ŒåŒ…å« {len(data)} ä¸ªå¥å­å¯¹")
        
        print(f"æ­£åœ¨æœç´¢sentence_id={sentence_id}çš„æ•°æ®...")
        for idx, item in enumerate(data):
            current_id = item.get('sentence_id')
            print(f"  æ£€æŸ¥ç¬¬ {idx+1} é¡¹: sentence_id = {current_id}")
            
            if current_id == sentence_id:
                english_sentences = item.get('english_sentence_text', [])
                chinese_sentences = item.get('chinese_sentence_text', [])
                
                print(f"âœ“ æ‰¾åˆ°ç›®æ ‡æ•°æ®!")
                print(f"  è‹±æ–‡å¥å­æ•°é‡: {len(english_sentences)}")
                print(f"  ä¸­æ–‡å¥å­æ•°é‡: {len(chinese_sentences)}")
                
                if english_sentences:
                    print(f"  è‹±æ–‡å¥å­è¯¦æƒ…:")
                    for i, sent in enumerate(english_sentences):
                        print(f"    [{i}] {sent}")
                        
                if chinese_sentences:
                    print(f"  ä¸­æ–‡å¥å­è¯¦æƒ…:")
                    for i, sent in enumerate(chinese_sentences):
                        print(f"    [{i}] {sent}")
                
                print("å¥å­æ•°æ®åŠ è½½å®Œæˆ\n")
                return english_sentences, chinese_sentences
        
        available_ids = [item.get('sentence_id', 'N/A') for item in data]
        error_msg = f"åœ¨æ–‡ä»¶ä¸­æœªæ‰¾åˆ°sentence_idä¸º{sentence_id}çš„æ•°æ®ã€‚å¯ç”¨çš„sentence_id: {available_ids}"
        print(f"âœ— é”™è¯¯: {error_msg}")
        raise ValueError(error_msg)
        
    except FileNotFoundError:
        error_msg = f"å¥å­æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {sentences_file_path}"
        print(f"âœ— æ–‡ä»¶é”™è¯¯: {error_msg}")
        raise FileNotFoundError(error_msg)
    except json.JSONDecodeError as e:
        error_msg = f"JSONæ–‡ä»¶æ ¼å¼é”™è¯¯: {sentences_file_path}, é”™è¯¯è¯¦æƒ…: {str(e)}"
        print(f"âœ— JSONè§£æé”™è¯¯: {error_msg}")
        raise ValueError(error_msg)


def load_similarity_matrix(matrix_file_path, sentence_id):
    """
    ä»JSONæ–‡ä»¶ä¸­è¯»å–æŒ‡å®šsentence_idçš„ç›¸ä¼¼åº¦çŸ©é˜µ
    """
    
    print(f"\n=== å¼€å§‹åŠ è½½ç›¸ä¼¼åº¦çŸ©é˜µ ===")
    print(f"çŸ©é˜µæ–‡ä»¶è·¯å¾„: {matrix_file_path}")
    print(f"ç›®æ ‡å¥å­ID: {sentence_id}")
    
    try:
        print("æ­£åœ¨è¯»å–ç›¸ä¼¼åº¦çŸ©é˜µæ–‡ä»¶...")
        with open(matrix_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"æ–‡ä»¶è¯»å–æˆåŠŸï¼ŒåŒ…å« {len(data)} ä¸ªçŸ©é˜µæ•°æ®")
        
        print(f"æ­£åœ¨æœç´¢sentence_id={sentence_id}çš„ç›¸ä¼¼åº¦çŸ©é˜µ...")
        for idx, item in enumerate(data):
            current_id = item.get('sentence_id')
            print(f"  æ£€æŸ¥ç¬¬ {idx+1} é¡¹: sentence_id = {current_id}")
            
            if current_id == sentence_id:
                print(f"âœ“ æ‰¾åˆ°ç›®æ ‡ç›¸ä¼¼åº¦çŸ©é˜µ!")
                
                matrix_data = item.get('semantic_similarity_matrix')
                if matrix_data is None:
                    raise ValueError(f"sentence_id={sentence_id}çš„æ•°æ®ä¸­ç¼ºå°‘'semantic_similarity_matrix'å­—æ®µ")
                
                print("æ­£åœ¨è½¬æ¢ä¸ºnumpyæ•°ç»„...")
                matrix = np.array(matrix_data, dtype=np.float64)
                
                if len(matrix.shape) != 2:
                    raise ValueError(f"ç›¸ä¼¼åº¦çŸ©é˜µå¿…é¡»æ˜¯äºŒç»´çš„ï¼Œå½“å‰ç»´åº¦: {matrix.shape}")
                
                rows, cols = matrix.shape
                print(f"  çŸ©é˜µç»´åº¦: {rows} x {cols} (è‹±æ–‡å¥å­ x ä¸­æ–‡å¥å­)")
                print(f"  æ•°æ®ç±»å‹: {matrix.dtype}")
                
                print(f"  å®Œæ•´ç›¸ä¼¼åº¦çŸ©é˜µ ({rows}x{cols}):")
                for i in range(rows):
                    row_values = [f"{matrix[i,j]:.4f}" for j in range(cols)]
                    print(f"    [{i}] {' '.join(row_values)}")
                
                max_pos = np.unravel_index(np.argmax(matrix), matrix.shape)
                print(f"  æœ€é«˜ç›¸ä¼¼åº¦ä½ç½®: ({max_pos[0]}, {max_pos[1]}) = {matrix[max_pos]:.4f}")
                
                print("ç›¸ä¼¼åº¦çŸ©é˜µåŠ è½½å®Œæˆ\n")
                return matrix
        
        available_ids = [item.get('sentence_id', 'N/A') for item in data]
        error_msg = f"åœ¨æ–‡ä»¶ä¸­æœªæ‰¾åˆ°sentence_idä¸º{sentence_id}çš„æ•°æ®ã€‚å¯ç”¨çš„sentence_id: {available_ids}"
        print(f"âœ— é”™è¯¯: {error_msg}")
        raise ValueError(error_msg)
        
    except FileNotFoundError:
        error_msg = f"ç›¸ä¼¼åº¦çŸ©é˜µæ–‡ä»¶æœªæ‰¾åˆ°: {matrix_file_path}"
        print(f"âœ— æ–‡ä»¶é”™è¯¯: {error_msg}")
        raise FileNotFoundError(error_msg)
    except json.JSONDecodeError as e:
        error_msg = f"JSONæ–‡ä»¶æ ¼å¼é”™è¯¯: {matrix_file_path}, é”™è¯¯è¯¦æƒ…: {str(e)}"
        print(f"âœ— JSONè§£æé”™è¯¯: {error_msg}")
        raise ValueError(error_msg)


def extract_candidate_points_from_matrix(params, sim_mat):
    """
    åŸºäºç›¸ä¼¼åº¦çŸ©é˜µçš„æ™ºèƒ½å€™é€‰é”šç‚¹æå–å™¨ - å¢å¼ºç‰ˆæœ¬æ”¯æŒé”šç‚¹ç¼–å·å’Œè¯¦ç»†ä¿¡æ¯
    """
    
    print(f"\n=== å¼€å§‹æå–å€™é€‰é”šç‚¹ ===")
    
    len_sents1, len_sents2 = sim_mat.shape
    print(f"ç›¸ä¼¼åº¦çŸ©é˜µç»´åº¦: {len_sents1} Ã— {len_sents2} (è‹±æ–‡ Ã— ä¸­æ–‡)")
    
    points = {}
    x = []
    y = []
    
    threshold = params.get('cosThreshold', 0.5)
    margin = params.get('margin', 0.1)
    
    print(f"ç®—æ³•å‚æ•°:")
    print(f"  ç›¸ä¼¼åº¦é˜ˆå€¼: {threshold}")
    print(f"  è¾¹è·é˜ˆå€¼: {margin}")
    
    max_dimension = max(len_sents1, len_sents2)
    k_best = math.ceil(max_dimension / 2)
    
    print(f"  æœ€å¤§çš„æ–‡æ¡£é•¿åº¦æˆ–å¥å­æ•°: {max_dimension}")
    print(f"  è‡ªé€‚åº”kå€¼: {k_best} (æ¯è¡Œæœ€å¤šé€‰æ‹©{k_best}ä¸ªé”šç‚¹)")
    print()
    
    anchor_id = 1  # é”šç‚¹ç¼–å·ä»1å¼€å§‹è‡ªåŠ¨é€’å¢
    total_candidates = 0
    threshold_filtered = 0
    margin_filtered = 0
    final_selected = 0
    
    print("å¼€å§‹é€è¡Œæ‰«æç›¸ä¼¼åº¦çŸ©é˜µ...")
    
    for i in range(len_sents1):
        row_similarities = sim_mat[i, :]
        
        if params.get('veryVerbose', False):
            print(f"\nå¤„ç†è‹±æ–‡å¥å­ {i}:")
            print(f"  ç›¸ä¼¼åº¦åˆ†å¸ƒ: min={np.min(row_similarities):.4f}, max={np.max(row_similarities):.4f}, mean={np.mean(row_similarities):.4f}")
        
        candidates = []
        for j in range(len_sents2):
            if row_similarities[j] > threshold:
                candidates.append((row_similarities[j], j))
            else:
                threshold_filtered += 1
        
        total_candidates += len(candidates)
        
        if not candidates:
            if params.get('veryVerbose', False):
                print(f"  â†’ æ— å€™é€‰ç‚¹ï¼ˆæ‰€æœ‰ç›¸ä¼¼åº¦å‡ä½äºé˜ˆå€¼{threshold}ï¼‰")
            continue
        
        if params.get('veryVerbose', False):
            print(f"  é€šè¿‡é˜ˆå€¼è¿‡æ»¤çš„å€™é€‰ç‚¹: {len(candidates)} ä¸ª")
            
        candidates.sort(key=lambda x: x[0], reverse=True)
        
        if params.get('veryVerbose', False):
            print(f"  æœ€ä½³å€™é€‰è¯¦æƒ…: {[(f'{score:.4f}', idx) for score, idx in candidates]}")
        
        if len(candidates) > 1:
            best_score = candidates[0][0]
            second_score = candidates[1][0]
            score_gap = best_score - second_score
            
            if score_gap < margin:
                if params.get('veryVerbose', False):
                    print(f"  â†’ è¾¹è·è¿‡æ»¤: æœ€ä½³åˆ†æ•°å·®è· {score_gap:.4f} < {margin} â†’ å»é™¤è¯¥è¡Œæ‰€æœ‰å€™é€‰ç‚¹")
                margin_filtered += len(candidates)
                continue
            else:
                if params.get('veryVerbose', False):
                    print(f"  â†’ è¾¹è·è¿‡æ»¤: æœ€ä½³åˆ†æ•°å·®è· {score_gap:.4f} >= {margin} â†’ ä¿ç•™å€™é€‰ç‚¹")
        
        selected_candidates = candidates[:min(k_best, len(candidates))]
        
        if params.get('veryVerbose', False):
            print(f"  æœ€ç»ˆé€‰æ‹©: {len(selected_candidates)} ä¸ªé”šç‚¹")
        
        for sim_score, j in selected_candidates:
            # åˆ›å»ºå¢å¼ºçš„é”šç‚¹ä¿¡æ¯
            anchor_info = {
                'id': anchor_id,
                'type': 'auto_detected',
                'similarity': float(sim_score),
                'source': 'threshold_filter',
                'rank_in_row': len([c for c in candidates if c[0] > sim_score]) + 1,
                'total_candidates_in_row': len(candidates)
            }
            points[(i, j)] = anchor_info
            x.append(i)
            y.append(j)
            final_selected += 1
            anchor_id += 1
            
            if params.get('veryVerbose', False):
                print(f"    é”šç‚¹#{anchor_info['id']} ({i},{j}): ç›¸ä¼¼åº¦={sim_score:.4f}, è¡Œå†…æ’å={anchor_info['rank_in_row']}/{anchor_info['total_candidates_in_row']}")
    
    print(f"\n=== å€™é€‰é”šç‚¹æå–å®Œæˆ ===")
    print(f"æ‰«æç»Ÿè®¡:")
    print(f"  æ€»æ‰«æç‚¹æ•°: {len_sents1 * len_sents2}")
    print(f"  é˜ˆå€¼è¿‡æ»¤æ‰: {threshold_filtered} ä¸ª")
    print(f"  è¾¹è·è¿‡æ»¤æ‰: {margin_filtered} ä¸ª")
    print(f"  æœ€ç»ˆé€‰ä¸­: {final_selected} ä¸ªå€™é€‰é”šç‚¹ï¼ˆç¼–å·1-{anchor_id-1}ï¼‰")
    
    if final_selected > 0 and params.get('verbose', False):
        print(f"å®Œæ•´é”šç‚¹åˆ†å¸ƒè¯¦æƒ…:")
        for idx in range(len(x)):
            anchor_info = points[(x[idx], y[idx])]
            print(f"  é”šç‚¹#{anchor_info['id']} ({x[idx]:2d}, {y[idx]:2d}): ç›¸ä¼¼åº¦={anchor_info['similarity']:.4f}, ç±»å‹={anchor_info['type']}")
    
    print()
    return points, x, y


def save_filtered_data_to_json(output_file_path, sentence_id, filtered_x, filtered_y, anchor_points, 
                              english_sentences, chinese_sentences, len_sents1, len_sents2, 
                              sim_mat, params, average_density):
    """
    å°†ç¬¬ä¸€éƒ¨åˆ†è¿‡æ»¤åçš„æ•°æ®ä¿å­˜åˆ°JSONæ–‡ä»¶ï¼Œä¾›ç¬¬äºŒéƒ¨åˆ†ä½¿ç”¨
    
    å‚æ•°ï¼š
    output_file_path (str): è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„
    sentence_id (int): å¥å­ID
    filtered_x (list): è¿‡æ»¤åçš„xåæ ‡åˆ—è¡¨
    filtered_y (list): è¿‡æ»¤åçš„yåæ ‡åˆ—è¡¨
    anchor_points (dict): é”šç‚¹å­—å…¸
    english_sentences (list): è‹±æ–‡å¥å­åˆ—è¡¨
    chinese_sentences (list): ä¸­æ–‡å¥å­åˆ—è¡¨
    len_sents1 (int): è‹±æ–‡å¥å­æ€»æ•°
    len_sents2 (int): ä¸­æ–‡å¥å­æ€»æ•°
    sim_mat (numpy.ndarray): ç›¸ä¼¼åº¦çŸ©é˜µ
    params (dict): å‚æ•°å­—å…¸
    average_density (float): å¹³å‡å¯†åº¦
    """
    
    print(f"\n=== ä¿å­˜è¿‡æ»¤åæ•°æ®åˆ°JSONæ–‡ä»¶ ===")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_file_path}")
    
    # é‡æ–°åˆ†é…è¿ç»­çš„é”šç‚¹ç¼–å·ï¼ˆä»1å¼€å§‹ï¼‰
    print(f"é‡æ–°åˆ†é…é”šç‚¹ç¼–å·ï¼Œç¡®ä¿è¿ç»­æ€§...")
    final_anchor_list = []
    
    for idx, (x, y) in enumerate(zip(filtered_x, filtered_y)):
        anchor_info = anchor_points.get((x, y), {})
        anchor_id = idx + 1  # è¿ç»­ç¼–å·ä»1å¼€å§‹
        
        if isinstance(anchor_info, dict):
            # ä»anchor_infoä¸­ç§»é™¤idï¼Œå°†å…¶æå‡åˆ°é¡¶çº§
            updated_anchor_info = dict(anchor_info)
            updated_anchor_info.pop('id', None)  # ç§»é™¤å†…éƒ¨çš„id
            updated_anchor_info['final_rank'] = idx + 1  # æ·»åŠ æœ€ç»ˆæ’å
        else:
            # å…¼å®¹æ—§æ ¼å¼
            updated_anchor_info = {
                'type': 'legacy',
                'similarity': float(sim_mat[x, y]),
                'final_rank': idx + 1
            }
        
        # å°†ç¼–å·æå‡åˆ°é”šç‚¹å¯¹è±¡çš„é¡¶çº§
        final_anchor_list.append({
            'id': anchor_id,
            'coordinates': [int(x), int(y)],
            'anchor_info': updated_anchor_info
        })
        
        print(f"  é”šç‚¹#{anchor_id}: ({x},{y}) ç›¸ä¼¼åº¦={updated_anchor_info.get('similarity', 'N/A'):.4f}")
    
    # æ„å»ºä¼˜åŒ–çš„æ•°æ®ç»“æ„
    data = {
        "sentence_id": sentence_id,
        "filtered_anchors": {
            "count": len(filtered_x),
            "anchors": final_anchor_list
        },
        "sentence_data": {
            "english_sentences": english_sentences,
            "chinese_sentences": chinese_sentences,
            "len_sents1": len_sents1,
            "len_sents2": len_sents2
        },
        "similarity_matrix": sim_mat.tolist(),
        "parameters": params,
        "statistics": {
            "average_density": average_density,
            "matrix_shape": list(sim_mat.shape),
            "final_anchor_count": len(filtered_x)
        }
    }
    
    try:
        with open(output_file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æ•°æ®å·²æˆåŠŸä¿å­˜åˆ°: {output_file_path}")
        print(f"  è¿‡æ»¤åé”šç‚¹æ•°é‡: {len(filtered_x)}")
        print(f"  é”šç‚¹å­—å…¸å¤§å°: {len(anchor_points)}")
        print(f"  è‹±æ–‡å¥å­æ•°: {len_sents1}")
        print(f"  ä¸­æ–‡å¥å­æ•°: {len_sents2}")
        print(f"  ç›¸ä¼¼åº¦çŸ©é˜µç»´åº¦: {sim_mat.shape}")
        print()
        
    except Exception as e:
        error_msg = f"ä¿å­˜JSONæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        print(f"âŒ {error_msg}")
        raise RuntimeError(error_msg)


def run_anchor_filtering_phase(params, sentences_file_path, matrix_file_path, sentence_id, 
                              output_json_path):
    """
    æ‰§è¡Œé”šç‚¹è¿‡æ»¤é˜¶æ®µï¼ˆç¬¬ä¸€éƒ¨åˆ†ï¼‰çš„ä¸»å‡½æ•°
    
    å‚æ•°ï¼š
    params (dict): å‚æ•°å­—å…¸
    sentences_file_path (str): å¥å­æ–‡ä»¶è·¯å¾„
    matrix_file_path (str): ç›¸ä¼¼åº¦çŸ©é˜µæ–‡ä»¶è·¯å¾„
    sentence_id (int): å¥å­ID
    output_json_path (str): è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„
    
    è¿”å›å€¼ï¼š
    tuple: (filtered_x, filtered_y, len_sents1, len_sents2)
    """
    
    print(f"\n" + "="*60)
    print(f"é”šç‚¹è¿‡æ»¤é˜¶æ®µï¼ˆç¬¬ä¸€éƒ¨åˆ†ï¼‰")
    print(f"="*60)
    
    try:
        # é˜¶æ®µ1ï¼šæ•°æ®åŠ è½½
        print(f"ğŸ“ é˜¶æ®µ1ï¼šæ•°æ®åŠ è½½")
        english_sentences, chinese_sentences = load_sentences_data(sentences_file_path, sentence_id)
        sim_mat = load_similarity_matrix(matrix_file_path, sentence_id)
        
        len_sents1, len_sents2 = sim_mat.shape
        
        if len_sents1 != len(english_sentences):
            raise ValueError(f"è‹±æ–‡æ•°æ®ä¸åŒ¹é…ï¼šçŸ©é˜µè¡Œæ•°{len_sents1} â‰  å¥å­æ•°{len(english_sentences)}")
        if len_sents2 != len(chinese_sentences):
            raise ValueError(f"ä¸­æ–‡æ•°æ®ä¸åŒ¹é…ï¼šçŸ©é˜µåˆ—æ•°{len_sents2} â‰  å¥å­æ•°{len(chinese_sentences)}")
        
        # é˜¶æ®µ2ï¼šå€™é€‰é”šç‚¹æå–å’Œç¼–å·
        print(f"\nğŸ¯ é˜¶æ®µ2ï¼šæ™ºèƒ½å€™é€‰é”šç‚¹æå–")
        points, x, y = extract_candidate_points_from_matrix(params, sim_mat)
        
        if len(points) == 0:
            print(f"âš ï¸  è­¦å‘Šï¼šæœªæ‰¾åˆ°ä»»ä½•å€™é€‰é”šç‚¹")
            return [], [], len_sents1, len_sents2
        
        # é˜¶æ®µ3ï¼šé”šç‚¹è´¨é‡è¿‡æ»¤
        print(f"\nğŸ” é˜¶æ®µ3ï¼šé”šç‚¹è´¨é‡è¿‡æ»¤")
        
        # åˆ›å»ºé”šç‚¹å·¥ä½œå‰¯æœ¬
        anchor_points = dict.copy(points)
        points_key = list(anchor_points.keys())
        
        # è®¡ç®—å…¨å±€å¹³å‡å¯†åº¦
        print(f"å¼€å§‹è®¡ç®— {len(points_key)} ä¸ªå€™é€‰é”šç‚¹çš„å±€éƒ¨å¯†åº¦...")
        
        tot_density = 0
        density_list = []
        
        for idx, point in enumerate(points_key):
            (x2, y2) = point
            local_density = compute_local_density(params, x2, y2, anchor_points, len_sents1 - 1, len_sents2 - 1, sim_mat)
            tot_density += local_density
            density_list.append(local_density)
            
            if params.get('veryVerbose', False):
                print(f"  ç‚¹ {idx+1}/{len(points_key)}: ({x2},{y2}) å¯†åº¦={local_density:.4f}")

        average_density = tot_density / float(len(points_key))
        
        max_density = max(density_list)
        min_density = min(density_list)
        density_std = np.std(density_list)
        
        print(f"âœ… å…¨å±€å¯†åº¦è®¡ç®—å®Œæˆ:")
        print(f"  å¹³å‡å¯†åº¦: {average_density:.4f}")
        print(f"  å¯†åº¦èŒƒå›´: [{min_density:.4f}, {max_density:.4f}]")
        print(f"  å¯†åº¦æ ‡å‡†å·®: {density_std:.4f}")

        # ç¬¬ä¸€è½®è¿‡æ»¤ï¼šç§»é™¤ä½å¯†åº¦ç‚¹
        print("ç¬¬ä¸€è½®è¿‡æ»¤ï¼šç§»é™¤å±€éƒ¨å¯†åº¦è¿‡ä½çš„é”šç‚¹...")
        (anchor_points, filtered_x, filtered_y) = filter_points(params, anchor_points, 
                                                                len_sents1 - 1, len_sents2 - 1, average_density, sim_mat)
        
        # è§£å†³å†²çªç‚¹
        print("å†²çªè§£å†³ï¼šå¤„ç†åŒä¸€åæ ‡çš„å¤šä¸ªå€™é€‰é”šç‚¹...")
        (anchor_points, filtered_x, filtered_y) = resolving_conflicts(params, anchor_points, 
                                                                      len_sents1 - 1, len_sents2 - 1, sim_mat)

        # å¯é€‰çš„ç¬¬äºŒè½®ä¸¥æ ¼è¿‡æ»¤
        if params.get('reiterateFiltering', False):
            print("ç¬¬äºŒè½®è¿‡æ»¤ï¼šåº”ç”¨æ›´ä¸¥æ ¼çš„å¯†åº¦æ ‡å‡†...")
            strict_threshold = average_density * 2
            print(f"ä½¿ç”¨ä¸¥æ ¼é˜ˆå€¼: {strict_threshold:.4f} (2å€å¹³å‡å¯†åº¦)")
            (anchor_points, filtered_x, filtered_y) = filter_points(params, anchor_points, 
                                                                    len_sents1 - 1, len_sents2 - 1,
                                                                    strict_threshold, sim_mat)

        print(f"âœ… é”šç‚¹è´¨é‡è¿‡æ»¤å®Œæˆ:")
        print(f"  æœ€ç»ˆä¿ç•™é”šç‚¹: {len(filtered_x)} ä¸ª")
        
        # æ˜¾ç¤ºæœ€ç»ˆé”šç‚¹åˆ—è¡¨è¯¦æƒ…
        if len(filtered_x) > 0:
            print(f"  ğŸ“ æœ€ç»ˆé”šç‚¹è¯¦æƒ…åˆ—è¡¨:")
            for idx, (x, y) in enumerate(zip(filtered_x, filtered_y)):
                anchor_info = anchor_points.get((x, y), {})
                anchor_id = anchor_info.get('id', 'N/A')
                similarity = anchor_info.get('similarity', sim_mat[x, y])
                quality_score = anchor_info.get('quality_score', 'N/A')
                anchor_type = anchor_info.get('type', 'unknown')
                print(f"    é”šç‚¹{idx+1}: #{anchor_id} ({x},{y}) ç›¸ä¼¼åº¦={similarity:.4f} è´¨é‡={quality_score:.4f} ç±»å‹={anchor_type}")
        
        # ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶
        save_filtered_data_to_json(output_json_path, sentence_id, filtered_x, filtered_y, anchor_points,
                                  english_sentences, chinese_sentences, len_sents1, len_sents2,
                                  sim_mat, params, average_density)
        
        print(f"âœ… ç¬¬ä¸€éƒ¨åˆ†å¤„ç†å®Œæˆï¼")
        print(f"="*60)
        
        return filtered_x, filtered_y, len_sents1, len_sents2
        
    except Exception as e:
        print(f"\nâŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯:")
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
        raise


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # å‚æ•°é…ç½®
    params = {
        'verbose': True,
        'veryVerbose': True,
        'cosThreshold': 0.5,
        'margin': 0.1,
        'minDensityRatio': 0.8,
        'detectIntervals': True,
        'sentRatio': 0,
        'maxDistToTheDiagonal': 10,
        'maxGapSize': 20,
        'minHorizontalDensity': 0.1,
        'localDiagBeam': 0.3,
        'reiterateFiltering': False,
        'kBest': 3,
    }
    
    # è‡ªåŠ¨æ£€æµ‹è·¯å¾„é…ç½®
    def get_data_paths():
        """æ ¹æ®å½“å‰å·¥ä½œç›®å½•è‡ªåŠ¨è°ƒæ•´æ•°æ®æ–‡ä»¶è·¯å¾„"""
        current_dir = os.path.basename(os.getcwd())
        if current_dir == "new-code":
            # ä» new-code ç›®å½•å†…è¿è¡Œ
            data_prefix = "../new-data/"
        else:
            # ä»é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
            data_prefix = "new-data/"

        return {
            "sentences_file_path": f"{data_prefix}sentence.json",
            "matrix_file_path": f"{data_prefix}Qwen3-Embedding-8B-output.json",
            "output_json_path": f"{data_prefix}anchor_filter-output.json"
        }

    # è·å–è·¯å¾„é…ç½®
    paths = get_data_paths()
    sentences_file_path = paths["sentences_file_path"]
    matrix_file_path = paths["matrix_file_path"]
    output_json_path = paths["output_json_path"]
    sentence_id = 0
    
    print("ğŸš€ å¼€å§‹æ‰§è¡Œé”šç‚¹è¿‡æ»¤é˜¶æ®µï¼ˆç¬¬ä¸€éƒ¨åˆ†ï¼‰...")
    
    try:
        filtered_x, filtered_y, len_sents1, len_sents2 = run_anchor_filtering_phase(
            params, sentences_file_path, matrix_file_path, sentence_id, output_json_path
        )
        
        print(f"\nğŸ‰ ç¬¬ä¸€éƒ¨åˆ†æ‰§è¡ŒæˆåŠŸï¼")
        print(f"  è¿‡æ»¤åé”šç‚¹æ•°é‡: {len(filtered_x)}")
        print(f"  æ•°æ®å·²ä¿å­˜è‡³: {output_json_path}")
        print(f"  è¯·è¿è¡Œç¬¬äºŒéƒ¨åˆ†ä»£ç ç»§ç»­å¤„ç†...")
        
    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œå¤±è´¥: {e}") 