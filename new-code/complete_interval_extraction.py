# -*- coding:utf8 -*-
"""
å®Œæ•´çš„é”šç‚¹æå–å’ŒåŒºé—´åˆ’åˆ†æ¨¡å— - å®Œå…¨å¤ç°å‚è€ƒä»£ç é€»è¾‘

æœ¬æ¨¡å—å®Œæ•´å¤ç° reference-code/anchor_points.py ä¸­çš„ extract_anchor_points å‡½æ•°ï¼Œ
åŒ…æ‹¬é”šç‚¹å¯†åº¦è¿‡æ»¤ååˆ°åŒºé—´åˆ’åˆ†å®Œæˆä¹‹é—´çš„æ‰€æœ‰å¤„ç†é€»è¾‘ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. å®Œæ•´å¤ç°å‚è€ƒä»£ç çš„åŒºé—´åˆ’åˆ†ç®—æ³•
2. åŒ…å«å•è°ƒæ€§æ£€æŸ¥ã€å¯†åº¦æ¯”ç‡æ£€æŸ¥ã€å‰ç»æ€§æ£€æŸ¥
3. å®ç°æœ€ç»ˆçš„å¯¹è§’çº¿è¿‡æ»¤æ­¥éª¤
4. ä¿æŒä¸å‚è€ƒä»£ç å®Œå…¨ä¸€è‡´çš„å¤„ç†é€»è¾‘

ä½œè€…ï¼šAI Assistant
æ—¥æœŸï¼š2025-01-29
"""

import os
import sys
import json
import math
import time
import numpy as np
from typing import List, Tuple, Dict, Any, Optional


def compute_local_density(params: Dict[str, Any], x: int, y: int, anchor_points: Dict[Tuple[int, int], int], 
                         len_sents1: int, len_sents2: int, sim_mat: np.ndarray) -> float:
    """
    è®¡ç®—å±€éƒ¨å¯†åº¦ - å¤ç°å‚è€ƒä»£ç é€»è¾‘
    
    å‚æ•°ï¼š
    params: å‚æ•°å­—å…¸
    x, y: é”šç‚¹åæ ‡
    anchor_points: é”šç‚¹å­—å…¸
    len_sents1, len_sents2: æ–‡æ¡£å¤§å°
    sim_mat: ç›¸ä¼¼åº¦çŸ©é˜µ
    
    è¿”å›å€¼ï¼š
    float: å±€éƒ¨å¯†åº¦å€¼
    """
    
    # ä½¿ç”¨å‚è€ƒä»£ç ä¸­çš„å¯†åº¦è®¡ç®—é€»è¾‘
    delta_x = params.get('deltaX', 10)
    delta_y = params.get('deltaY', 10)
    
    count = 0
    total_similarity = 0
    
    # è®¡ç®—çª—å£å†…çš„é”šç‚¹æ•°é‡å’Œç›¸ä¼¼åº¦
    for (ax, ay) in anchor_points.keys():
        if abs(ax - x) <= delta_x and abs(ay - y) <= delta_y:
            count += 1
            if 0 <= ax < sim_mat.shape[0] and 0 <= ay < sim_mat.shape[1]:
                total_similarity += sim_mat[ax, ay]
    
    # è®¡ç®—çª—å£é¢ç§¯
    window_area = (2 * delta_x + 1) * (2 * delta_y + 1)

    # è¿”å›å¯†åº¦å€¼ï¼ˆç»“åˆæ•°é‡å’Œç›¸ä¼¼åº¦ï¼‰
    if window_area > 0 and count > 0:
        return (count / window_area) * (total_similarity / count)
    return 0.0


def safe_character_count(sentences: List[str], start_idx: int, end_idx: int) -> int:
    """
    å®‰å…¨çš„å­—ç¬¦è®¡æ•°å‡½æ•°ï¼Œé¿å…è¶Šç•Œè®¿é—®å¥å­åˆ—è¡¨
    """
    char_count = 0
    safe_start = max(0, start_idx)
    safe_end = min(end_idx, len(sentences) - 1)
    
    if safe_start <= safe_end and safe_start < len(sentences):
        for n in range(safe_start, safe_end + 1):
            if 0 <= n < len(sentences):
                char_count += len(sentences[n])
    
    return char_count


def extract_anchor_points_complete(params: Dict[str, Any], filtered_x: List[int], filtered_y: List[int],
                                  anchor_points: Dict[Tuple[int, int], int], average_density: float,
                                  sents1: List[str], sents2: List[str], len_sents1: int, len_sents2: int,
                                  sim_mat: np.ndarray) -> Tuple[List[int], List[int], List[Tuple[Tuple[int, int], Tuple[int, int]]],
                                                               int, int, int, int]:
    """
    å®Œæ•´å¤ç°å‚è€ƒä»£ç çš„åŒºé—´åˆ’åˆ†å’Œç»“æœè¾“å‡ºé€»è¾‘ï¼ˆç¬¬3ã€4æ­¥éª¤ï¼‰

    ä¸¥æ ¼æŒ‰ç…§ reference-code/anchor_points.py ç¬¬649-875è¡Œçš„é€»è¾‘å®ç°ï¼š
    - STEP 8: å¯»æ‰¾å¯å¯¹é½åŒºé—´ (ç¬¬649-838è¡Œ)
    - æœ€ç»ˆå¯¹è§’çº¿è¿‡æ»¤ (ç¬¬843-872è¡Œ)
    - ä¸åŒ…å«é¢„å®šä¹‰é”šç‚¹é€»è¾‘ (è·³è¿‡ç¬¬662-673è¡Œ)
    - æ·»åŠ æ–‡æ¡£ç»“æŸç‚¹æ—¶æ£€æŸ¥é‡å¤ (æ”¹è¿›ç¬¬653-654è¡Œ)

    å‚æ•°ï¼š
    params: å‚æ•°å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰ç®—æ³•å‚æ•°
    filtered_x, filtered_y: å¯†åº¦è¿‡æ»¤åçš„é”šç‚¹åæ ‡åˆ—è¡¨
    anchor_points: é”šç‚¹å­—å…¸ï¼Œç”¨äºå¯†åº¦è®¡ç®—
    average_density: å…¨å±€å¹³å‡å¯†åº¦
    sents1, sents2: è‹±æ–‡å’Œä¸­æ–‡å¥å­åˆ—è¡¨
    len_sents1, len_sents2: æ–‡æ¡£å¥å­æ•°é‡
    sim_mat: ç›¸ä¼¼åº¦çŸ©é˜µ

    è¿”å›å€¼ï¼š
    Tuple: (final_filtered_x, final_filtered_y, intervals, interval_length_sent1, interval_length_sent2, interval_length_char1, interval_length_char2)
    - final_filtered_x, final_filtered_y: æœ€ç»ˆè¿‡æ»¤åçš„é”šç‚¹åæ ‡
    - intervals: å¯å¯¹é½åŒºé—´åˆ—è¡¨
    - interval_length_sent1, interval_length_sent2: åŒºé—´å†…å¥å­æ€»æ•°
    - interval_length_char1, interval_length_char2: åŒºé—´å†…å­—ç¬¦æ€»æ•°
    """

    print(f"\n" + "="*80)
    print(f"ğŸš€ å¼€å§‹å®Œæ•´çš„é”šç‚¹æå–å’ŒåŒºé—´åˆ’åˆ†ï¼ˆå®Œå…¨å¤ç°å‚è€ƒä»£ç ï¼‰")
    print(f"="*80)
    print(f"ğŸ“Š è¾“å…¥æ•°æ®æ¦‚è§ˆ:")
    print(f"   â€¢ è¾“å…¥é”šç‚¹æ•°é‡: {len(filtered_x)}")
    print(f"   â€¢ æ–‡æ¡£å¤§å°: è‹±æ–‡{len_sents1}å¥, ä¸­æ–‡{len_sents2}å¥")
    print(f"   â€¢ å¹³å‡å¯†åº¦: {average_density:.4f}")
    print(f"   â€¢ ç›¸ä¼¼åº¦çŸ©é˜µç»´åº¦: {sim_mat.shape}")
    print(f"   â€¢ é”šç‚¹å­—å…¸å¤§å°: {len(anchor_points)}")

    if len(filtered_x) > 0:
        print(f"ğŸ“ è¾“å…¥é”šç‚¹è¯¦æƒ…:")
        for i, (x, y) in enumerate(zip(filtered_x, filtered_y)):
            print(f"   é”šç‚¹{i+1}: ({x},{y})")

    # ç‰¹æ®Šæƒ…å†µï¼šå¦‚æœæ²¡æœ‰é”šç‚¹ï¼Œåˆ›å»ºè¦†ç›–æ•´ä¸ªæ–‡æ¡£çš„åŒºé—´
    if len(filtered_x) == 0:
        print(f"\nâš ï¸  ç‰¹æ®Šæƒ…å†µå¤„ç†ï¼šæ²¡æœ‰è¾“å…¥é”šç‚¹")
        print(f"   â†’ å°†åˆ›å»ºè¦†ç›–æ•´ä¸ªæ–‡æ¡£çš„å•ä¸€åŒºé—´")

        begin_int = (-1, -1)
        last_i = len_sents1 - 1
        last_j = len_sents2 - 1

        print(f"   â†’ è®¡ç®—åŒºé—´ç»Ÿè®¡ä¿¡æ¯...")
        interval_length_sent1 = last_i - begin_int[0]
        interval_length_sent2 = last_j - begin_int[1]
        interval_length_char1 = safe_character_count(sents1, 0, last_i)
        interval_length_char2 = safe_character_count(sents2, 0, last_j)
        intervals = [(begin_int, (last_i, last_j))]

        print(f"   â†’ è‹±æ–‡å¥å­æ•°: {interval_length_sent1}")
        print(f"   â†’ ä¸­æ–‡å¥å­æ•°: {interval_length_sent2}")
        print(f"   â†’ è‹±æ–‡å­—ç¬¦æ•°: {interval_length_char1}")
        print(f"   â†’ ä¸­æ–‡å­—ç¬¦æ•°: {interval_length_char2}")
        print(f"âœ… åˆ›å»ºå•ä¸€åŒºé—´: {intervals[0]}")
        print(f"   â†’ è¿”å›ç©ºé”šç‚¹åˆ—è¡¨å’Œå•ä¸€åŒºé—´")

        return [], [], intervals, interval_length_sent1, interval_length_sent2, interval_length_char1, interval_length_char2
    
    # =====> STEP 8 : finding aligned intervals (ä¸¥æ ¼å¤ç°å‚è€ƒä»£ç ç¬¬649-838è¡Œ)

    print(f"\n" + "="*60)
    print(f"ğŸ“ STEP 8: å¯»æ‰¾å¯å¯¹é½åŒºé—´")
    print(f"="*60)
    print(f"ğŸ”§ æ­¥éª¤8.1: åˆå§‹åŒ–åŒºé—´åˆ’åˆ†å˜é‡")

    # å¤ç°å‚è€ƒä»£ç ç¬¬651è¡Œï¼šåˆå§‹åŒ–åŒºé—´èµ·ç‚¹
    begin_int = (-1, -1)
    print(f"   â†’ åˆå§‹åŒ–åŒºé—´èµ·ç‚¹: {begin_int}")

    # å¤ç°å‚è€ƒä»£ç ç¬¬652-654è¡Œï¼šæ·»åŠ æ–‡æ¡£æœ«å°¾ä½œä¸ºé”šç‚¹ï¼Œä½†è¦æ£€æŸ¥é‡å¤
    print(f"ğŸ”§ æ­¥éª¤8.2: å‡†å¤‡å·¥ä½œé”šç‚¹åˆ—è¡¨")
    work_x = filtered_x.copy()
    work_y = filtered_y.copy()
    print(f"   â†’ å¤åˆ¶è¾“å…¥é”šç‚¹åˆ—è¡¨: {len(work_x)}ä¸ªé”šç‚¹")

    last_doc_x = len_sents1 - 1
    last_doc_y = len_sents2 - 1
    print(f"   â†’ æ–‡æ¡£æœ«å°¾åæ ‡: ({last_doc_x},{last_doc_y})")

    # æ”¹è¿›ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ æ–‡æ¡£æœ«å°¾é”šç‚¹ï¼ˆé¿å…é‡å¤ï¼‰
    if not work_x or work_x[-1] != last_doc_x or work_y[-1] != last_doc_y:
        work_x.append(last_doc_x)
        work_y.append(last_doc_y)
        print(f"   âœ… æ·»åŠ æ–‡æ¡£æœ«å°¾é”šç‚¹: ({last_doc_x},{last_doc_y})")
    else:
        print(f"   â„¹ï¸  æ–‡æ¡£æœ«å°¾é”šç‚¹å·²å­˜åœ¨ï¼Œæ— éœ€é‡å¤æ·»åŠ : ({last_doc_x},{last_doc_y})")

    # å¤ç°å‚è€ƒä»£ç ç¬¬655-660è¡Œï¼šåˆå§‹åŒ–å˜é‡
    print(f"ğŸ”§ æ­¥éª¤8.3: åˆå§‹åŒ–åŒºé—´åˆ’åˆ†çŠ¶æ€å˜é‡")
    last_i = 0
    last_j = 0
    intervals = []  # åŒºé—´å¯¹åˆ—è¡¨ (beginInt, endInt)
    nb_in_interval = 0

    print(f"   â†’ last_i (ä¸Šä¸€ä¸ªæœ‰æ•ˆé”šç‚¹X): {last_i}")
    print(f"   â†’ last_j (ä¸Šä¸€ä¸ªæœ‰æ•ˆé”šç‚¹Y): {last_j}")
    print(f"   â†’ intervals (åŒºé—´åˆ—è¡¨): ç©ºåˆ—è¡¨")
    print(f"   â†’ nb_in_interval (å½“å‰åŒºé—´å†…é”šç‚¹æ•°): {nb_in_interval}")

    # åˆå§‹åŒ–åŒºé—´é•¿åº¦ç»Ÿè®¡
    interval_length_sent1 = 0
    interval_length_sent2 = 0
    interval_length_char1 = 0
    interval_length_char2 = 0

    print(f"ğŸ”§ æ­¥éª¤8.4: æ˜¾ç¤ºæœ€ç»ˆå·¥ä½œé”šç‚¹åˆ—è¡¨")
    print(f"   â†’ å·¥ä½œé”šç‚¹æ€»æ•°: {len(work_x)}")
    for i, (x, y) in enumerate(zip(work_x, work_y)):
        print(f"   é”šç‚¹{i+1}: ({x},{y})")

    # è·³è¿‡é¢„å®šä¹‰é”šç‚¹é€»è¾‘ï¼ˆç¬¬662-673è¡Œï¼‰ï¼Œå› ä¸ºæˆ‘ä»¬ä¸ä½¿ç”¨é¢„å®šä¹‰é”šç‚¹
    print(f"ğŸ”§ æ­¥éª¤8.5: è·³è¿‡é¢„å®šä¹‰é”šç‚¹é€»è¾‘ï¼ˆæˆ‘ä»¬ä¸ä½¿ç”¨é¢„å®šä¹‰é”šç‚¹ï¼‰")
    print(f"   â†’ ç›´æ¥è¿›å…¥ detectIntervals åˆ†æ”¯")
    # ç›´æ¥è¿›å…¥ detectIntervals åˆ†æ”¯ï¼ˆä¸¥æ ¼å¤ç°å‚è€ƒä»£ç ç¬¬674-838è¡Œï¼‰

    if params.get('detectIntervals', True):
        print(f"\n" + "="*60)
        print(f"ğŸ” å¼€å§‹æ£€æµ‹å¯å¯¹é½åŒºé—´ï¼ˆdetectIntervals=Trueï¼‰")
        print(f"="*60)

        # å¤ç°å‚è€ƒä»£ç ç¬¬675è¡Œï¼šè®¡ç®—å¥å­é•¿åº¦æ¯”ä¾‹ç³»æ•°
        print(f"ğŸ”§ æ­¥éª¤8.6: è®¡ç®—å¥å­é•¿åº¦æ¯”ä¾‹ç³»æ•°")
        coeff = 1 if params.get('sentRatio', 0) == 0 else params.get('sentRatio', 1)
        print(f"   â†’ sentRatioå‚æ•°: {params.get('sentRatio', 0)}")
        print(f"   â†’ è®¡ç®—å¾—åˆ°çš„æ¯”ä¾‹ç³»æ•°: {coeff}")
        print(f"   â†’ ç”¨é€”: è®¡ç®—æœŸæœ›çš„Yåæ ‡ä½ç½®")

        # å¤ç°å‚è€ƒä»£ç ç¬¬676è¡Œï¼šéå†æ‰€æœ‰é”šç‚¹
        print(f"\nğŸ”§ æ­¥éª¤8.7: éå†æ‰€æœ‰å·¥ä½œé”šç‚¹è¿›è¡ŒåŒºé—´åˆ’åˆ†")
        print(f"   â†’ æ€»é”šç‚¹æ•°: {len(work_x)}")
        print(f"   â†’ å¼€å§‹é€ä¸ªå¤„ç†æ¯ä¸ªé”šç‚¹...")

        for num in range(0, len(work_x)):
            i, j = work_x[num], work_y[num]

            print(f"\n" + "-"*50)
            print(f"ğŸ” å¤„ç†é”šç‚¹ {num+1}/{len(work_x)}: ({i},{j})")
            print(f"-"*50)

            # å¤ç°å‚è€ƒä»£ç ç¬¬679-682è¡Œï¼šè®¡ç®—å±€éƒ¨å¯†åº¦å’Œå¯†åº¦æ¯”ç‡
            print(f"   ğŸ“Š æ­¥éª¤8.7.1: è®¡ç®—å±€éƒ¨å¯†åº¦å’Œå¯†åº¦æ¯”ç‡")
            local_density = compute_local_density(params, i, j, anchor_points, len_sents1, len_sents2, sim_mat)
            density_ratio = 0
            if average_density > 0:
                density_ratio = local_density / average_density

            print(f"      â†’ å±€éƒ¨å¯†åº¦: {local_density:.4f}")
            print(f"      â†’ å…¨å±€å¹³å‡å¯†åº¦: {average_density:.4f}")
            print(f"      â†’ å¯†åº¦æ¯”ç‡: {density_ratio:.4f}")

            # å¤ç°å‚è€ƒä»£ç ç¬¬684-687è¡Œï¼šè®¡ç®—ä¸å¯¹è§’çº¿çš„åå·®
            print(f"   ğŸ“ æ­¥éª¤8.7.2: è®¡ç®—ä¸å¯¹è§’çº¿çš„åå·®")
            expected_j = last_j + (i - last_i) * coeff
            vertical_deviation = abs(j - expected_j)
            new_interval = False

            print(f"      â†’ ä¸Šä¸€ä¸ªæœ‰æ•ˆé”šç‚¹: ({last_i},{last_j})")
            print(f"      â†’ å½“å‰é”šç‚¹: ({i},{j})")
            print(f"      â†’ æœŸæœ›Yåæ ‡: {expected_j:.1f}")
            print(f"      â†’ å®é™…Yåæ ‡: {j}")
            print(f"      â†’ å‚ç›´åå·®: {vertical_deviation:.1f}")

            if params.get('veryVerbose', False):
                print(f"      â†’ è¯¦ç»†è®¡ç®—: {last_j} + ({i} - {last_i}) * {coeff} = {expected_j:.1f}")
                print(f"      â†’ åå·®è®¡ç®—: |{j} - {expected_j:.1f}| = {vertical_deviation:.1f}")

            # å¤ç°å‚è€ƒä»£ç ç¬¬689-701è¡Œï¼šå•è°ƒæ€§çº¦æŸæ£€æŸ¥
            print(f"   ğŸ” æ­¥éª¤8.7.3: å•è°ƒæ€§çº¦æŸæ£€æŸ¥")
            if num > 1 and num < len(work_x) - 2:
                print(f"      â†’ æ£€æŸ¥ä½ç½®: é”šç‚¹{num+1}åœ¨åºåˆ—ä¸­é—´ï¼Œå¯ä»¥è¿›è¡Œå•è°ƒæ€§æ£€æŸ¥")
                print(f"      â†’ æ£€æŸ¥èŒƒå›´: é”šç‚¹{num-1}åˆ°é”šç‚¹{num+3}")

                x_monotonic = (work_x[num - 2] <= work_x[num - 1] <= work_x[num + 1] <= work_x[num + 2])
                y_monotonic = (work_y[num - 2] <= work_y[num - 1] <= work_y[num + 1] <= work_y[num + 2])
                x_in_range = (work_x[num - 1] <= i <= work_x[num + 1])
                y_in_range = (work_y[num - 1] <= j <= work_y[num + 1])

                print(f"      â†’ Xåºåˆ—å•è°ƒæ€§: {x_monotonic} ({work_x[num-2]} <= {work_x[num-1]} <= {work_x[num+1]} <= {work_x[num+2]})")
                print(f"      â†’ Yåºåˆ—å•è°ƒæ€§: {y_monotonic} ({work_y[num-2]} <= {work_y[num-1]} <= {work_y[num+1]} <= {work_y[num+2]})")
                print(f"      â†’ å½“å‰ç‚¹Xåœ¨èŒƒå›´å†…: {x_in_range} ({work_x[num-1]} <= {i} <= {work_x[num+1]})")
                print(f"      â†’ å½“å‰ç‚¹Yåœ¨èŒƒå›´å†…: {y_in_range} ({work_y[num-1]} <= {j} <= {work_y[num+1]})")

                if x_monotonic and y_monotonic and (not x_in_range or not y_in_range):
                    print(f"      âŒ å•è°ƒæ€§æ£€æŸ¥å¤±è´¥ï¼šé”šç‚¹({i},{j})ç ´åå•è°ƒæ€§ï¼Œè¢«å¿½ç•¥")
                    # å½“å‰ç‚¹è¢«è·³è¿‡
                    work_x[num] = last_i
                    work_y[num] = last_j
                    continue
                else:
                    print(f"      âœ… å•è°ƒæ€§æ£€æŸ¥é€šè¿‡")
            else:
                print(f"      â†’ è·³è¿‡å•è°ƒæ€§æ£€æŸ¥ï¼ˆé”šç‚¹ä½ç½®ï¼š{num+1}ï¼Œä¸åœ¨å¯æ£€æŸ¥èŒƒå›´å†…ï¼‰")

            # å¤ç°å‚è€ƒä»£ç ç¬¬703-713è¡Œï¼šåç¦»ä¸”ä½å¯†åº¦ç‚¹æ£€æŸ¥
            print(f"   ğŸ” æ­¥éª¤8.7.4: åç¦»ä¸”ä½å¯†åº¦ç‚¹æ£€æŸ¥")
            max_dist_half = params.get('maxDistToTheDiagonal', 10) / 2
            min_density_ratio = params.get('minDensityRatio', 0.8)

            is_deviating = vertical_deviation > max_dist_half
            is_backward = i < last_i or j < last_j
            is_low_density = density_ratio < min_density_ratio

            print(f"      â†’ åç¦»æ£€æŸ¥: å‚ç›´åå·®{vertical_deviation:.1f} > {max_dist_half} = {is_deviating}")
            print(f"      â†’ åé€€æ£€æŸ¥: ({i},{j}) < ({last_i},{last_j}) = {is_backward}")
            print(f"      â†’ ä½å¯†åº¦æ£€æŸ¥: å¯†åº¦æ¯”ç‡{density_ratio:.4f} < {min_density_ratio} = {is_low_density}")

            if (is_deviating or is_backward) and is_low_density:
                print(f"      âŒ åç¦»ä¸”ä½å¯†åº¦æ£€æŸ¥å¤±è´¥ï¼šé”šç‚¹({i},{j})è¢«å¿½ç•¥")
                print(f"         åŸå› : {'åç¦»å¯¹è§’çº¿' if is_deviating else ''}{'ä¸”' if is_deviating and is_backward else ''}{'ä½ç½®åé€€' if is_backward else ''}ä¸”å¯†åº¦è¿‡ä½")
                # å½“å‰ç‚¹è¢«è·³è¿‡
                work_x[num] = last_i
                work_y[num] = last_j
                continue
            else:
                print(f"      âœ… åç¦»ä¸”ä½å¯†åº¦æ£€æŸ¥é€šè¿‡")

            # å¤ç°å‚è€ƒä»£ç ç¬¬715-720è¡Œï¼šæ£€æŸ¥æ˜¯å¦åœ¨å¯¹è§’çº¿é™„è¿‘
            print(f"   ğŸ” æ­¥éª¤8.7.5: å¯¹è§’çº¿é™„è¿‘æ£€æŸ¥")
            max_dist_to_diagonal = params.get('maxDistToTheDiagonal', 10)
            print(f"      â†’ æœ€å¤§å…è®¸è·ç¦»: {max_dist_to_diagonal}")
            print(f"      â†’ å½“å‰å‚ç›´åå·®: {vertical_deviation:.1f}")

            if vertical_deviation <= max_dist_to_diagonal:
                nb_in_interval += 1
                last_i = i
                last_j = j
                print(f"      âœ… é”šç‚¹({i},{j})åœ¨å¯¹è§’çº¿é™„è¿‘ï¼Œè¢«æ¥å—")
                print(f"      â†’ æ›´æ–°last_i: {last_i}, last_j: {last_j}")
                print(f"      â†’ å½“å‰åŒºé—´å†…é”šç‚¹æ•°: {nb_in_interval}")
            else:
                # å¤ç°å‚è€ƒä»£ç ç¬¬722-723è¡Œï¼šåç¦»ç‚¹çš„è¯¦ç»†æ—¥å¿—
                print(f"      âš ï¸  é”šç‚¹({i},{j})åç¦»å¯¹è§’çº¿")
                print(f"      â†’ åå·®{vertical_deviation:.1f} > é˜ˆå€¼{max_dist_to_diagonal}")
                print(f"      â†’ è¿›å…¥åç¦»ç‚¹å¤„ç†æµç¨‹...")

                # å¤ç°å‚è€ƒä»£ç ç¬¬725-754è¡Œï¼šå‰ç»æ€§æ£€æŸ¥
                print(f"   ğŸ”® æ­¥éª¤8.7.6: å‰ç»æ€§æ£€æŸ¥ï¼ˆé¢„æµ‹åç»­é”šç‚¹è¡Œä¸ºï¼‰")
                preview_scope = 2
                print(f"      â†’ å‰ç»èŒƒå›´: {preview_scope}ä¸ªé”šç‚¹")

                if num + preview_scope < len(work_x):
                    next_i, next_j = work_x[num + preview_scope], work_y[num + preview_scope]
                    print(f"      â†’ å‰ç»é”šç‚¹: ({next_i},{next_j})")

                    # æ£€æŸ¥å‰ç»é”šç‚¹ä¸å‰ä¸€ä¸ªç‚¹çš„å¯¹é½æƒ…å†µ
                    next_expected_j = last_j + (next_i - last_i) * params.get('sentRatio', 1)
                    next_vertical_deviation = abs(next_j - next_expected_j)

                    print(f"      â†’ å‰ç»é”šç‚¹ç›¸å¯¹äºå‰ä¸€ä¸ªç‚¹çš„æœŸæœ›Y: {next_expected_j:.1f}")
                    print(f"      â†’ å‰ç»é”šç‚¹çš„å‚ç›´åå·®: {next_vertical_deviation:.1f}")

                    # ä¸‹ä¸€ä¸ªç‚¹ä¸å‰ä¸€ä¸ªç‚¹å¯¹é½
                    if next_vertical_deviation <= params.get('maxDistToTheDiagonal', 10):
                        print(f"      âŒ å‰ç»é”šç‚¹ä¸å‰ä¸€ä¸ªç‚¹å¯¹é½ï¼Œå¿½ç•¥å½“å‰ç‚¹")
                        print(f"         â†’ å½“å‰ç‚¹({i},{j})è¢«è·³è¿‡")
                        # å½“å‰ç‚¹è¢«è·³è¿‡
                        work_x[num] = last_i
                        work_y[num] = last_j
                        continue
                    else:
                        # æ£€æŸ¥ä¸‹ä¸€ä¸ªç‚¹æ˜¯å¦ä¸å½“å‰ç‚¹å¯¹é½
                        print(f"      â†’ æ£€æŸ¥å‰ç»é”šç‚¹æ˜¯å¦ä¸å½“å‰ç‚¹å¯¹é½...")
                        next_expected_j = j + (next_i - i) * params.get('sentRatio', 1)
                        next_vertical_deviation = abs(next_j - next_expected_j)

                        print(f"      â†’ å‰ç»é”šç‚¹ç›¸å¯¹äºå½“å‰ç‚¹çš„æœŸæœ›Y: {next_expected_j:.1f}")
                        print(f"      â†’ å‰ç»é”šç‚¹ç›¸å¯¹äºå½“å‰ç‚¹çš„åå·®: {next_vertical_deviation:.1f}")

                        # å¦‚æœä¸‹ä¸€ä¸ªç‚¹ä¸å½“å‰ç‚¹å¯¹é½ï¼Œåˆ™åº”è¯¥åˆ›å»ºæ–°åŒºé—´
                        if next_vertical_deviation <= params.get('maxDistToTheDiagonal', 10) and density_ratio > params.get('minDensityRatio', 0.8):
                            print(f"      âœ… å‰ç»é”šç‚¹ä¸å½“å‰ç‚¹å¯¹é½ä¸”å¯†åº¦è¶³å¤Ÿï¼Œä¿ç•™å½“å‰ç‚¹ç”¨äºæ–°åŒºé—´")
                            new_interval = True
                        else:
                            print(f"      âŒ å‰ç»é”šç‚¹ä¸ä¸å½“å‰ç‚¹å¯¹é½æˆ–å¯†åº¦ä¸è¶³ï¼Œå¿½ç•¥å½“å‰ç‚¹")
                            print(f"         â†’ åå·®: {next_vertical_deviation:.1f}, å¯†åº¦æ¯”ç‡: {density_ratio:.4f}")
                            # å½“å‰ç‚¹è¢«è·³è¿‡
                            work_x[num] = last_i
                            work_y[num] = last_j
                            continue
                else:
                    print(f"      â†’ æ— æ³•è¿›è¡Œå‰ç»æ£€æŸ¥ï¼ˆå‰©ä½™é”šç‚¹ä¸è¶³{preview_scope}ä¸ªï¼‰")

                # å¤ç°å‚è€ƒä»£ç ç¬¬767-770è¡Œï¼šè®¡ç®—è·ç¦»ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦åˆ›å»ºæ–°åŒºé—´
                print(f"   ğŸ“ æ­¥éª¤8.7.7: è·ç¦»é—´éš™æ£€æŸ¥")
                d = math.sqrt((i - last_i) ** 2 + (j - last_j) ** 2)
                max_gap_size = params.get('maxGapSize', 20)
                high_density_threshold = 1.5

                print(f"      â†’ å½“å‰ç‚¹åˆ°ä¸Šä¸€ä¸ªæœ‰æ•ˆç‚¹çš„è·ç¦»: {d:.2f}")
                print(f"      â†’ æœ€å¤§å…è®¸é—´éš™: {max_gap_size}")
                print(f"      â†’ é«˜å¯†åº¦é˜ˆå€¼: {high_density_threshold}")
                print(f"      â†’ å½“å‰å¯†åº¦æ¯”ç‡: {density_ratio:.4f}")

                # å¦‚æœæœ‰é—´éš™ï¼Œå‰ä¸€ä¸ªåŒºé—´è¢«å…³é—­ï¼Œæ–°åŒºé—´å°†å¼€å§‹
                if d > max_gap_size and density_ratio > high_density_threshold:
                    print(f"      âœ… è·ç¦»è¿‡å¤§ä¸”å¯†åº¦é«˜ï¼Œéœ€è¦åˆ›å»ºæ–°åŒºé—´")
                    print(f"         â†’ è·ç¦»{d:.2f} > é˜ˆå€¼{max_gap_size} ä¸” å¯†åº¦{density_ratio:.4f} > {high_density_threshold}")
                    new_interval = True
                else:
                    print(f"      â†’ è·ç¦»é—´éš™æ£€æŸ¥é€šè¿‡ï¼Œç»§ç»­å½“å‰åŒºé—´")

            # å¤ç°å‚è€ƒä»£ç ç¬¬774-820è¡Œï¼šåˆ›å»ºæ–°åŒºé—´ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if new_interval:
                print(f"\n   ğŸ”„ æ­¥éª¤8.7.8: åˆ›å»ºæ–°åŒºé—´")
                end_int = (last_i, last_j)
                print(f"      â†’ å…³é—­å½“å‰åŒºé—´: {begin_int} â†’ {end_int}")
                print(f"      â†’ è§¦å‘ç‚¹: ({i},{j})")

                if begin_int[0] < last_i and begin_int[1] < last_j:
                    # ä¸ºäº†ä¿å­˜åŒºé—´ï¼Œæˆ‘ä»¬æ ¹æ®æ°´å¹³å®½åº¦è®¡ç®—é€‰å®šç‚¹çš„å¯†åº¦
                    print(f"      â†’ éªŒè¯åŒºé—´æœ‰æ•ˆæ€§...")
                    horizontal_width = last_i - begin_int[0]
                    horizontal_density = nb_in_interval / horizontal_width if horizontal_width > 0 else 0
                    min_horizontal_density = params.get('minHorizontalDensity', 0.1)

                    print(f"         â€¢ åŒºé—´æ°´å¹³å®½åº¦: {horizontal_width}")
                    print(f"         â€¢ åŒºé—´å†…é”šç‚¹æ•°: {nb_in_interval}")
                    print(f"         â€¢ æ°´å¹³å¯†åº¦: {horizontal_density:.4f}")
                    print(f"         â€¢ æœ€å°å¯†åº¦è¦æ±‚: {min_horizontal_density}")
                    print(f"         â€¢ æœ€å°é”šç‚¹æ•°è¦æ±‚: 2")

                    if horizontal_density >= min_horizontal_density and nb_in_interval > 1:
                        print(f"      âœ… åŒºé—´æœ‰æ•ˆï¼Œä¿å­˜åˆ°åŒºé—´åˆ—è¡¨")
                        intervals.append((begin_int, end_int))

                        # è®¡ç®—åŒºé—´ç»Ÿè®¡
                        sent_count_1 = last_i - begin_int[0] + 1
                        sent_count_2 = last_j - begin_int[1] + 1
                        interval_length_sent1 += sent_count_1
                        interval_length_sent2 += sent_count_2

                        print(f"         â€¢ è‹±æ–‡å¥å­æ•°: {sent_count_1}")
                        print(f"         â€¢ ä¸­æ–‡å¥å­æ•°: {sent_count_2}")

                        # è®¡ç®—å­—ç¬¦æ•°
                        char_count_1 = 0
                        char_count_2 = 0
                        for n in range(max(0, begin_int[0]), last_i + 1):
                            if n < len(sents1):
                                char_count_1 += len(sents1[n])
                        for n in range(max(0, begin_int[1]), last_j + 1):
                            if n < len(sents2):
                                char_count_2 += len(sents2[n])

                        interval_length_char1 += char_count_1
                        interval_length_char2 += char_count_2

                        print(f"         â€¢ è‹±æ–‡å­—ç¬¦æ•°: {char_count_1}")
                        print(f"         â€¢ ä¸­æ–‡å­—ç¬¦æ•°: {char_count_2}")
                    else:
                        print(f"      âŒ åŒºé—´æ— æ•ˆï¼Œè¢«ä¸¢å¼ƒï¼ˆå¯†åº¦è¿‡ä½æˆ–é”šç‚¹ä¸è¶³ï¼‰")
                        print(f"         åŸå› : å¯†åº¦{horizontal_density:.4f} < {min_horizontal_density} æˆ– é”šç‚¹æ•°{nb_in_interval} <= 1")
                else:
                    print(f"      â†’ åŒºé—´æ— æ•ˆï¼ˆèµ·ç‚¹ä¸å°äºç»ˆç‚¹ï¼‰ï¼Œè·³è¿‡ä¿å­˜")

                print(f"      â†’ å¼€å§‹æ–°åŒºé—´: ({i},{j})")
                begin_int = (i, j)
                nb_in_interval = 0

                # å¤ç°å‚è€ƒä»£ç ç¬¬817-820è¡Œï¼šæ›´æ–°lastIå’ŒlastJ
                print(f"      â†’ æ›´æ–°æœ‰æ•ˆé”šç‚¹ä½ç½®: ({i},{j})")
                last_i = i
                last_j = j
    else:
        # å¤ç°å‚è€ƒä»£ç ç¬¬821-823è¡Œï¼šå¦‚æœä¸æ£€æµ‹åŒºé—´ï¼Œç›´æ¥ä½¿ç”¨æ•´ä¸ªæ–‡æ¡£
        print(f"\n" + "="*60)
        print(f"âš ï¸  detectIntervals=Falseï¼Œä½¿ç”¨æ•´ä¸ªæ–‡æ¡£ä½œä¸ºå•ä¸€åŒºé—´")
        print(f"="*60)
        last_i = len_sents1 - 1
        last_j = len_sents2 - 1
        print(f"   â†’ è®¾ç½®æœ€åæœ‰æ•ˆä½ç½®ä¸ºæ–‡æ¡£æœ«å°¾: ({last_i},{last_j})")

    # å¤ç°å‚è€ƒä»£ç ç¬¬829-838è¡Œï¼šå…³é—­æœ€åä¸€ä¸ªåŒºé—´
    print(f"\n" + "="*60)
    print(f"ğŸ”š å…³é—­æœ€åä¸€ä¸ªåŒºé—´")
    print(f"="*60)
    print(f"   â†’ æ£€æŸ¥æ˜¯å¦éœ€è¦å…³é—­æœ€ååŒºé—´...")
    print(f"   â†’ å½“å‰åŒºé—´èµ·ç‚¹: {begin_int}")
    print(f"   â†’ æœ€åæœ‰æ•ˆä½ç½®: ({last_i},{last_j})")

    if last_i != begin_int[0]:
        print(f"   âœ… éœ€è¦å…³é—­æœ€ååŒºé—´")

        # closing last interval
        sent_count_1 = last_i - begin_int[0] + 1
        sent_count_2 = last_j - begin_int[1] + 1
        interval_length_sent1 += sent_count_1
        interval_length_sent2 += sent_count_2

        print(f"   â†’ è®¡ç®—æœ€ååŒºé—´ç»Ÿè®¡...")
        print(f"      â€¢ è‹±æ–‡å¥å­æ•°: {sent_count_1}")
        print(f"      â€¢ ä¸­æ–‡å¥å­æ•°: {sent_count_2}")

        # è®¡ç®—å­—ç¬¦æ•°
        char_count_1 = 0
        char_count_2 = 0
        for n in range(max(0, begin_int[0]), last_i + 1):
            if n < len(sents1):
                char_count_1 += len(sents1[n])
        for n in range(max(0, begin_int[1]), last_j + 1):
            if n < len(sents2):
                char_count_2 += len(sents2[n])

        interval_length_char1 += char_count_1
        interval_length_char2 += char_count_2

        print(f"      â€¢ è‹±æ–‡å­—ç¬¦æ•°: {char_count_1}")
        print(f"      â€¢ ä¸­æ–‡å­—ç¬¦æ•°: {char_count_2}")

        intervals.append((begin_int, (last_i, last_j)))
        print(f"   âœ… æœ€ååŒºé—´å·²ä¿å­˜: {begin_int} â†’ ({last_i},{last_j})")
    else:
        print(f"   â†’ æ— éœ€å…³é—­æœ€ååŒºé—´ï¼ˆèµ·ç‚¹ä¸ç»ˆç‚¹ç›¸åŒï¼‰")

    print(f"\nğŸ“Š åŒºé—´åˆ’åˆ†å®Œæˆç»Ÿè®¡:")
    print(f"   â†’ æ€»åŒºé—´æ•°: {len(intervals)}")
    print(f"   â†’ æ€»è‹±æ–‡å¥å­æ•°: {interval_length_sent1}")
    print(f"   â†’ æ€»ä¸­æ–‡å¥å­æ•°: {interval_length_sent2}")
    print(f"   â†’ æ€»è‹±æ–‡å­—ç¬¦æ•°: {interval_length_char1}")
    print(f"   â†’ æ€»ä¸­æ–‡å­—ç¬¦æ•°: {interval_length_char2}")

    if intervals:
        print(f"\nğŸ“‹ æ‰€æœ‰åŒºé—´è¯¦æƒ…:")
        for idx, (begin, end) in enumerate(intervals):
            print(f"   åŒºé—´{idx+1}: {begin} â†’ {end}")
    
    print(f"\n=== åŒºé—´åˆ’åˆ†å®Œæˆ ===")
    print(f"æ€»åŒºé—´æ•°: {len(intervals)}")
    print(f"åŒºé—´è¦†ç›–ç»Ÿè®¡:")
    print(f"  è‹±æ–‡å¥å­: {interval_length_sent1}/{len_sents1}")
    print(f"  ä¸­æ–‡å¥å­: {interval_length_sent2}/{len_sents2}")
    print(f"  è‹±æ–‡å­—ç¬¦: {interval_length_char1}")
    print(f"  ä¸­æ–‡å­—ç¬¦: {interval_length_char2}")
    
    # æ˜¾ç¤ºæ‰€æœ‰åŒºé—´è¯¦æƒ…
    if intervals:
        print(f"\nğŸ“‹ åŒºé—´è¯¦æƒ…åˆ—è¡¨:")
        for idx, (begin, end) in enumerate(intervals):
            print(f"  åŒºé—´{idx+1}: {begin} â†’ {end}")
    
    # å¤ç°å‚è€ƒä»£ç ç¬¬843-875è¡Œï¼šæœ€ç»ˆè¿‡æ»¤æ­¥éª¤ - å¯¹äºæ¯ä¸ªåŒºé—´ï¼Œè·ç¦»å¯¹è§’çº¿å¤ªè¿œçš„ç‚¹è¢«ä¸¢å¼ƒ
    print(f"\n" + "="*80)
    print(f"ğŸ” æœ€ç»ˆå¯¹è§’çº¿è¿‡æ»¤æ­¥éª¤ï¼ˆç¬¬4æ­¥éª¤çš„ä¸€éƒ¨åˆ†ï¼‰")
    print(f"="*80)
    print(f"ğŸ“Š è¿‡æ»¤å‰çŠ¶æ€:")
    print(f"   â†’ é”šç‚¹æ€»æ•°: {len(work_x)}")
    print(f"   â†’ åŒºé—´æ€»æ•°: {len(intervals)}")

    if work_x:
        print(f"   â†’ è¿‡æ»¤å‰é”šç‚¹åˆ—è¡¨:")
        for idx, (x, y) in enumerate(zip(work_x, work_y)):
            print(f"      é”šç‚¹{idx+1}: ({x},{y})")

    removed_count = 0
    i = 0

    for interval_idx, (begin, end) in enumerate(intervals):
        x_begin, y_begin = begin
        x_end, y_end = end

        print(f"\nğŸ” å¤„ç†åŒºé—´{interval_idx+1}: {begin} â†’ {end}")

        if (x_end - x_begin) * (y_end - y_begin) == 0:
            print(f"   â†’ è·³è¿‡æ— æ•ˆåŒºé—´ï¼ˆé¢ç§¯ä¸º0ï¼‰")
            continue

        print(f"   â†’ åŒºé—´æœ‰æ•ˆï¼Œå¼€å§‹è¿‡æ»¤åŒºé—´å†…çš„é”šç‚¹...")

        # å¯»æ‰¾åŒºé—´å†…çš„é”šç‚¹
        print(f"   â†’ å¯»æ‰¾åŒºé—´å†…çš„é”šç‚¹ï¼ˆä»ç´¢å¼•{i}å¼€å§‹ï¼‰...")
        while i < len(work_x) and work_x[i] < x_begin:
            print(f"      è·³è¿‡åŒºé—´å¤–é”šç‚¹: ({work_x[i]},{work_y[i]})")
            i += 1

        # å¦‚æœç‚¹iè½åœ¨xåŒºé—´å†…
        points_in_interval = 0
        while i < len(work_x) and work_x[i] >= x_begin and work_x[i] <= x_end:
            points_in_interval += 1
            delete = False
            current_x, current_y = work_x[i], work_y[i]

            print(f"\n   ğŸ“ æ£€æŸ¥é”šç‚¹{i+1}: ({current_x},{current_y})")

            # å¦‚æœç‚¹iä¸åœ¨yåŒºé—´å†…ï¼Œåˆ é™¤ç‚¹
            if current_y < y_begin or current_y > y_end:
                delete = True
                print(f"      âŒ Yåæ ‡è¶…å‡ºåŒºé—´èŒƒå›´: {current_y} âˆ‰ [{y_begin},{y_end}]")

            # è®¡ç®—æœŸæœ›çš„Yåæ ‡ï¼ˆåœ¨åŒºé—´å¯¹è§’çº¿ä¸Šï¼‰
            expected_y = y_begin + (current_x - x_begin) / (x_end - x_begin) * (y_end - y_begin)
            print(f"      â†’ æœŸæœ›Yåæ ‡ï¼ˆå¯¹è§’çº¿ä¸Šï¼‰: {expected_y:.2f}")

            # è®¡ç®—åå·®
            relative_deviation = abs((current_y - expected_y) / (y_end - y_begin)) if (y_end - y_begin) > 0 else 0
            absolute_deviation = abs(current_y - expected_y)

            local_diag_beam = params.get('localDiagBeam', 0.3)
            max_dist_to_diagonal = params.get('maxDistToTheDiagonal', 10)

            print(f"      â†’ ç›¸å¯¹åå·®: {relative_deviation:.4f} (é˜ˆå€¼: {local_diag_beam})")
            print(f"      â†’ ç»å¯¹åå·®: {absolute_deviation:.2f} (é˜ˆå€¼: {max_dist_to_diagonal})")

            # å¦‚æœç‚¹iè·ç¦»å¯¹è§’çº¿å¤ªè¿œï¼Œåˆ é™¤ç‚¹
            if relative_deviation > local_diag_beam or absolute_deviation > max_dist_to_diagonal:
                delete = True
                print(f"      âŒ è·ç¦»å¯¹è§’çº¿å¤ªè¿œ")
                if relative_deviation > local_diag_beam:
                    print(f"         ç›¸å¯¹åå·® {relative_deviation:.4f} > {local_diag_beam}")
                if absolute_deviation > max_dist_to_diagonal:
                    print(f"         ç»å¯¹åå·® {absolute_deviation:.2f} > {max_dist_to_diagonal}")

            if delete:
                print(f"      ğŸ—‘ï¸  åˆ é™¤é”šç‚¹({current_x},{current_y})")
                del work_x[i]
                del work_y[i]
                removed_count += 1
                if i >= len(work_x):
                    break
            else:
                print(f"      âœ… ä¿ç•™é”šç‚¹({current_x},{current_y})")
                i += 1

        print(f"   â†’ åŒºé—´{interval_idx+1}å¤„ç†å®Œæˆï¼Œæ£€æŸ¥äº†{points_in_interval}ä¸ªé”šç‚¹")

    print(f"\nğŸ“Š æœ€ç»ˆè¿‡æ»¤å®Œæˆç»Ÿè®¡:")
    print(f"   â†’ è¿‡æ»¤åé”šç‚¹æ•°é‡: {len(work_x)}")
    print(f"   â†’ ç§»é™¤é”šç‚¹æ•°é‡: {removed_count}")

    if work_x:
        print(f"   â†’ æœ€ç»ˆä¿ç•™çš„é”šç‚¹:")
        for idx, (x, y) in enumerate(zip(work_x, work_y)):
            print(f"      é”šç‚¹{idx+1}: ({x},{y})")
    else:
        print(f"   â†’ æ‰€æœ‰é”šç‚¹éƒ½è¢«è¿‡æ»¤æ‰äº†")

    return work_x, work_y, intervals, interval_length_sent1, interval_length_sent2, interval_length_char1, interval_length_char2


def load_filtered_anchor_data_complete(json_file_path: str) -> Dict[str, Any]:
    """
    ä»JSONæ–‡ä»¶ä¸­åŠ è½½å¯†åº¦è¿‡æ»¤åçš„é”šç‚¹æ•°æ®ï¼ˆå®Œæ•´ç‰ˆæœ¬ï¼‰


    1. è¯¦ç»†çš„æ•°æ®éªŒè¯å’Œå®Œæ•´æ€§æ£€æŸ¥
    2. ä¸°å¯Œçš„é”šç‚¹ä¿¡æ¯æå–å’Œå±•ç¤º
    3. ç»“æ„åŒ–çš„æ•°æ®ç»„ç»‡å’Œé”™è¯¯å¤„ç†
    4. æ”¯æŒå¤šé”šç‚¹çš„è¯¦ç»†ä¿¡æ¯ç®¡ç†

    å‚æ•°ï¼š
    json_file_path (str): è¿‡æ»¤åé”šç‚¹æ•°æ®çš„JSONæ–‡ä»¶è·¯å¾„

    è¿”å›å€¼ï¼š
    Dict[str, Any]: åŒ…å«æ‰€æœ‰å¿…è¦æ•°æ®çš„å­—å…¸ï¼Œæ”¯æŒå¤šé”šç‚¹å¤„ç†
    """

    print(f"\n" + "="*80)
    print(f"ğŸ“‚ åŠ è½½è¿‡æ»¤åçš„é”šç‚¹æ•°æ®ï¼ˆå®Œæ•´å¢å¼ºç‰ˆæœ¬ï¼‰")
    print(f"="*80)
    print(f"ğŸ“ æ•°æ®æ–‡ä»¶è·¯å¾„: {json_file_path}")

    try:
        print(f"ğŸ”„ æ­£åœ¨è¯»å–JSONæ–‡ä»¶...")
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        print(f"âœ… æ–‡ä»¶è¯»å–æˆåŠŸ")

        # æå–åŸºæœ¬ä¿¡æ¯
        print(f"\nğŸ” æ­¥éª¤1: æå–åŸºæœ¬ä¿¡æ¯")
        sentence_id = data.get('sentence_id')
        filtered_anchors = data.get('filtered_anchors', {})
        sentence_data = data.get('sentence_data', {})
        similarity_matrix = data.get('similarity_matrix', [])
        parameters = data.get('parameters', {})
        statistics = data.get('statistics', {})

        print(f"   â†’ å¥å­ID: {sentence_id}")
        print(f"   â†’ æ•°æ®ç»“æ„å®Œæ•´æ€§: {'âœ…' if all([filtered_anchors, sentence_data, similarity_matrix]) else 'âš ï¸'}")

        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        print(f"\nğŸ” æ­¥éª¤2: éªŒè¯æ•°æ®å®Œæ•´æ€§")
        anchor_count = filtered_anchors.get('count', 0)
        anchors = filtered_anchors.get('anchors', [])

        if anchor_count != len(anchors):
            print(f"âš ï¸  è­¦å‘Šï¼šé”šç‚¹æ•°é‡ä¸åŒ¹é… - å£°æ˜{anchor_count}ä¸ªï¼Œå®é™…{len(anchors)}ä¸ª")
        else:
            print(f"âœ… é”šç‚¹æ•°é‡éªŒè¯é€šè¿‡: {anchor_count}ä¸ª")

        # è¯¦ç»†çš„æ•°æ®æ¦‚è§ˆ
        print(f"\nğŸ“Š æ•°æ®æ¦‚è§ˆ:")
        print(f"   â€¢ å¥å­ID: {sentence_id}")
        print(f"   â€¢ è¿‡æ»¤åé”šç‚¹æ•°é‡: {anchor_count}")
        print(f"   â€¢ è‹±æ–‡å¥å­æ•°: {sentence_data.get('len_sents1', 0)}")
        print(f"   â€¢ ä¸­æ–‡å¥å­æ•°: {sentence_data.get('len_sents2', 0)}")
        print(f"   â€¢ ç›¸ä¼¼åº¦çŸ©é˜µç»´åº¦: {statistics.get('matrix_shape', 'N/A')}")
        print(f"   â€¢ å¹³å‡å¯†åº¦: {statistics.get('average_density', 'N/A'):.4f}")

        # æå–é”šç‚¹åæ ‡å’Œæ„å»ºå¢å¼ºçš„é”šç‚¹å­—å…¸
        print(f"\nğŸ” æ­¥éª¤3: æ„å»ºå¢å¼ºçš„é”šç‚¹æ•°æ®ç»“æ„")
        filtered_x = []
        filtered_y = []
        anchor_points = {}  # ç®€å•é”šç‚¹å­—å…¸ï¼ˆç”¨äºå¯†åº¦è®¡ç®—ï¼‰
        anchor_details = {}  # è¯¦ç»†é”šç‚¹ä¿¡æ¯å­—å…¸
        anchor_list = []     # é”šç‚¹åˆ—è¡¨ï¼ˆä¿æŒé¡ºåºï¼‰

        print(f"ğŸ“ é”šç‚¹è¯¦æƒ…:")
        for anchor in anchors:
            anchor_id = anchor.get('id')
            coordinates = anchor.get('coordinates', [])
            anchor_info = anchor.get('anchor_info', {})

            if len(coordinates) == 2:
                x, y = coordinates
                filtered_x.append(x)
                filtered_y.append(y)

                # ç®€å•é”šç‚¹å­—å…¸ï¼ˆå…¼å®¹åŸæœ‰é€»è¾‘ï¼‰
                anchor_points[(x, y)] = 1

                # è¯¦ç»†é”šç‚¹ä¿¡æ¯å­—å…¸
                anchor_details[(x, y)] = {
                    'id': anchor_id,
                    'coordinates': [x, y],
                    'type': anchor_info.get('type', 'unknown'),
                    'similarity': anchor_info.get('similarity', 0.0),
                    'source': anchor_info.get('source', 'unknown'),
                    'rank_in_row': anchor_info.get('rank_in_row', 0),
                    'total_candidates_in_row': anchor_info.get('total_candidates_in_row', 0),
                    'local_density': anchor_info.get('local_density', 0.0),
                    'density_ratio': anchor_info.get('density_ratio', 0.0),
                    'quality_score': anchor_info.get('quality_score', 0.0),
                    'filter_status': anchor_info.get('filter_status', 'unknown'),
                    'filter_reason': anchor_info.get('filter_reason', ''),
                    'final_rank': anchor_info.get('final_rank', 0)
                }

                # é”šç‚¹åˆ—è¡¨ï¼ˆä¿æŒé¡ºåºï¼‰
                anchor_list.append({
                    'id': anchor_id,
                    'coordinates': [x, y],
                    'info': anchor_details[(x, y)]
                })

                # æ˜¾ç¤ºé”šç‚¹è¯¦æƒ…
                similarity = anchor_info.get('similarity', 0.0)
                quality_score = anchor_info.get('quality_score', 0.0)
                density_ratio = anchor_info.get('density_ratio', 0.0)
                print(f"   é”šç‚¹#{anchor_id}: ({x},{y}) ç›¸ä¼¼åº¦={similarity:.4f} è´¨é‡={quality_score:.4f} å¯†åº¦æ¯”={density_ratio:.4f}")

        # è½¬æ¢ç›¸ä¼¼åº¦çŸ©é˜µ
        print(f"\nğŸ” æ­¥éª¤4: å¤„ç†ç›¸ä¼¼åº¦çŸ©é˜µ")
        sim_mat = np.array(similarity_matrix, dtype=np.float64)
        print(f"   â†’ ç›¸ä¼¼åº¦çŸ©é˜µå½¢çŠ¶: {sim_mat.shape}")
        print(f"   â†’ çŸ©é˜µæ•°æ®ç±»å‹: {sim_mat.dtype}")
        print(f"   â†’ çŸ©é˜µå€¼èŒƒå›´: [{sim_mat.min():.4f}, {sim_mat.max():.4f}]")

        # æ„å»ºå¢å¼ºçš„è¿”å›ç»“æœ
        result = {
            # åŸºæœ¬ä¿¡æ¯
            'sentence_id': sentence_id,
            'anchor_count': anchor_count,

            # é”šç‚¹æ•°æ®ï¼ˆå¤šç§æ ¼å¼æ”¯æŒï¼‰
            'filtered_x': filtered_x,
            'filtered_y': filtered_y,
            'anchor_points': anchor_points,      # ç®€å•å­—å…¸ï¼ˆå…¼å®¹æ€§ï¼‰
            'anchor_details': anchor_details,    # è¯¦ç»†ä¿¡æ¯å­—å…¸
            'anchor_list': anchor_list,          # æœ‰åºåˆ—è¡¨

            # ç»Ÿè®¡ä¿¡æ¯
            'average_density': statistics.get('average_density', 0.0),
            'matrix_shape': statistics.get('matrix_shape', sim_mat.shape),

            # å¥å­æ•°æ®
            'english_sentences': sentence_data.get('english_sentences', []),
            'chinese_sentences': sentence_data.get('chinese_sentences', []),
            'len_sents1': sentence_data.get('len_sents1', 0),
            'len_sents2': sentence_data.get('len_sents2', 0),

            # çŸ©é˜µæ•°æ®
            'sim_mat': sim_mat,
            'similarity_matrix': similarity_matrix,  # åŸå§‹æ ¼å¼

            # å…ƒæ•°æ®
            'parameters': parameters,
            'statistics': statistics,
            'source_file': json_file_path,
            'load_timestamp': time.strftime("%Y-%m-%d %H:%M:%S")
        }

        print(f"\nâœ… æ•°æ®åŠ è½½å®Œæˆ")
        print(f"   â†’ æˆåŠŸåŠ è½½ {len(filtered_x)} ä¸ªé”šç‚¹")
        print(f"   â†’ æ–‡æ¡£å¤§å°: è‹±æ–‡{result['len_sents1']}å¥, ä¸­æ–‡{result['len_sents2']}å¥")
        print(f"   â†’ æ•°æ®ç»“æ„: 3ç§é”šç‚¹æ ¼å¼ + å®Œæ•´å…ƒæ•°æ®")
        print()

        return result

    except FileNotFoundError:
        error_msg = f"æ–‡ä»¶æœªæ‰¾åˆ°: {json_file_path}"
        print(f"âŒ {error_msg}")
        raise FileNotFoundError(error_msg)
    except json.JSONDecodeError as e:
        error_msg = f"JSONè§£æé”™è¯¯: {str(e)}"
        print(f"âŒ {error_msg}")
        raise ValueError(error_msg)
    except Exception as e:
        error_msg = f"åŠ è½½æ•°æ®æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}"
        print(f"âŒ {error_msg}")
        raise RuntimeError(error_msg)


def save_complete_results_to_json(output_file_path: str, sentence_id: int,
                                 final_x: List[int], final_y: List[int],
                                 intervals: List[Tuple[Tuple[int, int], Tuple[int, int]]],
                                 interval_stats: Dict[str, int],
                                 anchor_data: Dict[str, Any], params: Dict[str, Any]) -> None:
    """
    å°†å®Œæ•´çš„åŒºé—´åˆ’åˆ†ç»“æœä¿å­˜åˆ°JSONæ–‡ä»¶ï¼ˆç¬¬4æ­¥éª¤ï¼šç»“æœè¾“å‡ºï¼‰

    å­¦ä¹  alignable_intervals_part2.py çš„ä¼˜ç‚¹ï¼š
    1. è¯¦ç»†çš„é”šç‚¹ä¿¡æ¯ä¿ç•™å’Œä¼ é€’
    2. ä¸°å¯Œçš„ç»Ÿè®¡ä¿¡æ¯å’Œè´¨é‡è¯„ä¼°
    3. å®Œæ•´çš„å¤„ç†å†å²å’Œå…ƒæ•°æ®è®°å½•
    4. æ”¯æŒå¤šé”šç‚¹çš„è¯¦ç»†ä¿¡æ¯ç®¡ç†

    å‚æ•°ï¼š
    output_file_path (str): è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„
    sentence_id (int): å¥å­ID
    final_x, final_y: æœ€ç»ˆè¿‡æ»¤åçš„é”šç‚¹åæ ‡
    intervals: å¯å¯¹é½åŒºé—´åˆ—è¡¨
    interval_stats: åŒºé—´ç»Ÿè®¡ä¿¡æ¯
    anchor_data: åŸå§‹é”šç‚¹æ•°æ®ï¼ˆåŒ…å«è¯¦ç»†ä¿¡æ¯ï¼‰
    params: å‚æ•°å­—å…¸
    """

    print(f"\n" + "="*80)
    print(f"ğŸ’¾ ä¿å­˜å®Œæ•´åŒºé—´åˆ’åˆ†ç»“æœåˆ°JSONæ–‡ä»¶ï¼ˆå¢å¼ºç‰ˆæœ¬ï¼‰")
    print(f"="*80)
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file_path}")

    # æ„å»ºå¢å¼ºçš„æœ€ç»ˆé”šç‚¹åˆ—è¡¨
    print(f"\nğŸ”§ æ­¥éª¤1: æ„å»ºæœ€ç»ˆé”šç‚¹æ•°æ®ç»“æ„")
    final_anchors = []
    anchor_processing_history = []

    for idx, (x, y) in enumerate(zip(final_x, final_y)):
        # è·å–åŸå§‹é”šç‚¹çš„è¯¦ç»†ä¿¡æ¯
        original_info = anchor_data.get('anchor_details', {}).get((x, y), {})

        # ç›´æ¥æ„å»ºæœ€ç»ˆé”šç‚¹ï¼Œä¼˜å…ˆä¿ç•™é«˜å±‚é”®å€¼
        final_anchor = {
            'id': idx + 1,
            'coordinates': [int(x), int(y)],
            'processing_status': 'final_kept',
            'final_rank': idx + 1
        }

        # å¦‚æœæœ‰åŸå§‹ä¿¡æ¯ï¼Œæ·»åŠ å…³é”®æŒ‡æ ‡åˆ°é¡¶å±‚
        if original_info:
            final_anchor.update({
                'original_id': original_info.get('id', 'unknown'),
                'similarity': original_info.get('similarity', 0.0),
                'quality_score': original_info.get('quality_score', 0.0),
                'density_ratio': original_info.get('density_ratio', 0.0),
                'source': original_info.get('source', 'unknown')
            })
            
            # æ„å»ºoriginal_infoæ—¶ç›´æ¥æ’é™¤å·²åœ¨é¡¶å±‚çš„å±æ€§
            final_anchor['original_info'] = {
                'type': original_info.get('type'),
                'rank_in_row': original_info.get('rank_in_row'),
                'total_candidates_in_row': original_info.get('total_candidates_in_row'),
                'local_density': original_info.get('local_density'),
                'filter_status': original_info.get('filter_status'),
                'filter_reason': original_info.get('filter_reason')
            }

            print(f"   é”šç‚¹#{idx+1}: ({x},{y}) åŸå§‹ID={original_info.get('id', 'N/A')} è´¨é‡={original_info.get('quality_score', 0.0):.4f}")
        else:
            print(f"   é”šç‚¹#{idx+1}: ({x},{y}) [æ–°å¢é”šç‚¹]")

        final_anchors.append(final_anchor)

    # è®°å½•é”šç‚¹å¤„ç†å†å²
    input_anchor_count = len(anchor_data.get('filtered_x', []))
    final_anchor_count = len(final_anchors)
    filtered_count = input_anchor_count - final_anchor_count

    anchor_processing_history = {
        'input_count': input_anchor_count,
        'final_count': final_anchor_count,
        'filtered_count': filtered_count,
        'filter_rate': (filtered_count / input_anchor_count * 100) if input_anchor_count > 0 else 0,
        'processing_steps': [
            'density_filtering_completed',
            'interval_division_applied',
            'final_diagonal_filtering_applied'
        ]
    }

    # æ„å»ºå¢å¼ºçš„åŒºé—´è¯¦æƒ…åˆ—è¡¨
    print(f"\nğŸ”§ æ­¥éª¤2: æ„å»ºåŒºé—´æ•°æ®ç»“æ„")
    interval_details = []
    total_interval_area = 0

    for idx, (begin, end) in enumerate(intervals):
        # è®¡ç®—åŒºé—´ç»Ÿè®¡
        english_sentences = int(end[0] - begin[0] + 1)
        chinese_sentences = int(end[1] - begin[1] + 1)
        interval_area = english_sentences * chinese_sentences
        total_interval_area += interval_area

        # è®¡ç®—åŒºé—´å†…çš„å­—ç¬¦æ•°
        english_chars = 0
        chinese_chars = 0

        english_sents = anchor_data.get('english_sentences', [])
        chinese_sents = anchor_data.get('chinese_sentences', [])

        for i in range(max(0, begin[0]), min(end[0] + 1, len(english_sents))):
            english_chars += len(english_sents[i])
        for i in range(max(0, begin[1]), min(end[1] + 1, len(chinese_sents))):
            chinese_chars += len(chinese_sents[i])

        interval_detail = {
            'interval_id': idx + 1,
            'coordinates': {
                'start': [int(begin[0]), int(begin[1])],
                'end': [int(end[0]), int(end[1])]
            },
            'sentence_counts': {
                'english': english_sentences,
                'chinese': chinese_sentences
            },
            'character_counts': {
                'english': english_chars,
                'chinese': chinese_chars
            },
            'statistics': {
                'area': interval_area,
                'density': interval_area / (anchor_data.get('len_sents1', 1) * anchor_data.get('len_sents2', 1)),
                'english_coverage': english_sentences / anchor_data.get('len_sents1', 1),
                'chinese_coverage': chinese_sentences / anchor_data.get('len_sents2', 1)
            }
        }
        interval_details.append(interval_detail)

        print(f"   åŒºé—´{idx+1}: {begin} â†’ {end} é¢ç§¯={interval_area} è‹±æ–‡={english_sentences}å¥ ä¸­æ–‡={chinese_sentences}å¥")

    # è®¡ç®—è´¨é‡è¯„ä¼°æŒ‡æ ‡
    print(f"\nğŸ”§ æ­¥éª¤3: è®¡ç®—è´¨é‡è¯„ä¼°æŒ‡æ ‡")
    total_doc_area = anchor_data.get('len_sents1', 1) * anchor_data.get('len_sents2', 1)
    coverage_ratio = total_interval_area / total_doc_area if total_doc_area > 0 else 0

    quality_metrics = {
        'overall_quality': min(1.0, coverage_ratio + (final_anchor_count / max(1, input_anchor_count)) * 0.3),
        'coverage_ratio': coverage_ratio,
        'anchor_retention_rate': (final_anchor_count / max(1, input_anchor_count)),
        'interval_efficiency': len(intervals) / max(1, final_anchor_count),
        'average_interval_size': total_interval_area / max(1, len(intervals))
    }

    print(f"   â†’ æ•´ä½“è´¨é‡è¯„åˆ†: {quality_metrics['overall_quality']:.4f}")
    print(f"   â†’ è¦†ç›–ç‡: {quality_metrics['coverage_ratio']:.4f}")
    print(f"   â†’ é”šç‚¹ä¿ç•™ç‡: {quality_metrics['anchor_retention_rate']:.4f}")

    # æ„å»ºå®Œæ•´çš„æ•°æ®ç»“æ„
    print(f"\nğŸ”§ æ­¥éª¤4: æ„å»ºå®Œæ•´çš„è¾“å‡ºæ•°æ®ç»“æ„")
    data = {
        "sentence_id": sentence_id,
        "final_anchors": {
            "count": len(final_anchors),
            "anchors": final_anchors,
            "processing_history": anchor_processing_history
        },
        "alignable_intervals": {
            "count": len(intervals),
            "intervals": interval_details,
            "total_area": total_interval_area
        },
        "interval_statistics": {
            "total_english_sentences": interval_stats.get('interval_length_sent1', 0),
            "total_chinese_sentences": interval_stats.get('interval_length_sent2', 0),
            "total_english_characters": interval_stats.get('interval_length_char1', 0),
            "total_chinese_characters": interval_stats.get('interval_length_char2', 0),
            "coverage_english_percent": (interval_stats.get('interval_length_sent1', 0) / anchor_data.get('len_sents1', 1)) * 100,
            "coverage_chinese_percent": (interval_stats.get('interval_length_sent2', 0) / anchor_data.get('len_sents2', 1)) * 100
        },
        "quality_metrics": quality_metrics,
        "source_data": {
            "input_anchor_count": input_anchor_count,
            "final_anchor_count": final_anchor_count,
            "filtered_anchor_count": filtered_count,
            "document_size": {
                "english_sentences": anchor_data.get('len_sents1', 0),
                "chinese_sentences": anchor_data.get('len_sents2', 0)
            },
            "average_density": anchor_data.get('average_density', 0.0),
            "source_file": anchor_data.get('source_file', 'unknown')
        },
        "processing_metadata": {
            "parameters": params,
            "processing_timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "processing_note": "å®Œæ•´å¤ç°å‚è€ƒä»£ç çš„åŒºé—´åˆ’åˆ†å’Œç»“æœè¾“å‡ºé€»è¾‘ï¼ˆå¢å¼ºç‰ˆæœ¬ï¼‰",
            "algorithm_version": "complete_anchor_extraction_v2.0",
            "features": [
                "enhanced_anchor_tracking",
                "detailed_interval_statistics",
                "quality_metrics_calculation",
                "processing_history_recording"
            ]
        }
    }

    try:
        print(f"\nğŸ’¾ ä¿å­˜JSONæ–‡ä»¶...")
        with open(output_file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"âœ… å®Œæ•´ç»“æœå·²æˆåŠŸä¿å­˜åˆ°: {output_file_path}")
        print(f"\nğŸ“Š ä¿å­˜ç»“æœç»Ÿè®¡:")
        print(f"   â€¢ æœ€ç»ˆé”šç‚¹æ•°é‡: {len(final_anchors)}")
        print(f"   â€¢ åŒºé—´æ•°é‡: {len(intervals)}")
        print(f"   â€¢ è‹±æ–‡è¦†ç›–ç‡: {data['interval_statistics']['coverage_english_percent']:.1f}%")
        print(f"   â€¢ ä¸­æ–‡è¦†ç›–ç‡: {data['interval_statistics']['coverage_chinese_percent']:.1f}%")
        print(f"   â€¢ æ•´ä½“è´¨é‡è¯„åˆ†: {quality_metrics['overall_quality']:.4f}")
        print(f"   â€¢ é”šç‚¹ä¿ç•™ç‡: {quality_metrics['anchor_retention_rate']:.1%}")
        print()

    except Exception as e:
        error_msg = f"ä¿å­˜JSONæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        print(f"âŒ {error_msg}")
        raise RuntimeError(error_msg)


# ä½¿ç”¨ç¤ºä¾‹å’Œä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    print("="*100)
    print("ğŸš€ å®Œæ•´çš„é”šç‚¹æå–å’ŒåŒºé—´åˆ’åˆ†ç¨‹åºå¯åŠ¨")
    print("="*100)
    print("ğŸ“‹ ç¨‹åºåŠŸèƒ½è¯´æ˜:")
    print("   â€¢ ç¬¬3æ­¥éª¤: åŒºé—´åˆ’åˆ† - åŸºäºè¿‡æ»¤åçš„é”šç‚¹åˆ’åˆ†å¯å¯¹é½åŒºé—´")
    print("   â€¢ ç¬¬4æ­¥éª¤: ç»“æœè¾“å‡º - æœ€ç»ˆå¯¹è§’çº¿è¿‡æ»¤å¹¶ä¿å­˜ç»“æœ")
    print("   â€¢ å®Œå…¨å¤ç°å‚è€ƒä»£ç  anchor_points.py ç¬¬649-875è¡Œçš„é€»è¾‘")
    print("   â€¢ ä¸ºåç»­DTWå¯¹é½ç®—æ³•æä¾›æ ‡å‡†åŒ–è¾“å…¥æ•°æ®")
    print()

    # å‚æ•°é…ç½®ï¼ˆä¸å‚è€ƒä»£ç ä¿æŒä¸€è‡´ï¼‰
    print("ğŸ”§ æ­¥éª¤1: é…ç½®ç®—æ³•å‚æ•°")
    params = {
        'verbose': True,           # æ˜¾ç¤ºè¯¦ç»†å¤„ç†ä¿¡æ¯
        'veryVerbose': True,       # æ˜¾ç¤ºè¶…è¯¦ç»†è°ƒè¯•ä¿¡æ¯
        'detectIntervals': True,   # å¯ç”¨åŒºé—´æ£€æµ‹
        'sentRatio': 0,           # å¥å­æ¯”ä¾‹ï¼ˆ0è¡¨ç¤ºè‡ªåŠ¨è®¡ç®—ï¼‰
        'maxDistToTheDiagonal': 10,    # æœ€å¤§å¯¹è§’çº¿è·ç¦»
        'maxGapSize': 20,         # æœ€å¤§é—´éš™å¤§å°
        'minHorizontalDensity': 0.1,   # æœ€å°æ°´å¹³å¯†åº¦
        'localDiagBeam': 0.3,     # å±€éƒ¨å¯¹è§’çº¿æŸå®½
        'minDensityRatio': 0.8,   # æœ€å°å¯†åº¦æ¯”ç‡
        'deltaX': 10,             # Xæ–¹å‘å¯†åº¦è®¡ç®—çª—å£
        'deltaY': 10,             # Yæ–¹å‘å¯†åº¦è®¡ç®—çª—å£
    }

    print("   âœ… å‚æ•°é…ç½®å®Œæˆ:")
    for key, value in params.items():
        print(f"      â€¢ {key}: {value}")
    print()

    # è‡ªåŠ¨æ£€æµ‹è·¯å¾„é…ç½®
    def get_data_paths():
        """æ ¹æ®å½“å‰å·¥ä½œç›®å½•è‡ªåŠ¨è°ƒæ•´æ•°æ®æ–‡ä»¶è·¯å¾„"""
        current_dir = os.path.basename(os.getcwd())
        if current_dir == "new-code":
            # ä» new-code ç›®å½•å†…è¿è¡Œ
            data_prefix = "../new-data/"
        else:
            # ä»é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
            data_prefix = "new-data/"

        return {
            "input_json_path": f"{data_prefix}anchor_filter-output.json",
            "output_json_path": f"{data_prefix}complete_interval_extraction-output.json"
        }

    # è·å–è·¯å¾„é…ç½®
    paths = get_data_paths()
    input_json_path = paths["input_json_path"]
    output_json_path = paths["output_json_path"]

    print("ğŸ”§ æ­¥éª¤2: è®¾ç½®è¾“å…¥è¾“å‡ºæ–‡ä»¶è·¯å¾„")

    print(f"   â†’ è¾“å…¥æ–‡ä»¶: {input_json_path}")
    print(f"   â†’ è¾“å‡ºæ–‡ä»¶: {output_json_path}")
    print()

    print("ï¿½ æ­¥éª¤3: å¼€å§‹æ‰§è¡Œå®Œæ•´çš„åŒºé—´åˆ’åˆ†å’Œç»“æœè¾“å‡º...")
    print()

    try:
        # åŠ è½½æ•°æ®
        print("ğŸ“‚ æ­¥éª¤3.1: åŠ è½½è¿‡æ»¤åçš„é”šç‚¹æ•°æ®")
        anchor_data = load_filtered_anchor_data_complete(input_json_path)

        # æ˜¾ç¤ºåŠ è½½çš„æ•°æ®ç»“æ„è¯¦æƒ…
        print("ğŸ” æ­¥éª¤3.1.1: éªŒè¯åŠ è½½çš„æ•°æ®ç»“æ„")
        print(f"   â†’ æ•°æ®ç»“æ„ç±»å‹: {type(anchor_data)}")
        print(f"   â†’ ä¸»è¦é”®å€¼: {list(anchor_data.keys())}")
        print(f"   â†’ é”šç‚¹æ•°æ®æ ¼å¼:")
        print(f"      â€¢ filtered_x: {len(anchor_data.get('filtered_x', []))} ä¸ªXåæ ‡")
        print(f"      â€¢ filtered_y: {len(anchor_data.get('filtered_y', []))} ä¸ªYåæ ‡")
        print(f"      â€¢ anchor_points: {len(anchor_data.get('anchor_points', {}))} ä¸ªç®€å•é”šç‚¹")
        print(f"      â€¢ anchor_details: {len(anchor_data.get('anchor_details', {}))} ä¸ªè¯¦ç»†é”šç‚¹")
        print(f"      â€¢ anchor_list: {len(anchor_data.get('anchor_list', []))} ä¸ªæœ‰åºé”šç‚¹")

        if anchor_data.get('anchor_details'):
            print(f"   â†’ é”šç‚¹è¯¦ç»†ä¿¡æ¯ç¤ºä¾‹:")
            for coord, details in list(anchor_data['anchor_details'].items())[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
                print(f"      â€¢ {coord}: ID={details.get('id')}, è´¨é‡={details.get('quality_score', 0):.4f}")
        print()

        # æ‰§è¡Œå®Œæ•´çš„é”šç‚¹æå–å’ŒåŒºé—´åˆ’åˆ†
        print("âš™ï¸  æ­¥éª¤3.2: æ‰§è¡Œå®Œæ•´çš„é”šç‚¹æå–å’ŒåŒºé—´åˆ’åˆ†")
        print("ğŸ”§ æ­¥éª¤3.2.1: å‡†å¤‡ç®—æ³•è¾“å…¥å‚æ•°")
        print(f"   â†’ è¾“å…¥é”šç‚¹åæ ‡: X={anchor_data['filtered_x']}, Y={anchor_data['filtered_y']}")
        print(f"   â†’ é”šç‚¹å­—å…¸å¤§å°: {len(anchor_data['anchor_points'])}")
        print(f"   â†’ å¹³å‡å¯†åº¦: {anchor_data['average_density']:.4f}")
        print(f"   â†’ æ–‡æ¡£å¤§å°: è‹±æ–‡{anchor_data['len_sents1']}å¥, ä¸­æ–‡{anchor_data['len_sents2']}å¥")
        print(f"   â†’ ç›¸ä¼¼åº¦çŸ©é˜µ: {anchor_data['sim_mat'].shape}")

        final_x, final_y, intervals, sent1, sent2, char1, char2 = extract_anchor_points_complete(
            params,
            anchor_data['filtered_x'],
            anchor_data['filtered_y'],
            anchor_data['anchor_points'],
            anchor_data['average_density'],
            anchor_data['english_sentences'],
            anchor_data['chinese_sentences'],
            anchor_data['len_sents1'],
            anchor_data['len_sents2'],
            anchor_data['sim_mat']
        )

        print("ğŸ” æ­¥éª¤3.2.2: éªŒè¯å¤„ç†ç»“æœ")
        print(f"   â†’ æœ€ç»ˆé”šç‚¹æ•°é‡: {len(final_x)}")
        print(f"   â†’ è¾“å‡ºåŒºé—´æ•°é‡: {len(intervals)}")
        print(f"   â†’ é”šç‚¹ä¿ç•™ç‡: {len(final_x)/len(anchor_data['filtered_x'])*100:.1f}%")
        if final_x:
            print(f"   â†’ æœ€ç»ˆé”šç‚¹åæ ‡: {list(zip(final_x, final_y))}")
        if intervals:
            print(f"   â†’ åŒºé—´æ¦‚è§ˆ: {intervals}")
        print()

        # ä¿å­˜ç»“æœ
        print("ğŸ’¾ æ­¥éª¤3.3: ä¿å­˜å¤„ç†ç»“æœåˆ°JSONæ–‡ä»¶")
        print("ğŸ”§ æ­¥éª¤3.3.1: æ„å»ºåŒºé—´ç»Ÿè®¡æ•°æ®")
        interval_stats = {
            'interval_length_sent1': sent1,
            'interval_length_sent2': sent2,
            'interval_length_char1': char1,
            'interval_length_char2': char2
        }

        print(f"   â†’ åŒºé—´ç»Ÿè®¡:")
        print(f"      â€¢ è‹±æ–‡å¥å­æ€»æ•°: {sent1}")
        print(f"      â€¢ ä¸­æ–‡å¥å­æ€»æ•°: {sent2}")
        print(f"      â€¢ è‹±æ–‡å­—ç¬¦æ€»æ•°: {char1}")
        print(f"      â€¢ ä¸­æ–‡å­—ç¬¦æ€»æ•°: {char2}")

        print("ğŸ”§ æ­¥éª¤3.3.2: æ‰§è¡Œå¢å¼ºç‰ˆJSONä¿å­˜")
        save_complete_results_to_json(
            output_json_path,
            anchor_data['sentence_id'],
            final_x, final_y, intervals, interval_stats,
            anchor_data, params
        )

        print("="*100)
        print("ğŸ‰ ç¬¬3ã€4æ­¥éª¤å®Œæ•´å¤„ç†æˆåŠŸï¼")
        print("="*100)
        print("ğŸ“Š å¤„ç†ç»“æœæ€»ç»“:")
        print(f"   â€¢ è¾“å…¥é”šç‚¹æ•°é‡: {len(anchor_data['filtered_x'])}")
        print(f"   â€¢ æœ€ç»ˆé”šç‚¹æ•°é‡: {len(final_x)}")
        print(f"   â€¢ è¾“å‡ºåŒºé—´æ•°é‡: {len(intervals)}")
        print(f"   â€¢ è‹±æ–‡å¥å­è¦†ç›–: {sent1}/{anchor_data['len_sents1']} ({sent1/anchor_data['len_sents1']*100:.1f}%)")
        print(f"   â€¢ ä¸­æ–‡å¥å­è¦†ç›–: {sent2}/{anchor_data['len_sents2']} ({sent2/anchor_data['len_sents2']*100:.1f}%)")
        print(f"   â€¢ è‹±æ–‡å­—ç¬¦è¦†ç›–: {char1}")
        print(f"   â€¢ ä¸­æ–‡å­—ç¬¦è¦†ç›–: {char2}")
        print()
        print("ğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print(f"   â€¢ ç»“æœæ–‡ä»¶: {output_json_path}")
        print()
        print("ğŸš€ ä¸‹ä¸€æ­¥:")
        print("   âœ… æ•°æ®å·²å‡†å¤‡å°±ç»ªï¼Œå¯ç»§ç»­è¿›è¡ŒDTWå¯¹é½ç®—æ³•ï¼")
        print("   â†’ ä½¿ç”¨ç”Ÿæˆçš„åŒºé—´æ•°æ®è¿›è¡ŒåŠ¨æ€æ—¶é—´è§„æ•´å¯¹é½")
        print("="*100)

    except Exception as e:
        print("="*100)
        print("âŒ æ‰§è¡Œå¤±è´¥")
        print("="*100)
        print(f"é”™è¯¯ä¿¡æ¯: {e}")
        print("è¯·æ£€æŸ¥:")
        print("   â€¢ è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®")
        print("   â€¢ è¾“å‡ºç›®å½•æ˜¯å¦æœ‰å†™å…¥æƒé™")
        print("   â€¢ å‚æ•°é…ç½®æ˜¯å¦åˆç†")
        print("="*100)
